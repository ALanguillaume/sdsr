[
["index.html", "Spatial Data Science Preface", " Spatial Data Science Edzer Pebesma, Roger Bivand 2019-06-04 Preface Data science is concerned with finding answers to questions on the basis of available data, and communicating that effort. Besides showing the results, this communication involves sharing the data used, but also exposing the path that led to the answers in a comprehensive and reproducible way. It also acknowledges the fact that available data may not be sufficient to answer questions, and that any answers are conditional on the data collection or sampling protocols employed. This book introduces and explains the concepts underlying spatial data: points, lines, polygons, rasters, coverages, geometry attributes, data cubes, reference systems, as well as higher-level concepts including how attributes relate to geometries and how this affects analysis. The relationship of attributes to geometries is known as support, and changing support also changes the characteristics of attributes. Some data generation processes are continuous in space, and may be observed everywhere. Others are discrete, observed in tesselated containers. In modern spatial data analysis, tesellated methods are often used for all data, extending across the legacy partition into point process, geostatistical and lattice models. It is support (and the understanding of support) that underlies the importance of spatial representation. The book aims at data scientists who want to get a grip on using spatial data in their analysis. To exemplify how to do things, it uses R. It is often thought that spatial data boils down to having observations’ longitude and latitude in a dataset, and treating these just like any other variable. This carries the risk of missed opportunities and meaningless analyses. For instance, coordinate pairs really are pairs, and lose much of their meaning when treated independently rather than having point locations, observations are often associated with spatial lines, areas, or grid cells spatial distances between observations are often not well represented by straight-line distances, but by great circle distances, distances through networks, or by measuring the effort it takes getting from A to B We introduce the concepts behind spatial data, coordinate reference systems, spatial analysis, and introduce a number of packages, including sf (Pebesma 2018, E. Pebesma (2019b)), lwgeom (E. Pebesma 2019a), and stars (E. Pebesma 2019c), as well as a number of tidyverse (Wickham 2017) extensions, and a number of spatial analysis packages that can be used with these packages, including gstat (Pebesma and Graeler 2019), spdep (R. Bivand 2019b) and spatstat (Baddeley, Turner, and Rubak 2018). This work is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International License. References "],
["intro.html", "Chapter 1 Getting Started 1.1 A first map 1.2 Reading and writing 1.3 Exercises", " Chapter 1 Getting Started This chapter gives a quick start to get you going with spatial data science with R. It is easier to read when understanding R at the level of, say, R for Data Science (Wickham and Grolemund 2017). 1.1 A first map There is a lot to say about spatial data, but let us first create a map. We can create a simple map by: library(tidyverse) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; Registered S3 method overwritten by &#39;rvest&#39;: #&gt; method from #&gt; read_xml.response xml2 #&gt; ── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ── #&gt; ✔ ggplot2 3.1.1 ✔ purrr 0.3.2 #&gt; ✔ tibble 2.1.1 ✔ dplyr 0.8.1 #&gt; ✔ tidyr 0.8.3 ✔ stringr 1.4.0 #&gt; ✔ readr 1.3.1 ✔ forcats 0.4.0 #&gt; ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ── #&gt; ✖ dplyr::filter() masks stats::filter() #&gt; ✖ dplyr::lag() masks stats::lag() library(sf) #&gt; Linking to GEOS 3.7.0, GDAL 2.4.0, PROJ 5.2.0 system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;) %&gt;% read_sf() %&gt;% st_transform(32119) %&gt;% select(BIR74) %&gt;% plot(graticule = TRUE, axes = TRUE) Figure 1.1: a first map A lot went on, here. We will describe the steps in detail. First, we loaded two R packages: library(tidyverse) library(sf) where tidyverse is needed for the tidyverse functions and methods, and sf is needed for the spatial commands and spatial tidyverse methods. Package sf implements simple features, a standardised way to encode vector data (points, lines, polygons). We will say more about simple features in chapter 3. Most commands in package sf start with st_, short for spatiotemporal, a convention it shares with e.g. PostGIS. The %&gt;% (pipe) symbols should be read as then: we read a %&gt;% b() %&gt;% c() %&gt;% d(n = 10) as with a do b then c then d, and that is just alternative syntax for d(c(b(a)), n = 10) or tmp1 &lt;- b(a) tmp2 &lt;- c(tmp1) tmp3 &lt;- d(tmp2, n = 10) To many, the pipe-form is easier to read because execution order follows reading order (from left to right). Like nested function calls, it avoids the need to choose names for intermediate results. For the illustration we picked a data file that comes with sf, the location of which depends on the operating system used. The following will give a different output on your computer: (file &lt;- system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;)) #&gt; [1] &quot;/home/edzer/R/x86_64-pc-linux-gnu-library/3.6/sf/gpkg/nc.gpkg&quot; Never use system.file if you want to read your own data; in that case, fname should be the data source (typically file or path) name (section 1.2). (Parens around this expression are used to have the result not only stored, but also printed.) Then, we read this file into R using read_sf: (file %&gt;% read_sf() -&gt; nc) #&gt; Simple feature collection with 100 features and 14 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -84.3 ymin: 33.9 xmax: -75.5 ymax: 36.6 #&gt; epsg (SRID): 4267 #&gt; proj4string: +proj=longlat +datum=NAD27 +no_defs #&gt; # A tibble: 100 x 15 #&gt; AREA PERIMETER CNTY_ CNTY_ID NAME FIPS FIPSNO CRESS_ID BIR74 SID74 #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.114 1.44 1825 1825 Ashe 37009 37009 5 1091 1 #&gt; 2 0.061 1.23 1827 1827 Alle… 37005 37005 3 487 0 #&gt; 3 0.143 1.63 1828 1828 Surry 37171 37171 86 3188 5 #&gt; 4 0.07 2.97 1831 1831 Curr… 37053 37053 27 508 1 #&gt; 5 0.153 2.21 1832 1832 Nort… 37131 37131 66 1421 9 #&gt; 6 0.097 1.67 1833 1833 Hert… 37091 37091 46 1452 7 #&gt; # … with 94 more rows, and 5 more variables: NWBIR74 &lt;dbl&gt;, BIR79 &lt;dbl&gt;, #&gt; # SID79 &lt;dbl&gt;, NWBIR79 &lt;dbl&gt;, geom &lt;MULTIPOLYGON [°]&gt; which creates a “spatial tibble”: class(nc) #&gt; [1] &quot;sf&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; This object is transformed into a new coordinate reference system (North Carolina State Plane, with EPSG code 32119): (nc %&gt;% st_transform(32119) -&gt; nc.32119) #&gt; Simple feature collection with 100 features and 14 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: 124000 ymin: 14700 xmax: 931000 ymax: 318000 #&gt; epsg (SRID): 32119 #&gt; proj4string: +proj=lcc +lat_1=36.16666666666666 +lat_2=34.33333333333334 +lat_0=33.75 +lon_0=-79 +x_0=609601.22 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs #&gt; # A tibble: 100 x 15 #&gt; AREA PERIMETER CNTY_ CNTY_ID NAME FIPS FIPSNO CRESS_ID BIR74 SID74 #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.114 1.44 1825 1825 Ashe 37009 37009 5 1091 1 #&gt; 2 0.061 1.23 1827 1827 Alle… 37005 37005 3 487 0 #&gt; 3 0.143 1.63 1828 1828 Surry 37171 37171 86 3188 5 #&gt; 4 0.07 2.97 1831 1831 Curr… 37053 37053 27 508 1 #&gt; 5 0.153 2.21 1832 1832 Nort… 37131 37131 66 1421 9 #&gt; 6 0.097 1.67 1833 1833 Hert… 37091 37091 46 1452 7 #&gt; # … with 94 more rows, and 5 more variables: NWBIR74 &lt;dbl&gt;, BIR79 &lt;dbl&gt;, #&gt; # SID79 &lt;dbl&gt;, NWBIR79 &lt;dbl&gt;, geom &lt;MULTIPOLYGON [m]&gt; and a single attribute column is selected (nc.32119 %&gt;% select(BIR74) -&gt; nc.32119.bir74) #&gt; Simple feature collection with 100 features and 1 field #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: 124000 ymin: 14700 xmax: 931000 ymax: 318000 #&gt; epsg (SRID): 32119 #&gt; proj4string: +proj=lcc +lat_1=36.16666666666666 +lat_2=34.33333333333334 +lat_0=33.75 +lon_0=-79 +x_0=609601.22 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs #&gt; # A tibble: 100 x 2 #&gt; BIR74 geom #&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [m]&gt; #&gt; 1 1091 (((387345 278387, 381334 282774, 379438 282943, 373250 290553, 363… #&gt; 2 487 (((408602 292425, 408565 293985, 406643 296873, 406420 3e+05, 4023… #&gt; 3 3188 (((478717 277490, 476936 278867, 471503 279173, 470806 281394, 469… #&gt; 4 508 (((878194 289128, 877381 291117, 875994 290881, 874941 292805, 870… #&gt; 5 1421 (((769835 277796, 768364 274842, 762616 274401, 763168 269009, 761… #&gt; 6 1452 (((812328 277876, 791158 277012, 789882 277579, 777724 277107, 769… #&gt; # … with 94 more rows Finally, the result is plotted, with the command: nc.32119.bir74 %&gt;% plot(graticule = TRUE, axes = TRUE) as shown in figure 1.1. Splitting up the steps lets us see what is happening, as errors from combined steps can be hard to assign to the right step. Repeating the same sequence, but with the wrong argument to st_transform gives: system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;) %&gt;% read_sf() %&gt;% st_transform(32999) %&gt;% select(BIR74) %&gt;% plot(graticule = TRUE, axes = TRUE) #&gt; Warning in CPL_crs_from_epsg(as.integer(x)): GDAL Error 6: EPSG PCS/GCS #&gt; code 32999 not found in EPSG support files. Is this a valid EPSG coordinate #&gt; system? #&gt; OGR: Corrupt data #&gt; Error in CPL_transform(x, crs$proj4string) : OGR error which is informative, but still needs more backtracking to find the cause than taking things step-by-step in the first instance. Again, mistyping a column name will cause an error: system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;) %&gt;% read_sf() %&gt;% st_transform(32119) %&gt;% select(BIR75) %&gt;% plot(graticule = TRUE, axes = TRUE) #&gt; Error in .f(.x[[i]], ...) : object &#39;BIR75&#39; not found #&gt; Error in .f(.x[[i]], ...) : object &#39;BIR75&#39; not found Where do these commands come from? library and system.file are base R. We can ask for help about a particular command by entering e.g. ?library The command read_sf is an alternative to the st_read, which returns a spatial tibble instead of a spatial data frame, and will be discussed in section 1.2. The st_transform method is used here to convert from the geographic coordinates (degrees longitude and latitude) into “flat” coordinates, meaning \\(x\\) and \\(y\\) coordinates in a planar system. It will be discussed in section 8.1. The plot method for sf objects chooses default colors and legends settings; we instructed it to add a graticule (the grey lines of equal longitude and latitude) and degree labels along the axes. It is described in chapter 9. As witnessed by the plot, the plot command receives county polygons as well as BIR74 values for each polygon. How is it possible that we select only the BIR74 variable, but still can plot the polygons? This is because package sf provides a select method: methods(select) #&gt; [1] select.data.frame* select.default* select.grouped_df* #&gt; [4] select.list select.sf* select.tbl_cube* #&gt; see &#39;?methods&#39; for accessing help and source code and this method (select.sf) makes the geometry (geom) sticky: nc %&gt;% select(BIR74) %&gt;% names() #&gt; [1] &quot;BIR74&quot; &quot;geom&quot; In sp, select methods using [ were sticky. We get the “normal” select behaviour if we first coerce to a normal tibble: nc %&gt;% as_tibble(validate = TRUE) %&gt;% select(BIR74) %&gt;% names() #&gt; [1] &quot;BIR74&quot; A ggplot is created when we use geom_sf: ggplot() + geom_sf(data = nc.32119) + aes(fill = BIR74) + theme(panel.grid.major = element_line(color = &quot;white&quot;)) + scale_fill_gradientn(colors = sf.colors(20)) Figure 1.2: first ggplot and a facet plot for a pair of columns in nc.32119 is obtained by gathering the columns: nc.32119 %&gt;% select(SID74, SID79) %&gt;% gather(VAR, SID, -geom) -&gt; nc2 ggplot() + geom_sf(data = nc2, aes(fill = SID)) + facet_wrap(~VAR, ncol = 1) + scale_y_continuous(breaks = 34:36) + scale_fill_gradientn(colors = sf.colors(20)) + theme(panel.grid.major = element_line(color = &quot;white&quot;)) Figure 1.3: ggplot with facet maps An interactive, leaflet-type map is obtained by suppressPackageStartupMessages(library(mapview)) nc.32119 %&gt;% mapview(zcol = &quot;BIR74&quot;, legend = TRUE, col.regions = sf.colors) 1.2 Reading and writing Typical R data science tasks start with reading data from an external source; this may be a file, or a set of files like a “shapefile”, or a database, or a web service. Package sf can read from a large number of different data source types, each having its own driver. The following commands show how many vector and raster drivers we have available: st_drivers(&quot;vector&quot;) %&gt;% nrow() # vector drivers #&gt; [1] 88 st_drivers(&quot;raster&quot;) %&gt;% nrow() # raster drivers #&gt; [1] 144 (the output you see may differ because of different operating system and configuration; when the same version of GDAL is in use, the drivers available in sf are the same as in rgdal.) 1.2.1 GDAL st_drivers lists the drivers available to GDAL, the geospatial data abstraction library. This library can be seen as the Swiss army knive of spatial data; besides for R it is being used in Python, QGIS, PostGIS, and more than 100 other software projects. The dependency of sf on other R packages and system libraries is shown in figure 1.4. Figure 1.4: sf and its dependencies; arrows indicate strong dependency, dashed arrows weak dependency Note that the C/C++ libraries used (GDAL, GEOS, PROJ, liblwgeom, udunits2) are all developed, maintained and used by data science communities that are large but different from the R community. By using these libraries, we share how we understand what we are doing with these other communities. Because R (and Python) provide interactive interfaces to this software, many R users get closer to these libraries than do users of other software based on these libraries. This is not only important for resolving problems, but also for reaching consensus on which findings are helpful. GDAL is a “library of libraries” – in order to read all these data sources it needs a large number of other libraries. It typically links to over 100 other libraries. Binary packages distributed by CRAN contain only statically linked code: CRAN does not want to make any assumptions about presence of third-party libraries on the host system. As a consequence, when the sf package is installed in binary form from CRAN, it includes a copy of all the required external libraries as well as their dependencies, which may amount to 100 Mb. 1.2.2 st_read or read_sf? The function to read vector data is st_read. Function read_sf is largely the same as st_read, but chooses a few tidyverse-style defaults: it is silent by default, where st_read gives a short report it returns a spatial tibble instead of a spatial data frame it sets as default stringsAsFactors = FALSE, where st_read listens to the global option default.stringsAsFactors() (which is TRUE by default) it accepts list-columns as input In the same fashion, compared to st_write, function write_sf, is also silent overwrites layers (i.e., sets delete_layer = TRUE) by default, which st_write does not do. 1.2.3 reading and writing raster data Raster data can be read with function read_stars from package stars library(stars) #&gt; Loading required package: abind tif = system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;) (x = tif %&gt;% read_stars()) #&gt; stars object with 3 dimensions and 1 attribute #&gt; attribute(s): #&gt; L7_ETMs.tif #&gt; Min. : 1.0 #&gt; 1st Qu.: 54.0 #&gt; Median : 69.0 #&gt; Mean : 68.9 #&gt; 3rd Qu.: 86.0 #&gt; Max. :255.0 #&gt; dimension(s): #&gt; from to offset delta refsys point values #&gt; x 1 349 288776 28.5 +proj=utm +zone=25 +south... FALSE NULL [x] #&gt; y 1 352 9120761 -28.5 +proj=utm +zone=25 +south... FALSE NULL [y] #&gt; band 1 6 NA NA NA NA NULL Plotting this object shows the six different spectral bands read, with color breaks based on quantiles of pixel values accross all bands: plot(x) Similarly, we can write raster data in a stars object with write_stars tif_file = paste0(tempfile(), &quot;.tif&quot;) write_stars(x, tif_file) We can read back the raster metadata (its dimensions and reference system, but not the actual pixel values) by read_stars(tif_file, proxy = TRUE) #&gt; stars_proxy object with 1 attribute in file: #&gt; $file4c1c3ed93dba.tif #&gt; [1] &quot;/tmp/Rtmp5nfXdA/file4c1c3ed93dba.tif&quot; #&gt; #&gt; dimension(s): #&gt; from to offset delta refsys point values #&gt; x 1 349 288776 28.5 +proj=utm +zone=25 +south... FALSE NULL [x] #&gt; y 1 352 9120761 -28.5 +proj=utm +zone=25 +south... FALSE NULL [y] #&gt; band 1 6 NA NA NA NA NULL Raster data analysis and its integration with vector data is explained in detail in chapter 4. 1.2.4 Reading from files, and legacy shapefiles We saw above that a spatial dataset can be read from a single file by system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;) %&gt;% read_sf() -&gt; nc In some cases, spatial datasets are contained in multiple files, e.g. in the case of shapefiles. A “shapefile” should be really understood as a set of files with a common prefix, or even a directory with several of such sets. Package sf comes with a couple of shapefiles packaged, a directory listing of the shape directory in the packge is obtained by list.files(system.file(&quot;shape/&quot;, package = &quot;sf&quot;)) #&gt; [1] &quot;nc.dbf&quot; &quot;nc.prj&quot; #&gt; [3] &quot;nc.shp&quot; &quot;nc.shx&quot; #&gt; [5] &quot;olinda1.dbf&quot; &quot;olinda1.prj&quot; #&gt; [7] &quot;olinda1.shp&quot; &quot;olinda1.shx&quot; #&gt; [9] &quot;storms_xyz_feature.dbf&quot; &quot;storms_xyz_feature.shp&quot; #&gt; [11] &quot;storms_xyz_feature.shx&quot; &quot;storms_xyz.dbf&quot; #&gt; [13] &quot;storms_xyz.shp&quot; &quot;storms_xyz.shx&quot; #&gt; [15] &quot;storms_xyzm_feature.dbf&quot; &quot;storms_xyzm_feature.shp&quot; #&gt; [17] &quot;storms_xyzm_feature.shx&quot; &quot;storms_xyzm.dbf&quot; #&gt; [19] &quot;storms_xyzm.shp&quot; &quot;storms_xyzm.shx&quot; We can read a single shapefile by system.file(&quot;shape/nc.shp&quot;, package=&quot;sf&quot;) %&gt;% read_sf() -&gt; nc and it is important to know that in that case all four files starting with nc are read from this directory. We can also read the directory with shapefiles by system.file(&quot;shape&quot;, package=&quot;sf&quot;) %&gt;% read_sf() -&gt; something #&gt; Warning in evalq((function (..., call. = TRUE, immediate. = FALSE, #&gt; noBreaks. = FALSE, : automatically selected the first layer in a data #&gt; source containing more than one. but we see some warnings now, indicating that we are reading only the first layer from a multi-layer dataset (and not nc.shp!). Indeed, this directory contains multiple layers, which can be queried by system.file(&quot;shape&quot;, package=&quot;sf&quot;) %&gt;% st_layers() #&gt; Driver: ESRI Shapefile #&gt; Available layers: #&gt; layer_name geometry_type features fields #&gt; 1 storms_xyzm_feature Measured Line String 71 1 #&gt; 2 storms_xyz 3D Line String 71 0 #&gt; 3 nc Polygon 100 14 #&gt; 4 storms_xyz_feature 3D Line String 71 1 #&gt; 5 olinda1 Polygon 470 6 #&gt; 6 storms_xyzm Measured Line String 71 0 From this list, we could pick one, and use it as the layer argument, as in dataset &lt;- system.file(&quot;shape&quot;, package=&quot;sf&quot;) layer &lt;- &quot;nc&quot; nc &lt;- read_sf(dataset, layer) which is essentially a convoluted way of what we did before to read nc.shp. Considering shapefiles in directories as layers in a dataset is not something that sf came up with, but is the way GDAL handles this. Although it is a good idea in general to give up using shapefiles, we cannot always control the format of the spatial data we get to start with. 1.2.5 Reading from a text string In the special case of a GeoJSON (Butler et al. 2016) dataset, when the dataset is contained in a length-one character vector, it can be directly passed to read_sf and read from memory: str &lt;- &#39;{ &quot;type&quot;: &quot;FeatureCollection&quot;, &quot;features&quot;: [ { &quot;type&quot;: &quot;Feature&quot;, &quot;geometry&quot;: { &quot;type&quot;: &quot;Point&quot;, &quot;coordinates&quot;: [102.0, 0.5] }, &quot;properties&quot;: { &quot;prop0&quot;: &quot;value0&quot; } }, { &quot;type&quot;: &quot;Feature&quot;, &quot;geometry&quot;: { &quot;type&quot;: &quot;LineString&quot;, &quot;coordinates&quot;: [ [102.0, 0.0], [103.0, 1.0], [104.0, 0.0], [105.0, 1.0] ] }, &quot;properties&quot;: { &quot;prop0&quot;: &quot;value0&quot;, &quot;prop1&quot;: 0.0 } }, { &quot;type&quot;: &quot;Feature&quot;, &quot;geometry&quot;: { &quot;type&quot;: &quot;Polygon&quot;, &quot;coordinates&quot;: [ [ [100.0, 0.0], [101.0, 0.0], [101.0, 1.0], [100.0, 1.0], [100.0, 0.0] ] ] }, &quot;properties&quot;: { &quot;prop0&quot;: &quot;value0&quot;, &quot;prop1&quot;: { &quot;this&quot;: &quot;that&quot; } } } ] }&#39; (sf_obj &lt;- read_sf(str)) #&gt; Simple feature collection with 3 features and 2 fields #&gt; geometry type: GEOMETRY #&gt; dimension: XY #&gt; bbox: xmin: 100 ymin: 0 xmax: 105 ymax: 1 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; # A tibble: 3 x 3 #&gt; prop0 prop1 geometry #&gt; &lt;chr&gt; &lt;chr&gt; &lt;GEOMETRY [°]&gt; #&gt; 1 value0 &lt;NA&gt; POINT (102 0.5) #&gt; 2 value0 0.0 LINESTRING (102 0, 103 1, 104 0, 105 1) #&gt; 3 value0 &quot;{ \\&quot;this\\&quot;: \\&quot;that\\&quot;… POLYGON ((100 0, 101 0, 101 1, 100 1, 100 … 1.2.6 Database Data can be read from a spatial database directly through two paths. The first is to use the standard database interface of R (DBI), for instance with a SQLITE database: library(RSQLite) db = system.file(&quot;sqlite/meuse.sqlite&quot;, package = &quot;sf&quot;) dbcon &lt;- dbConnect(dbDriver(&quot;SQLite&quot;), db) (s = st_read(dbcon, &quot;meuse.sqlite&quot;))[1:3,] #&gt; Simple feature collection with 3 features and 13 fields #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 181000 ymin: 334000 xmax: 181000 ymax: 334000 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; ogc_fid cadmium copper lead zinc elev dist om ffreq soil lime #&gt; 1 1 11.7 85 299 1022 7.91 0.00136 13.6 1 1 1 #&gt; 2 2 8.6 81 277 1141 6.98 0.01222 14.0 1 1 1 #&gt; 3 3 6.5 68 199 640 7.80 0.10303 13.0 1 1 1 #&gt; landuse dist.m GEOMETRY #&gt; 1 Ah 50 POINT (181072 333611) #&gt; 2 Ah 30 POINT (181025 333558) #&gt; 3 Ah 150 POINT (181165 333537) dbDisconnect(dbcon) Another way is to use GDAL database drivers, e.g. by st_read(db)[1:3,] #&gt; Reading layer `meuse.sqlite&#39; from data source `/home/edzer/R/x86_64-pc-linux-gnu-library/3.6/sf/sqlite/meuse.sqlite&#39; using driver `SQLite&#39; #&gt; Simple feature collection with 155 features and 12 fields #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 179000 ymin: 330000 xmax: 181000 ymax: 334000 #&gt; epsg (SRID): 28992 #&gt; proj4string: +proj=sterea +lat_0=52.15616055555555 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +ellps=bessel +towgs84=565.4171,50.3319,465.5524,-0.398957,0.343988,-1.87740,4.0725 +units=m +no_defs #&gt; Simple feature collection with 3 features and 12 fields #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 181000 ymin: 334000 xmax: 181000 ymax: 334000 #&gt; epsg (SRID): 28992 #&gt; proj4string: +proj=sterea +lat_0=52.15616055555555 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +ellps=bessel +towgs84=565.4171,50.3319,465.5524,-0.398957,0.343988,-1.87740,4.0725 +units=m +no_defs #&gt; cadmium copper lead zinc elev dist om ffreq soil lime landuse #&gt; 1 11.7 85 299 1022 7.91 0.00136 13.6 1 1 1 Ah #&gt; 2 8.6 81 277 1141 6.98 0.01222 14.0 1 1 1 Ah #&gt; 3 6.5 68 199 640 7.80 0.10303 13.0 1 1 1 Ah #&gt; dist.m GEOMETRY #&gt; 1 50 POINT (181072 333611) #&gt; 2 30 POINT (181025 333558) #&gt; 3 150 POINT (181165 333537) An advantage of the former approach may be that any query can be passed, allowing for reading only parts of a table into R’s memory. 1.3 Exercises Read the shapefile storms_xyz_feature from the shape directory in the sf package Copy this file to another directory on your computer, and read it from there (note: a shapefile consists of more than one file!) How many features does this dataset contain? Plot the dataset, with axes = TRUE (hint: before plotting, pipe through st_zm to drop Z and M coordinates; more about this in chapter 3). Before plotting, pipe the dataset through st_set_crs(4326). What is different in the plot obtained? References "],
["cs.html", "Chapter 2 Coordinate systems 2.1 Cartesian and geodetic coordinates 2.2 Ellipsoidal coordinates 2.3 Distances 2.4 Bounded spaces 2.5 Time 2.6 Exercises", " Chapter 2 Coordinate systems For spatial data, the location of observations are characterised by coordinates, and coordinates are defined in a coordinate system. Different coordinate systems can be used for this, and the most important difference is whether coordinates are defined over a 2-dimensional or 3-dimensional space referenced to orthogonal axes (Cartesian coordinates), or using distance and directions (polar coordinates, spherical coordinates). Figure 2.1: Two-dimensional polar (red) and Cartesian (blue) coordinates 2.1 Cartesian and geodetic coordinates Figure 2.1 shows both polar and Cartesian coordinates for a standard two-dimensional situation. In Cartesian coordinates, the point shown is \\((x,y) = (3,4)\\), for polar coordinates it is \\((r,\\phi) = (5, \\mbox{arctan}(4/3))\\), where \\(\\mbox{arctan}(4/3)\\) is approximately \\(0.93\\) radians, or \\(53^{\\circ}\\). Note that \\(x\\), \\(y\\) and \\(r\\) all have length units, where \\(\\phi\\) is an angle (a unitless length/length ratio). Converting back and forth between Cartesian and polar coordinates is trivial, \\[x = r~\\mbox{cos} \\phi\\] \\[y = r~\\mbox{sin} \\phi\\] \\[\\phi = \\mbox{arctan}(y/x)\\] \\[r = \\sqrt{x^2 + y^2}\\] but requires care with using the right quadrant for \\(\\mbox{arctan}\\); the atan2 function is helpful here. 2.2 Ellipsoidal coordinates In three dimensions, where Cartesian coordinates are expressed as \\((x,y,z)\\), spherical coordinates are the three-dimensional equivalent of polar coordinates and can be expressed as \\((r,\\lambda,\\phi)\\), where \\(r\\) is the radius of the sphere, \\(\\lambda\\) is the longitude, measured in the \\((x,y)\\) plane counter-clockwise from positive \\(x\\), and \\(\\phi\\) is the latitude, the angle between the vector and the \\((x,y)\\) plane. \\(\\lambda\\) typically varies between \\(-180^{\\circ}\\) and \\(180^{\\circ}\\) (or alternatively from \\(0^{\\circ}\\) to \\(360^{\\circ}\\)), \\(\\phi\\) from \\(-90^{\\circ}\\) to \\(90^{\\circ}\\). When we are only interested in points on a sphere with given radius, we can drop \\(r\\): \\((\\lambda,\\phi)\\) now suffice to identify any point. It should be noted that this is just a definition, one could for instance also choose to measure polar angle, i.e. the angle between the vector and \\(z\\), instead of latitude. There is also a long tradition of specifying points as \\((\\phi,\\lambda)\\) but throughout this book we will stick to longitude-latitude, \\((\\lambda,\\phi)\\). For points on an ellipse, there are two ways in which angle can be expressed (figure 2.2): measured from the center of the ellipse (\\(\\psi\\)), or measured perpendicular to the tangent on the ellipse at the target point (\\(\\phi\\)). Figure 2.2: Angles on an ellipse: geodetic (blue) and geocentric (red) latitude The most commonly used parametric model for the Earth is an ellipsoid of revolution, an ellipsoid with two equal semi-axes (Iliffe and Lott 2008). In effect, this is a flattened sphere (or spheroid): the distance between the poles is (slightly: about 0.33%) smaller than the distance between two opposite points on the equator. Under this model, longitude is always measured along a circle, but latitude along an ellipse. If we think of figure 2.2 as a cross section of the Earth passing through the poles, the latitude measure \\(\\phi\\) is the one used when no further specification is given; it is also called geodetic latitude. The latitude measure \\(\\psi\\) is called the geocentric latitude. In addition to longitude and latitude we can add altitude to define points that are not on the spheroid, and obtain a three dimensional space again. When defining altitude, we need to choose where zero altitude is: on the ellipsoid, or relative to the surface approximating mean sea level (the geoid)? which direction is positive, and which direction is “straight up”: perpendicular to the spheroid surface, or in the direction perpendicular to the surface of the geoid? All these choices may matter, depending on the application area. 2.3 Distances Distances between two points \\(p_i\\) and \\(p_j\\) in Cartesian coordinates are computed as Euclidian distances, in two dimensions by \\[d_{ij} = \\sqrt{(x_i-x_j)^2+(y_i-y_j)^2}\\] with \\(p_i = (x_i,y_i)\\) and in three dimensions by \\[d_{ij} = \\sqrt{(x_i-x_j)^2+(y_i-y_j)^2+(z_i-z_j)^2}\\] with \\(p_i = (x_i,y_i,z_i).\\) These distances represent the length of a straight line between two points \\(i\\) and \\(j\\). For two points on a circle, the length of the arc of two points \\(c_1 = (r,{\\phi}_i)\\) and \\(c_2 = (r, \\phi_2)\\) is \\[s_{ij}=r~|\\phi_1-\\phi_2| = r ~\\theta\\] with \\(\\theta\\) the angle between \\(\\phi_1\\) and \\(\\phi_2\\) in radians. For very small values of \\(\\theta\\), we will have \\(s_{ij} \\approx d_{ij}\\). For two points \\(p_1 = (\\lambda_1,\\phi_1)\\) and \\(p_2 = (\\lambda_2,\\phi_2)\\) on a sphere with radius \\(r&#39;\\), the great circle distance is the arc length between \\(p_1\\) and \\(p_2\\) on the circle that passes through \\(p_1\\) and \\(p_2\\) and has the center of the sphere as its center, and is given by \\(s_{12} = r ~ \\theta_{12}\\) with \\[\\theta_{12} = \\arccos(\\sin \\phi_1 \\cdot \\sin \\phi_2 + \\cos \\phi_1 \\cdot \\cos \\phi_2 \\cdot \\cos(|\\lambda_1-\\lambda_2|))\\] the angle between \\(p_1\\) and \\(p_2\\), in radians. Arc distances between two points on a spheroid are more complicated to compute. Details about the computation method used in package lwgeom (which, in turn, is used by package sf) are given in Karney (2013). 2.4 Bounded spaces All the above assumes pure geometric computations in spaces where there are no unexpected obstructions. When we move through space on a daily basis, we typically find constraints e.g. when we walk through a building when we cycle to work or when we drive to a shop. Distances, in such case, can be made up of a sequence of shortest (straight line or great circle) distances, but follow a more complex path than a straight line. Typical constraints come from road networks, or from the requirement for a certain habitat for movement (water for fish, forest for certain bird species). 2.5 Time When we describe over which “space” a certain phenomenon is characterised, time forms an integral component: nothing exists forever. It is tempting to think that compared to geographical “space”, time is one-dimensional “linear”, and that is often the way we address time, e.g. in R where Date is represented by the number of days since 1970-01-01 (d = as.Date(&quot;1970-02-01&quot;)) #&gt; [1] &quot;1970-02-01&quot; as.numeric(d) #&gt; [1] 31 and POSIXt time by the number of seconds since the start of that date (t = as.POSIXct(&quot;1970-01-02 00:00&quot;, tz = &quot;UTC&quot;)) #&gt; [1] &quot;1970-01-02 UTC&quot; as.numeric(t) #&gt; [1] 86400 In practical cases however, we may be interested in how certain phenomena vary over the day, or over the year, in which case it may be more convenient to represent time as the tuple (year, day-of-year), or (day, time-of-day). When we study for instance traffic patterns, day of week plays a role, in which we may end up with (year, week-of-year, day-of-week, time-of-day) in order to quantify hourly, daily, weekly, and yearly signals. This decomposes time essentially in two or more components, with some of them having a cyclic character. 2.6 Exercises convert the \\((x,y)\\) points \\((10,2)\\), \\((-10,-2)\\), \\((10,-2)\\) and \\((0,10)\\) to polar coordinates convert the polar \\((r,\\phi)\\) points \\((10,45^{\\circ})\\), \\((0,100^{\\circ})\\) and \\((5,359^{\\circ})\\) to Cartesian coordinates assuming the Earth is a sphere with a radius of 6371 km, compute for \\((\\lambda,\\phi)\\) points the great circle distance between \\((10,10)\\) and \\((11,10)\\), between \\((10,80)\\) and \\((11,80)\\), between \\((10,10)\\) and \\((10,11)\\) and between \\((10,80)\\) and \\((10,81)\\) (units: degree). What are the distance units? References "],
["geometries.html", "Chapter 3 Geometries 3.1 Simple feature geometry types 3.2 Simple features in sf 3.3 Tesselations: coverages, rasters 3.4 Networks 3.5 Geometries on the sphere", " Chapter 3 Geometries Having learned how we describe spaces, we can define how geometries can be described in these spaces. This chapter will mostly explain the geometries for simple features, and introduce the three classes sfg, sfc and sf for single geometries, geometry sets, and geometry sets with associated attributes. 3.1 Simple feature geometry types Simple feature geometries are a way to describe the geometries of features. By features we mean things that have a geometry, some time properties, and other attributes. The main application of simple feature geometries is to describe two-dimensional geometries by points, lines, or polygons. The “simple” adjective refers to the fact that the line or polygon geometries are represented by sequences of points connected with straight lines. Simple features access is a standard (Herring 2011, Herring (2010), ISO (2004)) for describing simple feature geometries that includes a class hierarchy a set of operations binary and text encodings We will now discuss the seven most common simple feature geometry types. Although in practice we will most often import spatial data from external sources (files, databases, web services), we will create them here from scratch using simple constructor functions. 3.1.1 The big seven The most commonly used simple features geometries, used to represent a single feature are: type description POINT single point geometry MULTIPOINT set of points LINESTRING single linestring (two or more points connected by straight lines) MULTILINESTRING set of linestrings POLYGON exterior ring with zero or more inner rings, denoting holes MULTIPOLYGON set of polygons GEOMETRYCOLLECTION set of the geometries above Points in a geometry contain at least two coordinates: x and y, in that order. 3.1.2 Valid geometries Valid geometries obey the following properties: linestrings shall not self-intersect polygon rings shall be closed (the last point equals the first) polygon holes (inner rings) shall be inside their exterior ring polygon inner rings shall maximally touch the exterior ring in single points, not over a line a polygon ring shall not repeat its own path If this is not the case, the geometry concerned is not valid. 3.1.3 Z and M In addition to X and Y coordinates, Single points (vertices) of simple feature geometries can have a Z coordinate, denoting altitude, and/or an M value, denoting some “measure” The M attribute shall be a property of the vertex. It sounds attractive to encode a time stamp in it, e.g. to pack trajectories in LINESTRINGs. These become however invalid once the trajectory self-intersects. Both Z and M are found relatively rarely, and software support to do something useful with them is (still) rather rare. 3.1.4 Ten further geometry types There are 10 more geometry types which are more rare, but increasingly find implementation: type description CIRCULARSTRING The CIRCULARSTRING is the basic curve type, similar to a LINESTRING in the linear world. A single segment requires three points, the start and end points (first and third) and any other point on the arc. The exception to this is for a closed circle, where the start and end points are the same. In this case the second point MUST be the center of the arc, ie the opposite side of the circle. To chain arcs together, the last point of the previous arc becomes the first point of the next arc, just like in LINESTRING. This means that a valid circular string must have an odd number of points greated than 1. COMPOUNDCURVE A compound curve is a single, continuous curve that has both curved (circular) segments and linear segments. That means that in addition to having well-formed components, the end point of every component (except the last) must be coincident with the start point of the following component. CURVEPOLYGON Example compound curve in a curve polygon: CURVEPOLYGON(COMPOUNDCURVE(CIRCULARSTRING(0 0,2 0, 2 1, 2 3, 4 3),(4 3, 4 5, 1 4, 0 0)), CIRCULARSTRING(1.7 1, 1.4 0.4, 1.6 0.4, 1.6 0.5, 1.7 1) ) MULTICURVE A MultiCurve is a 1-dimensional GeometryCollection whose elements are Curves, it can include linear strings, circular strings or compound strings. MULTISURFACE A MultiSurface is a 2-dimensional GeometryCollection whose elements are Surfaces, all using coordinates from the same coordinate reference system. CURVE A Curve is a 1-dimensional geometric object usually stored as a sequence of Points, with the subtype of Curve specifying the form of the interpolation between Points SURFACE A Surface is a 2-dimensional geometric object POLYHEDRALSURFACE A PolyhedralSurface is a contiguous collection of polygons, which share common boundary segments TIN A TIN (triangulated irregular network) is a PolyhedralSurface consisting only of Triangle patches. TRIANGLE A Triangle is a polygon with 3 distinct, non-collinear vertices and no interior boundary Note that CIRCULASTRING, COMPOUNDCURVE and CURVEPOLYGON are not described in the SFA standard, but in the SQL-MM part 3 standard. The descriptions above were copied from the PostGIS manual. 3.1.5 Encodings Part of the simple feature standard are two encodings: a text and a binary encoding. The text strings POINT (0 1) and so on indicate text encodings, also known as well-known text (WKT) encodings, of simple feature geometries. They are meant to be human-readable. 3.2 Simple features in sf This section describes the implementation of simple feature geometries in package sf. It will first explain how single simple feature geometries, explained in the previous section, are represented in R objects of class sfg. Next, it will explain how sets of simple feature geometry objects are collected in a list of class sfc. This list acts as a geometry list-column in data.frame objects, of class sf. 3.2.1 sfg: simple feature geometry Point sets are stored as numeric matrix, with 2 (XY), 3 (XYZ or XYM) or 4 (XYZM) columns, with a points in each row. Individual simple feature geometry objects are implemented as: numeric vector for POINT, numeric matrix for MULTIPOINT and LINESTRING list of numeric matrices for MULTILINESTRING and POLYGON list of lists of numeric matrices for MULTIPOLYGON list of (typed) geometries for GEOMETRYCOLLECTION All other geometry types follow this, using the simplest possible option. Note that matrices can have zero points, and lists can have zero elements, in which case we have empty geometries; more about this in section 3.2.1.2. Objects have a class indicating their dimension, type, and a superclass (sfg: simple feature geometry), and have no other attributes than their S3 class: (pt = st_point(c(0,1))) #&gt; POINT (0 1) attributes(pt) #&gt; $class #&gt; [1] &quot;XY&quot; &quot;POINT&quot; &quot;sfg&quot; We see that in addition to sfg the class attribute has two values: XY telling the dimension of the point(s), can also be XYZ, XYM or XYZM POINT revealing the geometry type. Examples of XYZ and XYM and XYZM geometries are found here: system.file(&quot;shape/storms_xyz_feature.shp&quot;, package=&quot;sf&quot;) %&gt;% st_read() #&gt; Reading layer `storms_xyz_feature&#39; from data source `/home/edzer/R/x86_64-pc-linux-gnu-library/3.6/sf/shape/storms_xyz_feature.shp&#39; using driver `ESRI Shapefile&#39; #&gt; Simple feature collection with 71 features and 1 field #&gt; geometry type: LINESTRING #&gt; dimension: XYZ #&gt; bbox: xmin: -102 ymin: 8.3 xmax: 0 ymax: 59.5 #&gt; epsg (SRID): NA #&gt; proj4string: NA system.file(&quot;shape/storms_xyzm_feature.shp&quot;, package=&quot;sf&quot;) %&gt;% # badly named! st_read() #&gt; Reading layer `storms_xyzm_feature&#39; from data source `/home/edzer/R/x86_64-pc-linux-gnu-library/3.6/sf/shape/storms_xyzm_feature.shp&#39; using driver `ESRI Shapefile&#39; #&gt; Simple feature collection with 71 features and 1 field #&gt; geometry type: LINESTRING #&gt; dimension: XYM #&gt; bbox: xmin: -102 ymin: 8.3 xmax: 0 ymax: 59.5 #&gt; epsg (SRID): NA #&gt; proj4string: NA (pzm = st_point(c(1,2,3,4))) #&gt; POINT ZM (1 2 3 4) A MULTIPOINT or a LINESTRING can be created by a matrix (m1 = rbind(c(8, 1), c(2, 5), c(3, 2))) #&gt; [,1] [,2] #&gt; [1,] 8 1 #&gt; [2,] 2 5 #&gt; [3,] 3 2 (mp = st_multipoint(m1)) #&gt; MULTIPOINT (8 1, 2 5, 3 2) (ls = st_linestring(m1)) #&gt; LINESTRING (8 1, 2 5, 3 2) Although these geometries contain the same points, they have entirely different meaning: the point set is a zero-dimensional, the line a one-dimensional geometry: st_dimension(mp) #&gt; [1] 0 st_dimension(ls) #&gt; [1] 1 A MULTILINESTRING can be constructed from a list of matrices, representing vertices: m2 = rbind(c(22,20), c(18, 15)) (mls = st_multilinestring(list(m1, m2))) #&gt; MULTILINESTRING ((8 1, 2 5, 3 2), (22 20, 18 15)) A POLYGON consists of an outer ring, followed by zero or more inner rings that denote holes in the outer ring: (ring1 = rbind(c(0,0), c(4,0), c(4,4), c(0,4), c(0,0))) #&gt; [,1] [,2] #&gt; [1,] 0 0 #&gt; [2,] 4 0 #&gt; [3,] 4 4 #&gt; [4,] 0 4 #&gt; [5,] 0 0 (p1 = st_polygon(list(ring1))) #&gt; POLYGON ((0 0, 4 0, 4 4, 0 4, 0 0)) (ring2 = ring1 + 5) #&gt; [,1] [,2] #&gt; [1,] 5 5 #&gt; [2,] 9 5 #&gt; [3,] 9 9 #&gt; [4,] 5 9 #&gt; [5,] 5 5 (ring3 = (ring1[5:1,] / 4) + 6) #&gt; [,1] [,2] #&gt; [1,] 6 6 #&gt; [2,] 6 7 #&gt; [3,] 7 7 #&gt; [4,] 7 6 #&gt; [5,] 6 6 (p2 = st_polygon(list(ring2, ring3))) #&gt; POLYGON ((5 5, 9 5, 9 9, 5 9, 5 5), (6 6, 6 7, 7 7, 7 6, 6 6)) A MULTIPOLYGON can be constructed as a list of lists of matrices: (mpol = st_multipolygon(list(list(ring1), list(ring2, ring3)))) #&gt; MULTIPOLYGON (((0 0, 4 0, 4 4, 0 4, 0 0)), ((5 5, 9 5, 9 9, 5 9, 5 5), (6 6, 6 7, 7 7, 7 6, 6 6))) And finally, a GEOMETRYCOLLECTION can be constructed from a list of typed geometries: st_geometrycollection(list(pt, mp, ls, mpol)) #&gt; GEOMETRYCOLLECTION (POINT (0 1), MULTIPOINT (8 1, 2 5, 3 2), LINESTRING (8 1, 2 5, 3 2), MULTIPOLYGON (((0 0, 4 0, 4 4, 0 4, 0 0)), ((5 5, 9 5, 9 9, 5 9, 5 5), (6 6, 6 7, 7 7, 7 6, 6 6)))) 3.2.1.1 WKT, WKB encodings By default, package sf prints the same number of digits as R, but this can be manipulated: st_point(c(1/3, 2/3)) #&gt; POINT (0.333 0.667) print(st_point(c(1/3, 2/3)), digits = 16) #&gt; POINT (0.3333333333333333 0.6666666666666666) print(st_point(c(1/3, 2/3)), digits = 3) #&gt; POINT (0.333 0.667) An encoding that is more useful for machine-to-machine communication is well-known binary. An example of a round-trip R \\(\\rightarrow\\) binary \\(\\rightarrow\\) R is (wkb = st_as_binary(st_point(c(1/3, 2/3)))) #&gt; [1] 01 01 00 00 00 55 55 55 55 55 55 d5 3f 55 55 55 55 55 55 e5 3f st_as_sfc(wkb)[[1]] #&gt; POINT (0.333 0.667) Object r is a raw vector, which is little useful in R. Binary conversion is used to communicate geometries to external libraries (GDAL, GEOS, liblwgeom) and spatial databases because it is fast and lossless. Whenever there is a choice, binary encoding should be prefered over text encoding. 3.2.1.2 simple, valid, empty Methods st_is_simple and st_is_valid help detect non-simple and non-valid geometries: st_is_simple(st_linestring(rbind(c(0,0), c(1,1), c(1,0), c(0,1)))) # self-intersects #&gt; [1] FALSE st_is_valid(st_polygon(list(rbind(c(1,1), c(0,0), c(1,1), c(2,2), c(2,1), c(1,1))))) # repeats #&gt; [1] FALSE A very important concept in the feature geometry framework is that of the empty geometry. We can think of an empty geometry as similar to the NA value in R vectors: it is a placeholder, but a usable value is not available. Empty geometries arise naturally when we do geometrical operations (chapter 5), for instance when we want to know where two disjoint geometries coincide: (e = st_intersection(st_point(c(0,0)), st_point(c(1,1)))) #&gt; GEOMETRYCOLLECTION EMPTY It is not entirely clear what the benefit is of having typed empty geometries, but according to the simple feature standard they are. They are detected by st_is_empty(e) #&gt; [1] TRUE 3.2.1.3 Conversion between geometry types Up to the extent that a conversion is feasible, we can convert simple feature geometries using the st_cast generic: methods(st_cast) #&gt; [1] st_cast.CIRCULARSTRING* st_cast.COMPOUNDCURVE* #&gt; [3] st_cast.CURVE* st_cast.GEOMETRYCOLLECTION* #&gt; [5] st_cast.LINESTRING* st_cast.MULTILINESTRING* #&gt; [7] st_cast.MULTIPOINT* st_cast.MULTIPOLYGON* #&gt; [9] st_cast.MULTISURFACE* st_cast.POINT* #&gt; [11] st_cast.POLYGON* st_cast.sf* #&gt; [13] st_cast.sfc* st_cast.sfc_CIRCULARSTRING* #&gt; see &#39;?methods&#39; for accessing help and source code Conversion is required e.g. to be able to plot curved geometries. CURVE, COMPOUNDCURVE and CIRCULARSTRING have st_cast methods to cast them to LINESTRING; MULTISURFACE has an st_cast method to MULTIPOLYGON. An example, needed for plotting, is (ls &lt;- st_as_sfc(&quot;CIRCULARSTRING(0 0,1 0,1 1)&quot;) %&gt;% st_cast(&quot;LINESTRING&quot;)) #&gt; Geometry set for 1 feature #&gt; geometry type: LINESTRING #&gt; dimension: XY #&gt; bbox: xmin: 0 ymin: -0.207 xmax: 1.21 ymax: 1 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; LINESTRING (0 0, 0.0361 -0.0337, 0.0745 -0.0647... plot(ls, axes = TRUE) It is convenient in other cases to analyse the point pattern from a set of vertices in a linestring. However, rbind(c(0,0), c(1,1), c(1,0), c(0,1)) %&gt;% st_linestring() %&gt;% st_cast(&quot;POINT&quot;) #&gt; Warning in st_cast.LINESTRING(., &quot;POINT&quot;): point from first coordinate only #&gt; POINT (0 0) does not what we expect, because it will convert a single geometry into a new single geometry. We can convert to a MULTIPOINT rbind(c(0,0), c(1,1), c(1,0), c(0,1)) %&gt;% st_linestring() %&gt;% st_cast(&quot;POINT&quot;) #&gt; Warning in st_cast.LINESTRING(., &quot;POINT&quot;): point from first coordinate only #&gt; POINT (0 0) but if we want to have a set of points, we need to work with sets (section 3.2.2) first, because we want a set with another cardinality: (p &lt;- rbind(c(0,0), c(1,1), c(1,0), c(0,1)) %&gt;% st_linestring() %&gt;% st_sfc() %&gt;% st_cast(&quot;POINT&quot;)) #&gt; Geometry set for 4 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 0 ymin: 0 xmax: 1 ymax: 1 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; POINT (0 0) #&gt; POINT (1 1) #&gt; POINT (1 0) #&gt; POINT (0 1) 3.2.1.4 GEOMETRYCOLLECTION handling Single features can have a geometry that consists of several subgeometries of different type, held in a GEOMETRYCOLLECTION. This may sound like looking for trouble, but these arise rather naturally when looking for intersections. For instance, the intersection of two LINESTRING geometries may be the combination of a LINESTRING and a POINT. Putting this intersection into a single feature geometry needs a GEOMETRYCOLLECTION. In case we end up with GEOMETRYCOLLECTION objects, the next question is often what to do with them. One thing we can do is extract elements from them: pt &lt;- st_point(c(1, 0)) ls &lt;- st_linestring(matrix(c(4, 3, 0, 0), ncol = 2)) poly1 &lt;- st_polygon(list(matrix(c(5.5, 7, 7, 6, 5.5, 0, 0, -0.5, -0.5, 0), ncol = 2))) poly2 &lt;- st_polygon(list(matrix(c(6.6, 8, 8, 7, 6.6, 1, 1, 1.5, 1.5, 1), ncol = 2))) multipoly &lt;- st_multipolygon(list(poly1, poly2)) j &lt;- st_geometrycollection(list(pt, ls, poly1, poly2, multipoly)) st_collection_extract(j, &quot;POLYGON&quot;) #&gt; Geometry set for 3 features #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: 5.5 ymin: -0.5 xmax: 8 ymax: 1.5 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; MULTIPOLYGON (((5.5 0, 7 0, 7 -0.5, 6 -0.5, 5.5... #&gt; MULTIPOLYGON (((6.6 1, 8 1, 8 1.5, 7 1.5, 6.6 1))) #&gt; MULTIPOLYGON (((5.5 0, 7 0, 7 -0.5, 6 -0.5, 5.5... st_collection_extract(j, &quot;POINT&quot;) #&gt; POINT (1 0) st_collection_extract(j, &quot;LINESTRING&quot;) #&gt; LINESTRING (4 0, 3 0) which sometimes results in a geometry set, sometimes in single geometries. 3.2.2 sfc: sets of geometries Rather than handling geometries individually, we typically handle them as sets. Package sf provides a dedicated class for this, called sfc (for simple feature geometry list column). We can create such a list column with constructor function st_sfc: (sfc = st_sfc(st_point(c(0,1)), st_point(c(-3,2)), crs = 4326)) #&gt; Geometry set for 2 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: -3 ymin: 1 xmax: 0 ymax: 2 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; POINT (0 1) #&gt; POINT (-3 2) The default report from the print method for sfc gives the number of features geometries the feature geometry type (here: POINT) the feature geometry dimension (here: XY) the bounding box for the set the coordinate reference system for the set (epsg and proj4string: see chapter 7.3) the first few geometries, as (abbreviated) WKT The class of the geometry list-column, class(sfc) #&gt; [1] &quot;sfc_POINT&quot; &quot;sfc&quot; is again a combination of a specific class, and a superclass. In addition to a class, the object has further attributes attributes(sfc) %&gt;% names() %&gt;% setdiff(&quot;class&quot;) #&gt; [1] &quot;precision&quot; &quot;bbox&quot; &quot;crs&quot; &quot;n_empty&quot; which are used to record for the whole set: a precision value (section 5.4) the bounding box enclosing all geometries (for x and y) a coordinate reference system (section 7.3) the number of empty geometries contained in the set This means that all these properties are defined for the set, and not for geometries individually. As we’ve seen above, sets of geometries arise when we tear apart compound geometries, as in (p &lt;- rbind(c(0,0), c(1,1), c(1,0), c(0,1)) %&gt;% st_linestring() %&gt;% st_sfc() %&gt;% st_cast(&quot;POINT&quot;)) #&gt; Geometry set for 4 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 0 ymin: 0 xmax: 1 ymax: 1 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; POINT (0 0) #&gt; POINT (1 1) #&gt; POINT (1 0) #&gt; POINT (0 1) Here, st_sfc creates a set of one LINESTRING, and the resulting set has size 4: length(p) #&gt; [1] 4 Going the other way around, we need st_combine to combine geometries into one: p %&gt;% st_combine #&gt; Geometry set for 1 feature #&gt; geometry type: MULTIPOINT #&gt; dimension: XY #&gt; bbox: xmin: 0 ymin: 0 xmax: 1 ymax: 1 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; MULTIPOINT (0 0, 1 1, 1 0, 0 1) p %&gt;% st_combine %&gt;% st_cast(&quot;LINESTRING&quot;) #&gt; Geometry set for 1 feature #&gt; geometry type: LINESTRING #&gt; dimension: XY #&gt; bbox: xmin: 0 ymin: 0 xmax: 1 ymax: 1 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; LINESTRING (0 0, 1 1, 1 0, 0 1) More general, in practice we will almost always work with sets of geometries, because in spatial data we typically associate an observation with a feature, which has a geometry, and we work with sets of observations. sfc objects are lists with each entry being an sfg object: p[[2]] #&gt; POINT (1 1) and we will use these lists as list columns in data.frame or tibble objects to represent simple features with geometries in a list column. These objects are of class sf (section 3.2.3). 3.2.2.1 Feature sets with mixed geometries Sets of simple features also consist of features with heterogeneous geometries. In this case, the geometry type of the set is GEOMETRY: (g = st_sfc(st_point(c(0,0)), st_linestring(rbind(c(0,0), c(1,1))))) #&gt; Geometry set for 2 features #&gt; geometry type: GEOMETRY #&gt; dimension: XY #&gt; bbox: xmin: 0 ymin: 0 xmax: 1 ymax: 1 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; POINT (0 0) #&gt; LINESTRING (0 0, 1 1) These can be filtered by using st_is g %&gt;% st_is(&quot;LINESTRING&quot;) #&gt; [1] FALSE TRUE or, when working with sf objects, st_sf(g) %&gt;% filter(st_is(., &quot;LINESTRING&quot;)) #&gt; Simple feature collection with 1 feature and 0 fields #&gt; geometry type: LINESTRING #&gt; dimension: XY #&gt; bbox: xmin: 0 ymin: 0 xmax: 1 ymax: 1 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; g #&gt; 1 LINESTRING (0 0, 1 1) 3.2.3 sf: geometries with attributes sf objects are tibble or data.frame objects with feature geometries in a list column, and an sf class added: sf = st_sf(sfc) class(sf) #&gt; [1] &quot;sf&quot; &quot;data.frame&quot; Although there is nothing against simply using data.frames or tibbles with sfc list columns, a number of methods have been written for sf objects that make life even more convenient, including plot methods to create maps. In addition to the usual data.frame attributes, sf objects have two more attributes: sf %&gt;% attributes() %&gt;% names() %&gt;% setdiff(c(&quot;row.names&quot;, &quot;class&quot;, &quot;names&quot;)) #&gt; [1] &quot;sf_column&quot; &quot;agr&quot; They are: sf_column: a length one character vector with the name of the (active) geometry list-column. Note that sf objects may contain multiple geometry list-columns, but the one named here is used for all operations, as the “active” geometry. agr: attribute-geometry relationships; this encodes for each of the attributes how it relates to the geometry (in case of a non-point geometry): is it constant throughout the geometry like a soil type, is it an aggregate over the geometry like a population count, or does it identify the geometry like a state name? This is explained in more depth in section 6.1. 3.3 Tesselations: coverages, rasters A common case in spatial data analysis is that an area is split (tesselated) in a number of non-overlapping regions. Although this can be modelled by a sequence of simple feature geometries (polygons), it is hard to guarantee for a set of simple feature polygons that they overlap nowhere, or that there are no gaps between them. More fundamental ways of storing such polygons use a topological model, examples of this are found in geographic information systems like GRASS GIS or ArcGIS. Topological models store every boundary between polygons only once, and register which polygon is on either side of a boundary. A simpler approach, associated with the term raster data, is to tesselate each spatial dimension \\(d\\) into regular cells, formed e.g. by left-closed and right-open intervals \\(d_i\\): \\[\\begin{equation} d_i = d_0 + [~ i \\cdot \\delta, (i+1) \\cdot \\delta~) \\end{equation}\\] with \\(d_0\\) an offset, \\(\\delta\\) the interval (cell or pixel) size, and where the cell index \\(i\\) is an arbitrary but consecutive set of integers. The \\(\\delta\\) value is often taken negative for the \\(y\\)-axis (Northing), indicating that raster row numbers increasing Southwards correspond to \\(y\\)-coordinates increasing Northwards. In arbitrary polygon tesselations, assigning points to polygons when they fall on a boundary shared by two polygons is ambiguous. Using left-closed “[” and right-open “)” intervals in regular tesselations removes this ambiguity. Tesselating the time dimension in this way is very common, and reflects the implicit assumption underlying time series packages such as xts in R. Different models can be combined: one could use simple feature polygons to tesselate space, and combine this with a regular tesselation of time in order to cover a space-time vector datacube. Raster data and data cubes are discussed in chapter 4. 3.4 Networks Spatial networks are typically composed of linear (LINESTRING) elements, but possess further topological properties describing the network coherence: start and endpoints of a linestring may be connected to other linestring start or end points, forming a set of nodes and edges edges may be directed, and allow for connections (flow, transport) in only one way. Several R packages (osmar, stplanr) have (limited) functionality available for constructing network objects, and working with them, e.g. computing shortest or fastest routes through a network. 3.5 Geometries on the sphere Geometries on the sphere are geometries made from geodetic coordinates. The easiest of these concern small regions near the equator not covering the date line, because in that case we can ignore all problems and still do a good job. Reality is nastier. The concept of a bounding box, defined from the coordinate ranges breaks easily down, e.g. when crossing the date line: pts = rbind(c(-179,0), c(179,0), c(179,1), c(-179,1), c(-179,0)) date_line = st_sfc(st_polygon(list(pts)), crs = 4326) st_bbox(date_line) %&gt;% st_as_sfc() #&gt; Geometry set for 1 feature #&gt; geometry type: POLYGON #&gt; dimension: XY #&gt; bbox: xmin: -179 ymin: 0 xmax: 179 ymax: 1 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; POLYGON ((-179 0, 179 0, 179 1, -179 1, -179 0)) or when a polygon contains one of the poles: pts = rbind(c(0,89), c(120,89), c(240,89), c(0,89)) pole = st_sfc(st_polygon(list(pts)), crs = 4326) st_bbox(pole) %&gt;% st_as_sfc() #&gt; Geometry set for 1 feature #&gt; geometry type: POLYGON #&gt; dimension: XY #&gt; bbox: xmin: 0 ymin: 89 xmax: 240 ymax: 89 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; POLYGON ((0 89, 240 89, 240 89, 0 89, 0 89)) where we see that in both cases the st_bbox implied polygon does not cover the area intended. 3.5.1 Straight lines? The simple feature model assumes that linestrings and polygons are formed of points connected by straight lines. When however representing the Earth surface, what does a straight line mean? The simple feature standard does not help much, here: it assumes Cartesian space. Technically speaking, a straight line between two points on a sphere exists, but it crosses the sphere, which is not very practical in most cases. The most common case is to use great circle segments to connect points: the shortest path that follows the surface of the sphere or ellipsoid. This means that (with longitude latitude coordinates) the line between POINT(0 50) and POINT(10 50) does not cross POINT(5 50). It also means that the line between points on opposite sides of the sphere is ambiguous. Also, the direction of a great circle segment, when defined as the angle it has with meridians, is not constant. 3.5.2 Ring direction for polygons The simple feature standard is not conclusive about the direction of points in a ring. It points out that exterior rings should be counter clockwise, when seen from above, and interior rings (holes) clockwise, but for instance st_is_valid does not invalidate clockwise exterior rings: st_is_valid(st_polygon(list(rbind(c(0,0), c(0,1), c(1,1), c(0,0))))) #&gt; [1] TRUE This may have several reasons: a lot of data may come with wrong ring directions, and the distinction between exterior and interior rings is already unambiguous by their order: the first is exterior, anything following is interior. On the sphere, any polygon divides the sphere surface in two finite areas, meaning there is no longer an unambiguous “inside” vs. “outside”: does the polygon with longitude latitude coordinates POLYGON((0 0, 120 0, 240 0, 0 0)) denote the northern or the southern hemisphere? One can still go two directions here: assume that in practice polygons never divide the Earth in two equal halves, and take the smaller area as the “inside” decide strongly about ring direction, e.g. counter-clockwise (following the ring, standing on the Earth, the left-side of the ring denotes the polygon interior) Package sf comes with a large number of functions that work both for projected (Cartesian) data as for data defined in spherical coordinates. Whenever it makes assumptions of Cartesian coordinates for spherical coordinates it emits a warning. This is discussed further in section 5.6. References "],
["raster.html", "Chapter 4 Raster and vector datacubes 4.1 Package stars 4.2 Raster data 4.3 Vector Datacubes 4.4 Exercises", " Chapter 4 Raster and vector datacubes Array data are data where values are indexed along multiple array dimensions. Raster and vector datacubes refer to array data, where one or more of the dimensions refer to space, and often other dimensions refer to time. 4.1 Package stars Athough package sp has always had limited support for raster data, over the last decade R package raster has clearly been dominant as the prime package for powerful, flexible and scalable raster analysis. Its data model is that of a 2D raster, or a set of raster layers (a “raster stack”). This follows the classical static GIS world view, where the world is modelled as a set of layers, each representing a different theme. A lot of data available today however is dynamic, and comes as time series of rasters for different themes. A raster stack does not meaningfully reflect this, requiring the user to do shadow book keeping of which layer represents what. Also, the raster package does an excellent job in scaling computations up to datasizes no larger than the local storage (the computer’s hard drives). Recent datasets however, including satellite imagery, climate model or weather forecasting data, often no longer fit in local storage. Package spacetime addresses the analysis of time series of vector geometries or raster grid cells, but does not extend to higher-dimensional arrays. Here, we introduce a new package for raster analysis, called stars (for scalable, spatiotemporal tidy arrays) that allows for representing dynamic raster stacks, in addition to regular grids handles rotated, sheared, rectilinear and curvilinear rasters, provides a tight integration with package sf, follows the tidyverse design principles, aims at being scalable, also beyond local disk size, also handles array data with non-raster spatial dimensions, the vector datacubes, provides further integration of novel features in the GDAL library than other R packages have given so far. Vector data cubes include for instance time series for simple features, or spatial graph data such as origin-destination matrices. The wider concept of spatial vector and raster data cubes is explained in section 4.3 4.2 Raster data As introduced in section 3.3, raster data are spatial datasets where observations are aligned on a regular grid usually with square grid cells (in some coordinate reference system, chapter 7). Raster datasets are used often to represent spatially continuously varying phenomena such as temperature or elevation, and also for observed imagery for instance obtained from satellites. 4.2.1 Reading and writing raster data Raster data typically are read from a file. We read an example file of a regular, non-rotated grid from the package stars: tif = system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;) library(stars) x = read_stars(tif) The dataset contains (a section of) a Landsat 7 scene, with the 6 30m-resolution bands (bands 1-5 and 7) for a region covering the city of Olinda, Brazil. A short summary of the data is given by x #&gt; stars object with 3 dimensions and 1 attribute #&gt; attribute(s): #&gt; L7_ETMs.tif #&gt; Min. : 1.0 #&gt; 1st Qu.: 54.0 #&gt; Median : 69.0 #&gt; Mean : 68.9 #&gt; 3rd Qu.: 86.0 #&gt; Max. :255.0 #&gt; dimension(s): #&gt; from to offset delta refsys point values #&gt; x 1 349 288776 28.5 +proj=utm +zone=25 +south... FALSE NULL [x] #&gt; y 1 352 9120761 -28.5 +proj=utm +zone=25 +south... FALSE NULL [y] #&gt; band 1 6 NA NA NA NA NULL where we see the offset, cellsize, coordinate reference system, and dimensions. The object x is a simple list of length one, holding a three-dimensional array: length(x) #&gt; [1] 1 class(x[[1]]) #&gt; [1] &quot;array&quot; dim(x[[1]]) #&gt; x y band #&gt; 349 352 6 and in addition holds an attribute with a dimensions table with all the metadata required to know what the array values refer to, obtained by st_dimensions(x) #&gt; from to offset delta refsys point values #&gt; x 1 349 288776 28.5 +proj=utm +zone=25 +south... FALSE NULL [x] #&gt; y 1 352 9120761 -28.5 +proj=utm +zone=25 +south... FALSE NULL [y] #&gt; band 1 6 NA NA NA NA NULL We can get the spatial extent of the array by st_bbox(x) #&gt; xmin ymin xmax ymax #&gt; 288776 9110729 298723 9120761 Raster data can be written to local disk using write_stars: tf = tempfile(fileext=&quot;.tif&quot;) write_stars(x, tf) where the format (in this case, GeoTIFF) is derived from the file extension. As for simple features, reading and writing uses the GDAL library; the list of available drivers for raster data is obtained by st_drivers(&quot;raster&quot;) 4.2.2 Plotting raster data We can use the base plot method for stars objects, shown in figure 4.1. plot(x) Figure 4.1: 6 30m Landsat bands downsampled to 90m for Olinda, Br. The default color scale uses grey tones, and stretches this such that color breaks correspond to data quantiles over all bands. A more familiar view is the rgb or false color composite: par(mfrow = c(1, 2)) plot(x, rgb = c(3,2,1), reset = FALSE, main = &quot;RGB&quot;) # rgb plot(x, rgb = c(4,3,2), main = &quot;False color (NIR-R-G)&quot;) # false color Figure 4.2: two RGB composites 4.2.3 Analysing raster data Element-wise mathematical operations on stars objects are just passed on to the arrays. This means that we can call functions and create expressions: log(x) #&gt; stars object with 3 dimensions and 1 attribute #&gt; attribute(s): #&gt; L7_ETMs.tif #&gt; Min. :0.00 #&gt; 1st Qu.:3.99 #&gt; Median :4.23 #&gt; Mean :4.12 #&gt; 3rd Qu.:4.45 #&gt; Max. :5.54 #&gt; dimension(s): #&gt; from to offset delta refsys point values #&gt; x 1 349 288776 28.5 +proj=utm +zone=25 +south... FALSE NULL [x] #&gt; y 1 352 9120761 -28.5 +proj=utm +zone=25 +south... FALSE NULL [y] #&gt; band 1 6 NA NA NA NA NULL x + 2 * log(x) #&gt; stars object with 3 dimensions and 1 attribute #&gt; attribute(s): #&gt; L7_ETMs.tif #&gt; Min. : 1.0 #&gt; 1st Qu.: 62.0 #&gt; Median : 77.5 #&gt; Mean : 77.1 #&gt; 3rd Qu.: 94.9 #&gt; Max. :266.1 #&gt; dimension(s): #&gt; from to offset delta refsys point values #&gt; x 1 349 288776 28.5 +proj=utm +zone=25 +south... FALSE NULL [x] #&gt; y 1 352 9120761 -28.5 +proj=utm +zone=25 +south... FALSE NULL [y] #&gt; band 1 6 NA NA NA NA NULL or even mask out certain values: x2 = x x2[x &lt; 50] = NA x2 #&gt; stars object with 3 dimensions and 1 attribute #&gt; attribute(s): #&gt; L7_ETMs.tif #&gt; Min. : 50 #&gt; 1st Qu.: 64 #&gt; Median : 75 #&gt; Mean : 79 #&gt; 3rd Qu.: 90 #&gt; Max. :255 #&gt; NA&#39;s :149170 #&gt; dimension(s): #&gt; from to offset delta refsys point values #&gt; x 1 349 288776 28.5 +proj=utm +zone=25 +south... FALSE NULL [x] #&gt; y 1 352 9120761 -28.5 +proj=utm +zone=25 +south... FALSE NULL [y] #&gt; band 1 6 NA NA NA NA NULL or un-mask areas: x2[is.na(x2)] = 0 x2 #&gt; stars object with 3 dimensions and 1 attribute #&gt; attribute(s): #&gt; L7_ETMs.tif #&gt; Min. : 0 #&gt; 1st Qu.: 54 #&gt; Median : 69 #&gt; Mean : 63 #&gt; 3rd Qu.: 86 #&gt; Max. :255 #&gt; dimension(s): #&gt; from to offset delta refsys point values #&gt; x 1 349 288776 28.5 +proj=utm +zone=25 +south... FALSE NULL [x] #&gt; y 1 352 9120761 -28.5 +proj=utm +zone=25 +south... FALSE NULL [y] #&gt; band 1 6 NA NA NA NA NULL Dimension-wise, we can apply functions to array dimensions of stars objects just like apply does this to matrices. For instance, to compute for each pixel the mean of the 6 band values we can do st_apply(x, c(&quot;x&quot;, &quot;y&quot;), mean) #&gt; stars object with 2 dimensions and 1 attribute #&gt; attribute(s): #&gt; mean #&gt; Min. : 25.5 #&gt; 1st Qu.: 53.3 #&gt; Median : 68.3 #&gt; Mean : 68.9 #&gt; 3rd Qu.: 82.0 #&gt; Max. :255.0 #&gt; dimension(s): #&gt; from to offset delta refsys point values #&gt; x 1 349 288776 28.5 +proj=utm +zone=25 +south... FALSE NULL [x] #&gt; y 1 352 9120761 -28.5 +proj=utm +zone=25 +south... FALSE NULL [y] A more meaningful function would e.g. compute the NDVI (normalized differenced vegetation index): ndvi = function(x) (x[4]-x[3])/(x[4]+x[3]) st_apply(x, c(&quot;x&quot;, &quot;y&quot;), ndvi) #&gt; stars object with 2 dimensions and 1 attribute #&gt; attribute(s): #&gt; ndvi #&gt; Min. :-0.753 #&gt; 1st Qu.:-0.203 #&gt; Median :-0.069 #&gt; Mean :-0.064 #&gt; 3rd Qu.: 0.187 #&gt; Max. : 0.587 #&gt; dimension(s): #&gt; from to offset delta refsys point values #&gt; x 1 349 288776 28.5 +proj=utm +zone=25 +south... FALSE NULL [x] #&gt; y 1 352 9120761 -28.5 +proj=utm +zone=25 +south... FALSE NULL [y] Alternatively, to compute for each band the mean of the whole image we can do as.data.frame(st_apply(x, c(&quot;band&quot;), mean)) #&gt; band mean #&gt; 1 1 79.1 #&gt; 2 2 67.6 #&gt; 3 3 64.4 #&gt; 4 4 59.2 #&gt; 5 5 83.2 #&gt; 6 6 60.0 which is so small it can be printed here as a data.frame. In these two examples, entire dimensions disappear. Sometimes, this does not happen; we can for instance compute the three quartiles for each band st_apply(x, c(&quot;band&quot;), quantile, c(.25, .5, .75)) #&gt; stars object with 2 dimensions and 1 attribute #&gt; attribute(s): #&gt; L7_ETMs.tif #&gt; Min. : 32.0 #&gt; 1st Qu.: 60.8 #&gt; Median : 66.5 #&gt; Mean : 69.8 #&gt; 3rd Qu.: 78.8 #&gt; Max. :112.0 #&gt; dimension(s): #&gt; from to offset delta refsys point values #&gt; quantile 1 3 NA NA NA NA 25%,...,75% #&gt; band 1 6 NA NA NA NA NULL and see that this creates a new dimension, quantile, with three values. Alternatively, the three quantiles over the 6 bands for each pixel are obtained by st_apply(x, c(&quot;x&quot;, &quot;y&quot;), quantile, c(.25, .5, .75)) #&gt; stars object with 3 dimensions and 1 attribute #&gt; attribute(s): #&gt; L7_ETMs.tif #&gt; Min. : 4.0 #&gt; 1st Qu.: 55.0 #&gt; Median : 69.2 #&gt; Mean : 67.2 #&gt; 3rd Qu.: 81.2 #&gt; Max. :255.0 #&gt; dimension(s): #&gt; from to offset delta refsys point #&gt; quantile 1 3 NA NA NA NA #&gt; x 1 349 288776 28.5 +proj=utm +zone=25 +south... FALSE #&gt; y 1 352 9120761 -28.5 +proj=utm +zone=25 +south... FALSE #&gt; values #&gt; quantile 25%,...,75% #&gt; x NULL [x] #&gt; y NULL [y] 4.2.4 Raster varieties: rectilinear, curvilinear Besides the regular raster with square cells and axes aligned with \\(x\\) and \\(y\\), several other raster types exist. The ones supported by package stars are shown in figure 4.3. Figure 4.3: raster types supported by the stars package The data model vignette of the package explains the models in detail, and points out how they can be constructed. There are several reasons why non-regular rasters occur. For one, when the data is Earth-bound, a regular raster does not fit the Earth surface, which is curved. Other reasons are: when we convert or transform a regular raster data into another coordinate reference system, it will become curvilinear unless we resample; resampling always goes at the cost of some loss of data and is not reversible. observation may lead to irregular rasters; e.g. for satellite swaths, we may have a regular raster in the direction of the satellite (not aligned with \\(x\\) or \\(y\\)), and rectilinear perpendicular to that (e.g. if the sensor discretizes the viewing angle in equal sections) 4.2.5 Handling large raster datasets A common challenge with raster datasets is not only that they come in large files (single Sentinel-2 tiles are around 1 Gb), but that many of these files, potentially thousands, are needed to address the area and time period of interest. At time of writing this, the Copernicus program which runs all Sentinel satellites publishes 160 Tb of images per day. This means that a classic pattern in using R, consisting of downloading data to local disc, loading the data in memory, analysing it is not going to work. Cloud-based Earth Observation processing platforms like Google Earth Engine (Gorelick et al. 2017) or Sentinel Hub recognize this and let users work with datasets up to 20 petabyte rather easily and with a great deal of interactivity. They share the following properties: computations are posponed as long as possible (lazy evaluation), only the data you ask for are being computed and returned, and nothing more, storing intermediate results is avoided in favour of on-the-fly computations, maps with useful results are generated and shown quickly to allow for interactive model development. This is similar to the dbplyr interface to databases and cloud-based analytics environments, but differs in the aspect of what we want to see quickly: rather than the first \\(n\\) records, we want a quick overview of the results, in the form of a map covering the whole area, or part of it, but at screen resolution rather than native (observation) resolution. If for instance we want to “see” results for the United States on screen with 1000 x 1000 pixels, we only need to compute results for this many pixels, which corresponds roughly to data on a grid with 3000 m x 3000 m grid cells. For Sentinel-2 data with 10 m resolution, this means we can subsample with a factor 300, giving 3 km x 3 km resolution. Processing, storage and network requirements then drop a factor \\(300^2 \\approx 10^5\\), compared to working on the native 10 m x 10 m resolution. On the platforms mentioned, zooming in the map triggers further computations on a finer resolution and smaller extent. A simple optimisation that follows these lines is how stars’ plot method works: in case of plotting large rasters, it subsamples the array before it plots, drastically saving time. The degree of subsampling is derived from the plotting region size and the plotting resolution (pixel density). For vector devices, such as pdf, R sets plot resolution to 75 dpi, corresponding to 0.3 mm per pixel. Enlarging plots may reveal this, but replotting to an enlarged devices will create a plot at target density. 4.2.6 stars proxy objects To handle datasets that are too large to fit in memory, stars provides stars_proxy objects. To demonstrate its use, we will use the starsdata package, an R data package with larger datasets (around 1 Gb total). It can be installed by install.packages(&quot;starsdata&quot;, repos = &quot;http://pebesma.staff.ifgi.de&quot;, type = &quot;source&quot;) We can “load” a Sentinel-2 image from it by granule = system.file(&quot;sentinel/S2A_MSIL1C_20180220T105051_N0206_R051_T32ULE_20180221T134037.zip&quot;, package = &quot;starsdata&quot;) file.size(granule) #&gt; [1] 7.69e+08 base_name = strsplit(basename(granule), &quot;.zip&quot;)[[1]] s2 = paste0(&quot;SENTINEL2_L1C:/vsizip/&quot;, granule, &quot;/&quot;, base_name, &quot;.SAFE/MTD_MSIL1C.xml:10m:EPSG_32632&quot;) (p = read_stars(s2, proxy = TRUE)) #&gt; stars_proxy object with 1 attribute in file: #&gt; $`MTD_MSIL1C.xml:10m:EPSG_32632` #&gt; [1] &quot;SENTINEL2_L1C:/vsizip//home/edzer/R/x86_64-pc-linux-gnu-library/3.6/starsdata/sentinel/S2A_MSIL1C_20180220T105051_N0206_R051_T32ULE_20180221T134037.zip/S2A_MSIL1C_20180220T105051_N0206_R051_T32ULE_20180221T134037.SAFE/MTD_MSIL1C.xml:10m:EPSG_32632&quot; #&gt; #&gt; dimension(s): #&gt; from to offset delta refsys point values #&gt; x 1 10980 3e+05 10 +proj=utm +zone=32 +datum... NA NULL [x] #&gt; y 1 10980 6e+06 -10 +proj=utm +zone=32 +datum... NA NULL [y] #&gt; band 1 4 NA NA NA NA NULL object.size(p) #&gt; 7336 bytes and we see that this does not actually load any of the pixel values, but keeps the reference to the dataset and fills the dimensions table. (The convoluted s2 name is needed to point GDAL to the right file inside the .zip file containing 115 files in total). The idea of a proxy object is that we can build expressions like p2 = p * 2 but that the computations for this are postponed. Only when we really need the data, e.g. because we want to plot it, is p * 2 evaluated. We need data when either we want to plot data, or we want to write an object to disk, with write_stars, or we want to explicitly load an object in memory, with st_as_stars In case the entire object does not fit in memory, plot and write_stars choose different strategies to deal with this: plot fetches only the pixels that can be seen, rather than all pixels available, and write_stars reads, processes, and writes data chunk by chunk. Downsampling and chunking is implemented for spatially dense images, not e.g. for dense time series, or other dense dimensions. As an example, plot(p) only fetches the pixels that can be seen on the plot device, rather than the 10980 x 10980 pixels available in each band. The downsampling ratio taken is floor(sqrt(prod(dim(p)) / prod(dev.size(&quot;px&quot;)))) #&gt; [1] 19 meaning that for every 19 x 19 sub-image in the original image, only one pixel is read, and plotted. This value is still a bit too high as it ignores the white space and space for the key on the plotting device. 4.2.7 Operations on proxy objects A few dedicated methods are available for stars_proxy objects: methods(class = &quot;stars_proxy&quot;) #&gt; [1] [ adrop aggregate aperm #&gt; [5] c dim Math merge #&gt; [9] Ops plot predict print #&gt; [13] split st_apply st_as_stars st_crop #&gt; [17] st_redimension write_stars #&gt; see &#39;?methods&#39; for accessing help and source code We have seen plot and print in action; dim reads out the dimension from the dimensions metadata table. The three methods that actually fetch data are st_as_stars, plot and write_stars. st_as_stars reads the actual data into a stars object, its argument downsample controls the downsampling rate. plot does this too, choosing an appropriate downsample value from the device resolution, and plots the object. write_stars writes a star_proxy object to disc. All other methods for stars_proxy objects do not actually operate on the raster data but add the operations to a to do list, attached to the object. Only when actual raster data are fetched, e.g. by calling plot or st_as_stars, the commands in this list are executed. st_crop limits the extent (area) of the raster that will be read. c combines stars_proxy objects, but still doesn’t read any data. adrop drops empty dimensions, aperm changes dimension order. write_stars reads and processes its input chunk-wise; it has an argument chunk_size that lets users control the size of spatial chunks. 4.3 Vector Datacubes Data cubes are multi-dimensional array data, where array dimensions are meaningfully related to categorical or continuous variables that may include space and time (Lu, Appel, and Pebesma 2018). We have seen raster data cubes so far, e.g. raster data naturally fit in two-dimensional arrays, multi-spectral raster data fit in three-dimensional arrays (cubes), and time series of multi-spectral raster data fit in four-dimensional arrays (hyper-cubes). Besides Earth Observation/satellite imagery data, a large class of datacubes come from modelling data, e.g. from oceanographic, meteorologic or climate models, where dimensions may include latitude and longitude altitude, or depth pressure level (substituting altitude) time time to forecast, in addition to time when a forecast was made we can add to this as an additional dimension variable of interest (pressure, temperature, humidity, wind speed, salinity, …) when we accept that categorical variables also “take” a dimension. The alternative would be to consider these as “fields”, or “attributes” of array records. Being able to swap dimensions to attributes flexibly and vice-versa leads to powerful analysis, as e.g. shown by the powerful array database SciDB (Brown 2010). We go from raster data cubes to vector data cubes if we replace the two or three raster dimensions with one dimension listing a set of feature geometries (points, lines or polygons). One example would be air quality data, where we could have \\(PM_{10}\\) measurements for a set of monitoring stations, and a sequence of time intervals aligned in a vector data cube. Another example would be demographic or epidemiological data, where we have a time series of (population, disease) counts, with number of persons by region, for \\(n\\) regions by age class, for \\(m\\) age classes, and by year, for \\(p\\) years. which forms an array with \\(n m p\\) elements. R has strong native support for arbitrarily dimensioned arrays, and we can get the value for year \\(i\\), age class \\(j\\) and year \\(k\\) from array a by a[i,j,k] and e.g. the sub-array for age class \\(j\\) by a[,j,] Thinking along the classical GIS lines, where we would have either raster or vector data, one is left with the question what to do when we have a raster time series data cube (e.g. a climate model forecast) and want to obtain a vector time series data cube with aggregates of the model forecast over polygons, as time series. For spatial data science, support of vector and raster data cubes is extremely useful, because many variables are both spatially and temporaly varying, and because we often want to either change dimensions or aggregate them out, but in a fully flexible manner and order. Examples of changing dimensions are interpolating air quality measurements to values on a regular grid (raster) estimating density maps from points or lines, e.g. with the number of flights passing by per week within a range of 1 km aggregating climate model predictions to summary indicators for administrative regions combining Earth observation data from different sensors, e.g. Modis (250 m pixels, every 16 days) with Sentinel-2 (10 m, every 5 days). Examples of aggregating one ore more full dimensions are assessments of which air quality monitoring stations indicate unhealthy conditions which region has the highest increase in disease incidence global warming (e.g. in degrees per year) 4.3.1 Example: aggregating air quality time series Air quality data from package spacetime were obtained from the airBase European air quality data base. Downloaded were daily average PM\\(_{10}\\) values for rural background stations in Germany, 1998-2009. We can create a stars object from the air matrix, the dates Date vector and the stations SpatialPoints objects by library(spacetime) #&gt; Registered S3 method overwritten by &#39;xts&#39;: #&gt; method from #&gt; as.zoo.xts zoo data(air) # this loads several datasets in .GlobalEnv dim(air) #&gt; space time #&gt; 70 4383 d = st_dimensions(station = st_as_sfc(stations), time = dates) (aq = st_as_stars(list(PM10 = air), dimensions = d)) #&gt; stars object with 2 dimensions and 1 attribute #&gt; attribute(s): #&gt; PM10 #&gt; Min. : 0 #&gt; 1st Qu.: 10 #&gt; Median : 15 #&gt; Mean : 18 #&gt; 3rd Qu.: 22 #&gt; Max. :274 #&gt; NA&#39;s :157659 #&gt; dimension(s): #&gt; from to offset delta refsys point #&gt; station 1 70 NA NA +proj=longlat +datum=WGS8... TRUE #&gt; time 1 4383 1998-01-01 1 days Date FALSE #&gt; values #&gt; station POINT (9.59 53.7),...,POINT (9.45 49.2) #&gt; time NULL We can see from figure 4.4 that the time series are quite long, but also have large missing value gaps. Figure 4.5 shows the spatial distribution measurement stations and mean PM\\(_{10}\\) values. image(aperm(log(aq), 2:1), main = &quot;NA pattern (white) in PM10 station time series&quot;) Figure 4.4: space-time diagram of PM\\(_{10}\\) measurements by time and station plot(st_as_sf(st_apply(aq, 1, mean, na.rm = TRUE)), reset = FALSE, pch = 16, ylim = st_bbox(DE)[c(2,4)]) plot(DE, add=TRUE) Figure 4.5: locations of PM\\(_{10}\\) measurement stations, showing mean values We can now aggregate these station time series to area means, mostly as a simple exercise. For this, we use the aggregate method for stars objects (a = aggregate(aq, st_as_sf(DE_NUTS1), mean, na.rm = TRUE)) #&gt; although coordinates are longitude/latitude, st_intersects assumes that they are planar #&gt; although coordinates are longitude/latitude, st_intersects assumes that they are planar #&gt; stars object with 2 dimensions and 1 attribute #&gt; attribute(s): #&gt; PM10 #&gt; Min. : 1 #&gt; 1st Qu.: 11 #&gt; Median : 15 #&gt; Mean : 18 #&gt; 3rd Qu.: 22 #&gt; Max. :172 #&gt; NA&#39;s :25679 #&gt; dimension(s): #&gt; from to offset delta refsys point #&gt; geometry 1 16 NA NA +proj=longlat +datum=WGS8... FALSE #&gt; time 1 4383 1998-01-01 1 days Date FALSE #&gt; values #&gt; geometry MULTIPOLYGON (((9.65 49.8, ...,...,MULTIPOLYGON (((10.8 51.6, ... #&gt; time NULL and we can now for instance show the maps for six arbitrarily chosen days (figure 4.6), library(tidyverse) a %&gt;% filter(time &gt;= &quot;2008-01-01&quot;, time &lt; &quot;2008-01-07&quot;) %&gt;% plot(key.pos = 4) Figure 4.6: areal mean PM\\(_{10}\\) values, for six days or a time series of mean values for a single state (figure 4.7). suppressPackageStartupMessages(library(xts)) plot(as.xts(a)[,4], main = DE_NUTS1$NAME_1[4]) Figure 4.7: areal mean PM\\(_{10}\\) values, for six days 4.3.2 Example: Bristol origin-destination datacube The data used for this example come from (Lovelace, Nowosad, and Muenchow 2019), and concern origin-destination (OD) counts: the number of persons going from region A to region B, by transportation mode. We have feature geometries for the 102 origin and destination regions, shown in figure 4.8. library(spDataLarge) plot(st_geometry(bristol_zones), axes = TRUE, graticule = TRUE) plot(st_geometry(bristol_zones)[33], col = &#39;red&#39;, add = TRUE) Figure 4.8: Origin destination data zones for Bristol, UK, with zone 33 (E02003043) colored red and the OD counts come in a table with OD pairs as records, and transportation mode as variables: head(bristol_od) #&gt; # A tibble: 6 x 7 #&gt; o d all bicycle foot car_driver train #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 E02002985 E02002985 209 5 127 59 0 #&gt; 2 E02002985 E02002987 121 7 35 62 0 #&gt; 3 E02002985 E02003036 32 2 1 10 1 #&gt; 4 E02002985 E02003043 141 1 2 56 17 #&gt; 5 E02002985 E02003049 56 2 4 36 0 #&gt; 6 E02002985 E02003054 42 4 0 21 0 We see that many combinations of origin and destination are implicit zeroes, otherwise these two numbers would have been the same: nrow(bristol_zones)^2 #&gt; [1] 10404 nrow(bristol_od) #&gt; [1] 2910 We will form a three-dimensional vector datacube with origin, destination and transportation mode as dimensions. For this, we first “tidy” the bristol_od table to have origin (o), destination (d), transportation mode (mode), and count (n) as variables, using gather: # create O-D-mode array: bristol_tidy &lt;- bristol_od %&gt;% select(-all) %&gt;% gather(&quot;mode&quot;, &quot;n&quot;, -o, -d) head(bristol_tidy) #&gt; # A tibble: 6 x 4 #&gt; o d mode n #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 E02002985 E02002985 bicycle 5 #&gt; 2 E02002985 E02002987 bicycle 7 #&gt; 3 E02002985 E02003036 bicycle 2 #&gt; 4 E02002985 E02003043 bicycle 1 #&gt; 5 E02002985 E02003049 bicycle 2 #&gt; 6 E02002985 E02003054 bicycle 4 Next, we form the three-dimensional array a, filled with zeroes: od = bristol_tidy %&gt;% pull(&quot;o&quot;) %&gt;% unique nod = length(od) mode = bristol_tidy %&gt;% pull(&quot;mode&quot;) %&gt;% unique nmode = length(mode) a = array(0L, c(nod, nod, nmode), dimnames = list(o = od, d = od, mode = mode)) We see that the dimensions are named with the zone names (o, d) and the transportation mode name (mode). Every row of bristol_tidy denotes an array entry, and we can use this to to fill the non-zero entries of the bristol_tidy table with their appropriate value (n): a[as.matrix(bristol_tidy[c(&quot;o&quot;, &quot;d&quot;, &quot;mode&quot;)])] = bristol_tidy$n To be sure that there is not an order mismatch between the zones in bristol_zones and the zone names in bristol_tidy, we can get the right set of zones by: order = match(od, bristol_zones$geo_code) # it happens this equals 1:102 zones = st_geometry(bristol_zones)[order] (It happens that the order is already correct, but it is good practice to not assume this). Next, with zones and modes we can create a stars dimensions object: library(stars) (d = st_dimensions(o = zones, d = zones, mode = mode)) #&gt; from to offset delta refsys point #&gt; o 1 102 NA NA +proj=longlat +datum=WGS8... FALSE #&gt; d 1 102 NA NA +proj=longlat +datum=WGS8... FALSE #&gt; mode 1 4 NA NA NA FALSE #&gt; values #&gt; o MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,... #&gt; d MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,... #&gt; mode bicycle,...,train and finally build or stars object from a and d: (odm = st_as_stars(list(N = a), dimensions = d)) #&gt; stars object with 3 dimensions and 1 attribute #&gt; attribute(s): #&gt; N #&gt; Min. : 0 #&gt; 1st Qu.: 0 #&gt; Median : 0 #&gt; Mean : 5 #&gt; 3rd Qu.: 0 #&gt; Max. :1296 #&gt; dimension(s): #&gt; from to offset delta refsys point #&gt; o 1 102 NA NA +proj=longlat +datum=WGS8... FALSE #&gt; d 1 102 NA NA +proj=longlat +datum=WGS8... FALSE #&gt; mode 1 4 NA NA NA FALSE #&gt; values #&gt; o MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,... #&gt; d MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,... #&gt; mode bicycle,...,train We can take a single slice through from this three-dimensional array, e.g. for zone 33 (figure 4.8), by odm[,,33], and plot it: plot(odm[,,33] + 1, logz = TRUE) #&gt; Warning in st_as_sf.stars(x): working on the first sfc dimension only Subsetting this way, we take all attributes (there is only one: N) since the first argument is empty, we take all origin regions (second argument empty), we take destination zone 33 (third argument), and all transportation modes (fourth argument empty, or missing). Why plotted this particular zone because it has the most travelers as its destination. We can find this out by summing all origins and travel modes by destination: d = st_apply(odm, 2, sum) which.max(d[[1]]) #&gt; [1] 33 Other aggregations we can carry out include: total transportation by OD (102 x 102): st_apply(odm, 1:2, sum) #&gt; stars object with 2 dimensions and 1 attribute #&gt; attribute(s): #&gt; sum #&gt; Min. : 0 #&gt; 1st Qu.: 0 #&gt; Median : 0 #&gt; Mean : 19 #&gt; 3rd Qu.: 19 #&gt; Max. :1434 #&gt; dimension(s): #&gt; from to offset delta refsys point #&gt; o 1 102 NA NA +proj=longlat +datum=WGS8... FALSE #&gt; d 1 102 NA NA +proj=longlat +datum=WGS8... FALSE #&gt; values #&gt; o MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,... #&gt; d MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,... Origin totals, by mode: st_apply(odm, c(1,3), sum) #&gt; stars object with 2 dimensions and 1 attribute #&gt; attribute(s): #&gt; sum #&gt; Min. : 1 #&gt; 1st Qu.: 58 #&gt; Median : 214 #&gt; Mean : 490 #&gt; 3rd Qu.: 771 #&gt; Max. :2903 #&gt; dimension(s): #&gt; from to offset delta refsys point #&gt; o 1 102 NA NA +proj=longlat +datum=WGS8... FALSE #&gt; mode 1 4 NA NA NA FALSE #&gt; values #&gt; o MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,... #&gt; mode bicycle,...,train Destination totals, by mode: st_apply(odm, c(2,3), sum) #&gt; stars object with 2 dimensions and 1 attribute #&gt; attribute(s): #&gt; sum #&gt; Min. : 0 #&gt; 1st Qu.: 13 #&gt; Median : 104 #&gt; Mean : 490 #&gt; 3rd Qu.: 408 #&gt; Max. :12948 #&gt; dimension(s): #&gt; from to offset delta refsys point #&gt; d 1 102 NA NA +proj=longlat +datum=WGS8... FALSE #&gt; mode 1 4 NA NA NA FALSE #&gt; values #&gt; d MULTIPOLYGON (((-2.51 51.4,...,...,MULTIPOLYGON (((-2.55 51.5,... #&gt; mode bicycle,...,train Origin totals, summed over modes: o = st_apply(odm, 1, sum) Destination totals, summed over modes (we had this): d = st_apply(odm, 2, sum) We take o and d together and plot them by x = (c(o, d, along = list(od = c(&quot;origin&quot;, &quot;destination&quot;)))) plot(x, logz = TRUE) There is something to say for the argument that such maps give the wrong message, as both amount (color) and polygon size give an impression of amount. To take out the amount in the count, we can compute densities (count / km\\(^2\\)), by library(units) #&gt; udunits system database from /usr/share/xml/udunits a = as.numeric(set_units(st_area(st_as_sf(o)), km^2)) dens_o = o / a dens_d = d / a plot(c(dens_o, dens_d, along = list(od = c(&quot;origin&quot;, &quot;destination&quot;))), logz = TRUE) 4.3.3 Are datacubes tidy? Yes! The tidy data paper (Wickham 2014b) may suggest that such array data should be processed not as an array, but in a long table where each row holds (region, class, year, value), and it is always good to be able to do this. For primary handling and storage however, this is often not an option, because a lot of array data are collected or generated as array data, e.g. by imagery or other sensory devices, or e.g. by climate models it is easier to derive the long table form from the array than vice versa the long table form requires much more memory, since the space occupied by dimension values is \\(O(nmp)\\), rather than \\(O(n+m+p)\\) when missing-valued cells are dropped, the long table form loses the implicit indexing of the array form To put this argument to the extreme, consider for instance that all image, video and sound data are stored in array form; few people would make a real case for storing them in a long table form instead. Nevertheless, R packages like tsibble take this approach, and have to deal with ambiguous ordering of multiple records with identical time steps for different spatial features and index them, which is solved for both automatically by using the array form. Package stars tries to follow the tidy manifesto to handle array sets, and has particularly developed support for the case where one or more of the dimensions refer to space, and/or time. 4.4 Exercises NDVI, normalized differenced vegetation index, is coputed as (NIR-R)/(NIR+R), with NIR the near infrared and R the red band. Read the L7_ETMs.tif file into object x, and distribute the band dimensions over attributes by split(x, &quot;band&quot;). Then, compute NDVI by using an expression that uses the NIR (band 4) and R (band 3) attributes directly. Compute NDVI for the S2 image, using st_apply and an a function ndvi = function(x) (x[4]-x[3])/(x[4]+x[3]). Plot the result, and write the result to a GeoTIFF. Explain the difference in runtime between plotting and writing. Use st_transform to transform the stars object read from L7_ETMs.tif to EPSG 4326. Print the object. Is this a regular grid? Plot the first band using arguments axes=TRUE and border=NA, and explain why this takes such a long time. Use st_warp to warp the L7_ETMs.tif object to EPSG 4326, and plot the resulting object with axes=TRUE. Why is the plot created much faster than after st_transform? References "],
["geommanip.html", "Chapter 5 Manipulating Geometries 5.1 Predicates 5.2 Measures 5.3 Geometry generating functions 5.4 Precision 5.5 Generating invalid geometries 5.6 Warnings for longitude/latitude geometries", " Chapter 5 Manipulating Geometries Simple feature geometries can be queried for properties, combined into new geometries, and combinations of geometries can be queried for properties. This chapter will give an overview of the operations offered by sf, entirely focusing on geometrical properties. The next chapter, 6, focuses on the analysis of non-geometrical feature properties, in relationship to their geometries. Some of the material in this chapter also appeared as (Pebesma 2018). Several of the concepts of geometric manipulations were introduced in chapter 3. This chapter gives a complete listing of all geometries permitted on geometries, illustrating some of them. We can categorise operations in terms of what they take as input, and what they give as output. In terms of output we have operations that give one or more predicates: a logical asserting a certain property is TRUE, measures: a value (e.g. a numeric value with measurement unit), or geometries and in terms of what they operate on, we distinguish operations that work on a single geometry (unary operations) pairs of geometries (binary operations) sets of geometries (n-ary operations) Before we will go through all combinations, we make two observations: most functions are implemented as methods, and operate equally on single geometry objects (sfg), geometry set objects (sfc) or simple feature (sf) objects. also for binary and n-ary operations, sfg or sf objects are accepted as input, and taken as a set of geometries. 5.1 Predicates Predicates return a logical, TRUE or FALSE value, or a set of those. 5.1.1 Unary predicates st_is_simple returns whether a geometry is simple: st_is_simple(st_sfc( st_point(c(0,1)), st_linestring(rbind(c(0,0), c(1,1), c(0,1), c(1,0))))) # self-intersects #&gt; [1] TRUE FALSE st_is_valid returns whether a geometry is valid st_is_valid(st_sfc( st_linestring(rbind(c(1,1), c(1,2))), st_linestring(rbind(c(1,1), c(1,1))))) # zero-length #&gt; [1] TRUE FALSE st_is_empty returns whether a geometry is empty st_is_empty(st_point()) #&gt; [1] TRUE st_is_longlat returns whether the coordinate reference system is geographic (chapter 2, 7): demo(nc, ask = FALSE, echo = FALSE) #&gt; Reading layer `nc.gpkg&#39; from data source `/home/edzer/R/x86_64-pc-linux-gnu-library/3.6/sf/gpkg/nc.gpkg&#39; using driver `GPKG&#39; #&gt; Simple feature collection with 100 features and 14 fields #&gt; Attribute-geometry relationship: 0 constant, 8 aggregate, 6 identity #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -84.3 ymin: 33.9 xmax: -75.5 ymax: 36.6 #&gt; epsg (SRID): 4267 #&gt; proj4string: +proj=longlat +datum=NAD27 +no_defs st_is_longlat(nc) #&gt; [1] TRUE nc2 &lt;- st_transform(nc, 3857) # to web Mercator st_is_longlat(nc2) #&gt; [1] FALSE st_is_longlat(st_point(0:1)) #&gt; [1] NA st_is is an easy way to check for the simple feature geometry type: st_is(st_point(0:1), &quot;POINT&quot;) #&gt; [1] TRUE all(st_is(nc, &quot;POLYGON&quot;)) #&gt; [1] FALSE all(st_is(nc, &quot;MULTIPOLYGON&quot;)) #&gt; [1] TRUE Equality and inequality of geometries can be checked by == or !=; it uses geometric equality, and is insensitive to the order of traversal of nodes: st_sfc(st_point(0:1), st_point(1:2)) == st_sfc(st_point(0:1)) #&gt; [1] TRUE FALSE st_linestring(rbind(c(0,0), c(1,1))) == st_linestring(rbind(c(1,1), c(0,0))) #&gt; [1] TRUE Under the hood, it uses st_equals, discussed by the binary predicates. 5.1.2 Binary predicates Binary predicates result in a TRUE or FALSE value for every pair of inputs. For two sets of inputs with \\(n\\) and \\(m\\) geometries respectively, this results in an \\(n \\times m\\) logical matrix. Because \\(n\\) and/or \\(m\\) may be very large and the predicate matrix typically contains mostly FALSE values, a sparse representation of it, a sparse geometry binary predicate (sgbp) object, is returned by all functions. They are simply lists of indices of the TRUE values in each row: (r &lt;- st_touches(nc2[1:2,], nc2)) #&gt; Sparse geometry binary predicate list of length 2, where the predicate was `touches&#39; #&gt; 1: 2, 18, 19 #&gt; 2: 1, 3, 18 str(r) #&gt; List of 2 #&gt; $ : int [1:3] 2 18 19 #&gt; $ : int [1:3] 1 3 18 #&gt; - attr(*, &quot;predicate&quot;)= chr &quot;touches&quot; #&gt; - attr(*, &quot;region.id&quot;)= chr [1:2] &quot;1&quot; &quot;2&quot; #&gt; - attr(*, &quot;ncol&quot;)= int 100 #&gt; - attr(*, &quot;class&quot;)= chr &quot;sgbp&quot; sgbp objects have the following methods: methods(class = &#39;sgbp&#39;) #&gt; [1] as.data.frame as.matrix dim Ops print #&gt; [6] t #&gt; see &#39;?methods&#39; for accessing help and source code For understanding predicates, the dimensionally extended 9-intersection model (DE-9IM, (Clementini, Di Felice, and Oosterom 1993, Egenhofer and Franzosa (1991))) is adopted, which is explained in more detail on Wikipedia. Briefly, it considers that every geometry has an interior, a boundary and an exterior. For polygons this is trivial, for points the boundary is an empty set, for linestrings the boundary is formed by the end points and the interior by all non end points. Also, any geometry has a dimension of 0 (points), 1 (lines) or 2 (polygons) or non-existent in the case of an empty geometry. A relationship between two geometries A and B is expressed by the dimension of the overlap (intersections) of 9 intersections, formed by the 9 pairs from the interior, boundary and exterior of A, and the interior, boundary and exterior of B. We can query this relation by using st_relate B = st_linestring(rbind(c(0,0), c(1,0))) A = st_point(c(0.5, 0)) # halfway the line st_relate(A, B) #&gt; [,1] #&gt; [1,] &quot;0FFFFF102&quot; In the relationship found, 0FFFFF102, F indicates empty geometries, and we see from 0FF that the (interior of the) point has 0-dimensional overlap with the interior of line (i.e., the overlap is a point), and no overlap with the boundary (end points) or the exterior of the line, FFF that the (empty) border of the point has nothing in common with the line, and 102 that the exterior of the point (all points except this one) have a 1-dimensional overlap with the interior of the line, a 0-dimensional overlap with the boundary of the line (its end points), and a 2-dimensional overlap with the exterior of the line. We can query whether a particular relationship holds by giving st_relate a pattern. To check for instance whether point A overlaps with an end point of linestring B, we can use st_relate(A, B, pattern = &quot;F0FFFFFFF&quot;) %&gt;% as.matrix() #&gt; [,1] #&gt; [1,] FALSE In these patterns, * can be used for anything, and T for non-empty (0, 1 or 2). The standard relationships below are all expressed as particular query patterns, the Wikipedia page gives details on the patterns used. The binary predicates provided by package sf are predicate value inverse of st_contains None of the points of A are outside B st_within st_contains_properly A contains B and B has no points in common with the boundary of A st_covers No points of B lie in the exterior of A st_covered_by st_covered_by inverse of st_covers st_crosses A and B have some but not all interior points in common st_disjoint A and B have no points in common st_intersects st_equals A and B are geometrically equal; node order number of nodes may differ; identical to A contains B AND A within B st_equals_exact A and B are geometrically equal, and have identical node order st_intersects A and B are not disjoint st_disjoint st_is_within_distance A is closer to B than a given distance st_within None of the points of B are outside A st_contains st_touches A and B have at least one boundary point in common, but no interior points st_overlaps A and B have some points in common; the dimension of these is identical to that of A and B st_relate given a pattern, returns whether A and B adhere to this pattern 5.1.3 N-ary Higher-order predicates are not supported by special functions. 5.2 Measures 5.2.1 Unary Unary measures return a single value that describes a property of the geometry: function returns st_dimension 0 for points, 1 for linear, 2 for polygons, NA for empty geometries st_area the area for geometries st_length the lengths of linear geometries lwgeom::st_geohash the geohash for geometries st_geometry_type the types of a set of geometries 5.2.2 Binary st_distance returns the distances between pairs of geometries, either as a vector with distances between the two first, the two second, … pairs, or as a matrix with all pairwise distances. The result is numeric, or is of class units (E. Pebesma, Mailund, and Hiebert 2016a) when distance units can be derived from the coordinate reference system (chapter 7): st_distance(nc[1:3,], nc[2:4,], by_element = TRUE) %&gt;% setNames(NULL) #&gt; Units: [m] #&gt; [1] 0 0 367505 st_distance(nc[1:3,], nc[2:4,]) #&gt; Units: [m] #&gt; [,1] [,2] [,3] #&gt; [1,] 0 25650 440513 #&gt; [2,] 0 0 409370 #&gt; [3,] 0 0 367505 st_relate returns the relation pattern, as explained in section 5.1.2, or an sgbp object when given a pattern template to match to. 5.2.3 N-ary No higher-order functions returning a measure are available. 5.3 Geometry generating functions 5.3.1 Unary Unary operations work on a per-geometry basis, and for each geometry return a new geometry. None of these functions operate on more than one feature geometry. Most functions are implemented as (S3) generic, with methods for sfg, sfc and sf; their output is of the same class as their input: for sfg input, an sfg value is returned for sfc input, a new set of geometries is returned as sfc for sf objects, the same sf object is returned which has geometries replaced with the new ones. function returns a geometry… st_centroid of type POINT with the geometry’s centroid st_buffer that is this larger (or smaller) than the input geometry, depending on the buffer size st_jitter that was moved in space a certain amount, using a bivariate uniform distribution st_wrap_dateline cut into pieces that do no longer cover the dateline st_boundary with the boundary of the input geometry st_convex_hull that forms the convex hull of the input geometry (figure 5.1) st_line_merge after merging connecting LINESTRING elements of a MULTILINESTRING into longer LINESTRINGs. st_make_valid that is valid st_node with added nodes to linear geometries at intersections without a node; only works on individual linear geometries st_point_on_surface with a (arbitrary) point on a surface st_polygonize of type polygon, created from lines that form a closed ring st_segmentize a (linear) geometry with nodes at a given density or minimal distance st_simplify simplified by removing vertices/nodes (lines or polygons) lwgeom::st_split that has been split with a splitting linestring st_transform transformed to a new coordinate reference system (chapter 7) st_triangulate with triangulated polygon(s) st_voronoi with the voronoi tesselation of an input geometry (figure 5.1) st_zm with removed or added Z and/or M coordinates st_collection_extract with subgeometries from a GEOMETRYCOLLECTION of a particular type st_cast that is converted to another type par(mar = rep(0,4), mfrow = c(1, 2)) plot(st_geometry(nc)[1], col = NA, border = &#39;black&#39;) plot(st_convex_hull(st_geometry(nc)[1]), add = TRUE, col = NA, border = &#39;red&#39;) box() set.seed(131) mp = st_multipoint(matrix(runif(20), 10)) plot(mp) plot(st_voronoi(mp), add = TRUE, col = NA, border = &#39;red&#39;) box() Figure 5.1: left: convex hull (red) around a polygon (black); right: voronoi diagram (red) from a MULTIPOINT (black) A number of operation can be applied directly to geometries (A = st_point(c(1,2))) #&gt; POINT (1 2) (B = st_linestring(rbind(c(2,2), c(3,4)))) #&gt; LINESTRING (2 2, 3 4) -A #&gt; POINT (-1 -2) B + A #&gt; LINESTRING (3 4, 4 6) st_sfc(B + A) * matrix(c(1,0,0,2), 2, 2) #&gt; Geometry set for 1 feature #&gt; geometry type: LINESTRING #&gt; dimension: XY #&gt; bbox: xmin: 3 ymin: 8 xmax: 4 ymax: 12 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; LINESTRING (3 8, 4 12) st_sfc(A, B) * c(3, 5) # scale first by 3, second by 5: #&gt; Geometry set for 2 features #&gt; geometry type: GEOMETRY #&gt; dimension: XY #&gt; bbox: xmin: 3 ymin: 6 xmax: 15 ymax: 20 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; POINT (3 6) #&gt; LINESTRING (10 10, 15 20) 5.3.2 Binary operations returning geometries Binary functions that return a geometry include function returns infix operator st_intersection the overlapping geometries for pair of geometries &amp; st_union the combination of the geometries; also removes duplicate points, nodes or line pieces | st_difference the geometries of the first after removing the overlap with the second geometry / st_sym_differenc the combinations of the geometries after removing where they overlap %/% When operating on two sfg, single geometries, it is clear what all these functions do: return a single geometry for this pair. When given two sets of geometries (sfc or sf objects), a new set of geometries is returned; for st_intersection containing only the non-empty geometries, for all other operations the geometries from all pairwise evaluation. In case the arguments are of class sf, the attributes of the objects are copied over to all intersections to which each of the features contributed: a = st_sf(a = 1, geom = st_sfc(st_linestring(rbind(c(0,0), c(1,0))))) b = st_sf(b = 1:3, geom = st_sfc(st_point(c(0,0)), st_point(c(1,0)), st_point(c(2,0)))) st_intersection(a, b) #&gt; Warning: attribute variables are assumed to be spatially constant #&gt; throughout all geometries #&gt; Simple feature collection with 2 features and 2 fields #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 0 ymin: 0 xmax: 1 ymax: 0 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; a b geom #&gt; 1 1 1 POINT (0 0) #&gt; 1.1 1 2 POINT (1 0) When st_intersection or st_difference are called with a single set of geometries (an sfc object), they perform an n-ary operation, explained in the next section. 5.3.3 N-ary operations returning a geometry 5.3.3.1 Union, c, and combine Calling st_union with only a single argument leads either to computing the union of all geometries, or applying union to each of the individual geometries, depending on the setting of by_feature: st_union(b, by_feature = FALSE) # default #&gt; Geometry set for 1 feature #&gt; geometry type: MULTIPOINT #&gt; dimension: XY #&gt; bbox: xmin: 0 ymin: 0 xmax: 2 ymax: 0 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; MULTIPOINT (0 0, 1 0, 2 0) st_union(b, by_feature = TRUE) # default #&gt; Simple feature collection with 3 features and 1 field #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 0 ymin: 0 xmax: 2 ymax: 0 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; b geom #&gt; 1 1 POINT (0 0) #&gt; 2 2 POINT (1 0) #&gt; 3 3 POINT (2 0) The c method combines sets of geometries bb = st_geometry(b) c(bb, bb) #&gt; Geometry set for 6 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 0 ymin: 0 xmax: 2 ymax: 0 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; First 5 geometries: #&gt; POINT (0 0) #&gt; POINT (1 0) #&gt; POINT (2 0) #&gt; POINT (0 0) #&gt; POINT (1 0) or single geometries into single a new single geometry c(st_point(0:1), st_point(1:2)) #&gt; MULTIPOINT (0 1, 1 2) and st_combine uses this to collapse features for different geometries into one: st_combine(c(bb, bb)) #&gt; Geometry set for 1 feature #&gt; geometry type: MULTIPOINT #&gt; dimension: XY #&gt; bbox: xmin: 0 ymin: 0 xmax: 2 ymax: 0 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; MULTIPOINT (0 0, 1 0, 2 0, 0 0, 1 0, 2 0) When using this on lines or polygons, it is easy to obtain invalid geometries, and one needs to use st_union on the result. (x = st_combine(st_sfc(st_linestring(rbind(c(0,0), c(1,1))), st_linestring(rbind(c(1,0),c(0,1)))))) #&gt; Geometry set for 1 feature #&gt; geometry type: MULTILINESTRING #&gt; dimension: XY #&gt; bbox: xmin: 0 ymin: 0 xmax: 1 ymax: 1 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; MULTILINESTRING ((0 0, 1 1), (1 0, 0 1)) st_is_valid(x) #&gt; [1] TRUE st_union(x) %&gt;% st_is_valid() #&gt; [1] TRUE 5.3.3.2 N-ary intersection and difference N-ary st_intersection and st_difference take a single argument, but operate (sequentially) on all pairs, triples, quadruples etc. Consider the plot in figure 5.2: how do we identify the box where all three overlap? Using binary intersections, as of gives us intersections of all pairs, double since x is passed twice: 1-1, 1-1, 1-3, 2-1, 2-2, 2-3, 3-1, 3-2, 3-3: sq = function(pt, sz = 1) st_polygon(list(rbind(c(pt - sz), c(pt[1] + sz, pt[2] - sz), c(pt + sz), c(pt[1] - sz, pt[2] + sz), c(pt - sz)))) x = st_sf(box = 1:3, st_sfc(sq(c(0,0)), sq(c(1.7, -0.5)), sq(c(0.5, 1)))) (ixx = st_intersection(x, x)) %&gt;% nrow #&gt; Warning: attribute variables are assumed to be spatially constant #&gt; throughout all geometries #&gt; [1] 9 lengths(st_overlaps(ixx, ixx)) #&gt; [1] 4 5 5 5 4 5 5 5 4 When we use however (i = st_intersection(x)) #&gt; Simple feature collection with 7 features and 3 fields #&gt; geometry type: POLYGON #&gt; dimension: XY #&gt; bbox: xmin: -1 ymin: -1.5 xmax: 2.7 ymax: 2 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; box n.overlaps origins #&gt; 1 1 1 1 #&gt; 1.1 1 2 1, 2 #&gt; 2 2 1 2 #&gt; 2.1 2 2 2, 3 #&gt; 1.2 1 3 1, 2, 3 #&gt; 1.3 1 2 1, 3 #&gt; 3 3 1 3 #&gt; st_sfc.sq.c.0..0....sq.c.1.7...0.5....sq.c.0.5..1... #&gt; 1 POLYGON ((0.7 -1, -1 -1, -1... #&gt; 1.1 POLYGON ((1 0, 1 -1, 0.7 -1... #&gt; 2 POLYGON ((1.5 0.5, 2.7 0.5,... #&gt; 2.1 POLYGON ((1 0.5, 1.5 0.5, 1... #&gt; 1.2 POLYGON ((1 0.5, 1 0, 0.7 0... #&gt; 1.3 POLYGON ((-0.5 1, 1 1, 1 0.... #&gt; 3 POLYGON ((-0.5 1, -0.5 2, 1... we end up with a set of all seven distinct intersections, without overlaps. lengths(st_overlaps(i, i)) #&gt; [1] 0 0 0 0 0 0 0 When given an sf object an sf is returned, with two additional fields, one with the number of overlapping features, and a list-column with the indexes of contributing feature geometries. Figure 5.2: left: three overlapping boxes – how do we identify the small box where all three overlap? right: unique, non-overlapping n-ary intersections Similarly, one can compute n-ary differences from a set \\(\\{s_1, s_2, s_3, ...\\}\\) by creating differences \\(\\{s_1, s_2-s_1, s_3-s_2-s_1, ...\\}\\). This is done by (xd = st_difference(x)) #&gt; Simple feature collection with 3 features and 1 field #&gt; geometry type: POLYGON #&gt; dimension: XY #&gt; bbox: xmin: -1 ymin: -1.5 xmax: 2.7 ymax: 2 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; box st_sfc.sq.c.0..0....sq.c.1.7...0.5....sq.c.0.5..1... #&gt; 1 1 POLYGON ((-1 -1, 1 -1, 1 1,... #&gt; 2 2 POLYGON ((1 0.5, 2.7 0.5, 2... #&gt; 3 3 POLYGON ((-0.5 1, -0.5 2, 1... The result is shown in figure 5.3, for x and for x[3:1], to make clear that the result here depends on order of the geometries. Figure 5.3: difference between subsequent boxes, left: in original order; right: in reverse order Resulting geometries do not overlap: lengths(st_overlaps(xd, xd)) #&gt; [1] 0 0 0 5.3.4 Other geometry manipulators st_make_grid creates a grid of square or hexagonal polygons, based on an input bounding box and a grid cell size. st_graticule creates a set of graticules, lines of constant latitude or longitude, which can serve as a reference on small-scale (large area) maps. 5.4 Precision Geometrical operations, such as finding out whether a certain point is on a line, may fail when coordinates are represented by highly precise floating point numbers, such as 8-byte doubles in R. A remedy might be to limit the precision of the coordinates before the operation. For this, a precision model is adopted by sf. It uses a precision value to round coordinates (X, Y, Z and M) right before they are encoded as well-known binary, and passed on to the libraries where this may have an effect (GEOS, GDAL, liblwgeom). We demonstrate this by an R - WKB - R roundtrip. Rounding can be done in two different ways. First, With a negative precision value, 8-byte doubles get converted to 4-byte floats and back again: (p = st_sfc(st_point(c(1e6/3, 1e4/3)), crs = 3857)) #&gt; Geometry set for 1 feature #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 333000 ymin: 3330 xmax: 333000 ymax: 3330 #&gt; epsg (SRID): 3857 #&gt; proj4string: +proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 +x_0=0.0 +y_0=0 +k=1.0 +units=m +nadgrids=@null +wktext +no_defs #&gt; POINT (333333 3333) p %&gt;% st_set_precision(-1) %&gt;% st_as_binary() %&gt;% st_as_sfc() %&gt;% `[[`(1) %&gt;% print(digits = 16) #&gt; POINT (333333.34375 3333.333251953125) Second, with a positive precision \\(p\\), each coordinate value \\(c\\) is replaced by \\[c&#39; = \\mbox{round}(p \\cdot c) / p\\] This implies that for instance with a precision of 1000, the number of decimal places to round to is 1/1000, or to mm if the unit of coordinates is metre: p %&gt;% st_set_precision(1000) %&gt;% st_as_binary() %&gt;% st_as_sfc() %&gt;% `[[`(1) #&gt; POINT (333333 3333) With a precision of e.g. 0.001 or 0.05, rounding to the nearest 1/precision, i.e. if the unit is m to the nearest 1000 m or 20 m, is obtained: p %&gt;% st_set_precision(0.001) %&gt;% st_as_binary() %&gt;% st_as_sfc() %&gt;% `[[`(1) # to nearest 1000 #&gt; POINT (333000 3000) p %&gt;% st_set_precision(0.05) %&gt;% st_as_binary() %&gt;% st_as_sfc() %&gt;% `[[`(1) # to nearest 20 #&gt; POINT (333340 3340) As a convenience, precisions can also be specified as a units object, with the unit to round to, e.g. to the nearest 5 cm: p %&gt;% st_set_precision(units::set_units(5, cm)) %&gt;% st_as_binary() %&gt;% st_as_sfc() %&gt;% `[[`(1) %&gt;% print(digits = 10) #&gt; POINT (333333.35 3333.35) but this requires that the object, p, has a coordinate reference system with known units. In essence, these rounding methods bring the coordinates to points on a regular grid, which is beneficial for geometric computations. Of course, it also affects all computations like areas and distances. Which precision values are best for which application is often a matter of common sense combined with trial and error. A reproducible example illustrating the need for setting precision is found here. 5.5 Generating invalid geometries It is rather easy to have st_intersection generate invalid geometries, resulting in an error. Consider the graph constructed and shown in figure 5.4. In this case, not setting the precision (i.e., precision has value 0) would have led to the cryptic error message Error in CPL_nary_intersection(x) : Evaluation error: TopologyException: found non-noded intersection between LINESTRING (0.329035 -0.0846201, 0.333671 -0.0835073) and LINESTRING (0.330465 -0.0842769, 0.328225 -0.0848146) at 0.32965918719530368 -0.084470389572422672. Calls: st_intersection ... st_intersection -&gt; st_intersection.sfc -&gt; CPL_nary_intersection However, with zero precision and a buf_size of 0.7 we will not get this error. n = 12 # n points, equally spread along unit circle: pts = (1:n)/n * 2 * pi xy = st_as_sf(data.frame(x = cos(pts), y = sin(pts)), coords = c(&quot;x&quot;, &quot;y&quot;)) buf_size = 0.8 precision = 1000 b = st_buffer(xy, buf_size) i = st_intersection(st_set_precision(b, precision)) par(mar = rep(0, 4)) plot(i[1], col = sf.colors(nrow(i), categorical = TRUE)) all(st_is_valid(i)) #&gt; [1] TRUE Figure 5.4: n-ary intersection that may lead to invalid geometries 5.6 Warnings for longitude/latitude geometries When working on geodetic coordinates (degrees longitude/latitude), package sf gives warnings when it makes the assumption that coordinates are Cartesian, e.g. in i = st_intersects(nc[1,], nc[2,]) #&gt; although coordinates are longitude/latitude, st_intersects assumes that they are planar In many cases, making this assumption is not a problem. It might be a problem when we have polygons that cover very large areas, cover North or South pole, or have lines crossing or polygons covering the dateline. References "],
["featureattributes.html", "Chapter 6 Feature attributes 6.1 Attribute-geometry relationships 6.2 Spatial join 6.3 Aggregate and summarise 6.4 Intersections 6.5 Area-weighted interpolation 6.6 Exercises", " Chapter 6 Feature attributes Feature attributes refer to the properties of features (“things”) that do not describe the feature’s geometry. Feature attributes can be derived from geometry (e.g. length of a LINESTRING, area of a POLYGON) but they can also refer to completely different properties, such as the name of a street or a county, the number of people living in a country, the type of a road the soil type in a polygon from a soil map. the opening hours of a shop the body weight of an animal the NO\\(_2\\) concentration measured at an air quality monitoring station Although temporal properties of features are no less fundamental than their spatial properties, the simple feature access standard and consequently the sf package does not give time a similar role as space; more on that in chapter 4. Most sf objects will contain both geometries and attributes for features. The geometric operations described in the previous chapter (5) operate on geometries only, and may occasionally add attributes, but will not modify attributes present. In all these cases, attribute values remain unmodified. At first sight, that looks rather harmless. But if we look into a simple case of replacing a county boundary with a county centroid, as in library(sf) library(dplyr) system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;) %&gt;% read_sf() %&gt;% st_transform(32119) %&gt;% select(BIR74, SID74, NAME) %&gt;% st_centroid() %&gt;% head(n = 1) -&gt; x # save as x #&gt; Warning in st_centroid.sf(.): st_centroid assumes attributes are constant #&gt; over geometries of x st_geometry(x)[[1]] #&gt; POINT (385605 3e+05) we receive a warning. This warning is justified for the first two variables shown (total births and number of SID disease cases, 1974) which, as such, are not associated with a feature whose geometry is POINT (385605.4 300303.5). The third variable, NAME is however still the county name for the point indicated, but the point geometry no longer is the county geometry. 6.1 Attribute-geometry relationships Changing the feature geometry without changing the feature attributes does change the feature, since the feature is characterised by the combination of geometry and attributes. Can we, ahead of time, predict whether the resulting feature will still meaningfully relate to the attribute data when we replace all geometries for instance with their convex hull or centroid? It depends. Take the example of a road, represented by a LINESTRING, which has an attribute property road width equal to 10 m. What can we say about the road width of an arbitray subsectin of this road? That depends on whether the attribute road length describes, for instance the road width everywhere, meaning that road width is constant along the road, or whether it describes an aggregate property, such as minimum or average road width. In case of the minimum, for an arbitrary subsection of the road one could still argue that the minimum road with must be at least as large as the minimum road width for the whole segment, but it may no longer be the minimum for that subsection. This gives us two “types” for the attribute-geometry relationship (AGR): constant the attribute value is valid everywhere in or over the geometry aggregate the attribute is an aggregate, a summary value over the geometry For polygon data, typical examples of constant AGR are land use for a land use polygon rock units or geologic strata in a geological map soil type in a soil map elevation class in a elevation map that shows elevation as classes climate zone in a climate zone map Typical examples for the aggregate AGR are population, either as number of persons or as population density other socio-economic variables, summarised by area total emission of pollutants by region block mean NO\\(_2\\) concentrations, as e.g. obtained by block kriging or a dispersion model that predicts areal means A third type of AGR is that where an attribute identifies a feature geometry. The example above is county NAME: the name identifies the county, and is still the county NAME for any sub-area. identity the attribute value uniquely identifies the geometry as a whole, there are no other geometries with the same value Arbitrary sub-areas will lose the identity property but becomes a constant attribute. An example is: any point inside a county is still part of the county and must have the same value for county name, but it does not longer represent the (entire) geometry corresponding to that county. We can specify the AGR of an attribute in an sf object by st_set_agr: nc &lt;- system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;) %&gt;% read_sf() %&gt;% st_transform(32119) nc1 &lt;- nc %&gt;% select(BIR74, SID74, NAME) %&gt;% st_set_agr(c(BIR74 = &quot;aggregate&quot;, SID74 = &quot;aggregate&quot;, NAME = &quot;identity&quot;)) This helps to get rid of warnings that a particular attribute is assumed to be constant over a geometry, if it already is. The following no longer generates a warning nc1 %&gt;% select(NAME) %&gt;% st_centroid() %&gt;% head(1) #&gt; Simple feature collection with 1 feature and 1 field #&gt; Attribute-geometry relationship: 1 constant, 0 aggregate, 0 identity #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 386000 ymin: 3e+05 xmax: 386000 ymax: 3e+05 #&gt; epsg (SRID): 32119 #&gt; proj4string: +proj=lcc +lat_1=36.16666666666666 +lat_2=34.33333333333334 +lat_0=33.75 +lon_0=-79 +x_0=609601.22 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs #&gt; # A tibble: 1 x 2 #&gt; NAME geom #&gt; &lt;chr&gt; &lt;POINT [m]&gt; #&gt; 1 Ashe (385605 3e+05) and also changes AGR for NAME from identity to constant when replacing the geometry with the geometry’s centroid: nc1 %&gt;% select(NAME) %&gt;% st_centroid() %&gt;% st_agr() #&gt; NAME #&gt; constant #&gt; Levels: constant aggregate identity Identifying attribute-geometry relationships, and warning against their absence is a first and simple implementation of the notion that the types of phenomena we encounter in spatial data science (like objects, fields, and aggregations) are not identified by their geometrical representations (points, lines, polygons, rasters). Making the wrong assumptions here easily leads to meaningless analysis results (Stasch et al. 2014,Scheider et al. (2016)). 6.2 Spatial join Spatial joins are similar to regular (left or inner) joins, where the join criterion is not equality of one or more fields, but a spatial predicate, such as that two records have intersecting geometries. As an example, we can create a join between two tables, a = st_sf(a = 1:2, geom = st_sfc(st_point(c(0,0)), st_point(c(1,1)))) b = st_sf(b = 3:4, geom = st_sfc(st_linestring(rbind(c(2,0), c(0,2))), st_point(c(1,1)))) st_join(a, b) #&gt; Simple feature collection with 3 features and 2 fields #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 0 ymin: 0 xmax: 1 ymax: 1 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; a b geom #&gt; 1 1 NA POINT (0 0) #&gt; 2 2 3 POINT (1 1) #&gt; 2.1 2 4 POINT (1 1) st_join(a, b, left = FALSE) #&gt; Simple feature collection with 2 features and 2 fields #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 1 xmax: 1 ymax: 1 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; a b geom #&gt; 2 2 3 POINT (1 1) #&gt; 2.1 2 4 POINT (1 1) st_join(b, a) #&gt; Simple feature collection with 2 features and 2 fields #&gt; geometry type: GEOMETRY #&gt; dimension: XY #&gt; bbox: xmin: 0 ymin: 0 xmax: 2 ymax: 2 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; b a geom #&gt; 1 3 2 LINESTRING (2 0, 0 2) #&gt; 2 4 2 POINT (1 1) We see that unless left = FALSE, we get all elements (and geometries) from the first argument, augmented with fields of the second argument when geometries match. The example shows the case where there are two geometries matching to point (1,1). The spatial join predicate function can be freely chosen, e.g. from the binary predicates listed in section 5.1.2. When we match two sets of polygons, it may be a bit of a mess to go through all the many matches. One way out of this is to only provide the match with the largest overlap with the target geometry, obtained by adding argument largest = TRUE. An example of this is shown (visually) in figure 6.1. Figure 6.1: example of st_join with largest = TRUE: the label of the polygon in the top figure with the largest intersection with polygons in the bottom figure is assigned to the polygons of the bottom figure 6.3 Aggregate and summarise Package sf provides sf methods for stats::aggregate and dplyr::summarise. Both do essentially the same: given a grouping predicate (for summarise, obtained from group_by) given an aggregation function aggregate the selected attributes using this function, per group aggregate in addition the geometries. if do_union is TRUE (the default), union the aggregated geometries. Unioning aggregated geometries dissolves for instance internal polygon boundaries, which otherwise would lead to invalid MULTIPOLYGON errors in subsequent analysis, or plotting of potentially unwanted internal polygon boundaries. Figure 6.2 illustrates this. Figure 6.2: left: invalid MULTIPOLYGON with two external rings with common boundary, right: valid POLYGON obtained after unioning the geometry left 6.4 Intersections Suppose we have two datasets with different geometries and attributes (left figure 6.3), and we want to compute their intersections: p1 = st_polygon(list(rbind(c(0,0), c(4,0), c(4,4), c(0,4), c(0,0)))) d1 = st_sf(a = c(3,1), geom = st_sfc(p1, p1 + c(4, 0))) d2 = st_sf(b = c(4), geom = st_sfc(p1 * .75 + c(3, 2))) What will the intersection of these two objects give? (i = st_intersection(d1, d2)) #&gt; Warning: attribute variables are assumed to be spatially constant #&gt; throughout all geometries #&gt; Simple feature collection with 2 features and 2 fields #&gt; geometry type: POLYGON #&gt; dimension: XY #&gt; bbox: xmin: 3 ymin: 2 xmax: 6 ymax: 4 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; a b geom #&gt; 1 3 4 POLYGON ((3 4, 4 4, 4 2, 3 ... #&gt; 2 1 4 POLYGON ((4 2, 4 4, 6 4, 6 ... plot(d1, xlim = c(0,8), ylim = c(0, 6), col = NA, border = 1, reset = FALSE) plot(d2, col = NA, border = &#39;red&#39;, add = TRUE, lwd = 2) plot(d1, xlim = c(0,8), ylim = c(0, 6), col = NA, border = 1, lwd = 2, reset = FALSE) plot(d2, col = NA, border = &#39;red&#39;, add = TRUE, lwd = 3) plot(st_geometry(i), add = TRUE, col = grey(c(.7,.9)), , border = &#39;green&#39;, lwd = 1) Figure 6.3: left: overlapping geometries (d2: red); right: intersection areas (i: grey) As we see, this gives the areas of intersection, along with the corresponding attributes for both contributing objects, and a warning that attributes were assumed to be spatially constant. Although this may be convenient in some cases, it may be entirely meaningless in others. For instance, in case attribute b in object d2 represents the number of people living in d2, then after the intersection we end up with twice as many people living over a smaller area. As seen in section 5.5, computing intersections easily leads to errors caused by invalid geometries. Setting precision (section 5.4) may prevent this. 6.5 Area-weighted interpolation Suppose we want to combine geometries and attributes of two datasets such, that we get attribute values of the first datasets summarised for the geometries of the second. There are various ways we can go for this. The simples one, building on the previous example, would be to obtain for the geometry of d2 the attribute of d1 that has the largest overlap with d2. This is obtained by st_join(d2, d1, largest = TRUE) #&gt; Warning: attribute variables are assumed to be spatially constant #&gt; throughout all geometries #&gt; Simple feature collection with 1 feature and 2 fields #&gt; geometry type: POLYGON #&gt; dimension: XY #&gt; bbox: xmin: 3 ymin: 2 xmax: 6 ymax: 5 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; b a geom #&gt; 1 4 1 POLYGON ((3 2, 6 2, 6 5, 3 ... Another option would be to summarise the attribute, e.g. taking its mean, regardless the amount of overlap. This is obtained by aggregate(d1, d2, mean) #&gt; Simple feature collection with 1 feature and 1 field #&gt; geometry type: POLYGON #&gt; dimension: XY #&gt; bbox: xmin: 3 ymin: 2 xmax: 6 ymax: 5 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; a geometry #&gt; 1 2 POLYGON ((3 2, 6 2, 6 5, 3 ... A third option is to apply area-weighted interpolation, meaning that we interpolate (average) the variable by taking into account the respective area contributions of overlap (Goodchild and Lam 1980,Do, Thomas-Agnan, and Vanhems (2015a),Do, Thomas-Agnan, and Vanhems (2015b)). This is done e.g. by d3 = st_sfc(p1 * .75 + c(3, 2), p1 * .75 + c(3,3)) st_interpolate_aw(d1, d3, extensive = FALSE)$a #&gt; Warning in st_interpolate_aw.sf(d1, d3, extensive = FALSE): #&gt; st_interpolate_aw assumes attributes are constant over areas of x #&gt; [1] 1.67 1.67 st_interpolate_aw(d1, d3, extensive = TRUE)$a #&gt; Warning in st_interpolate_aw.sf(d1, d3, extensive = TRUE): #&gt; st_interpolate_aw assumes attributes are constant over areas of x #&gt; [1] 0.625 0.312 6.5.1 Spatially intensive and extensive variables The difference between the two examples for area-weighted interpolation is how the final weighted sum (value times area of intersection) is normalised: by the target area (extensive), or by the sum of the area covered (intensive, extensive = FALSE). Spatially intensive variables are variables for which the value, when we split an area, does not principally change. An example might be temperature, elevation, or population density. Spatially extensive variables are variables for which the value is also split, according to the area. Examples are population (amount), or area. 6.6 Exercises Add a variable to the nc dataset by nc$State = &quot;North Carolina&quot;. Which value should you attach to this variable for the attribute-geometry relationship (agr)? Create a new sf object from the geometry obtained by st_union(nc), and assign &quot;North Carolina&quot; to the variable State. Which agr can you now assign to this attribute variable? Use st_area to add a variable with name area to nc. Compare the area and AREA variables in the nc dataset. What are the units of AREA? Are the two linearly related? If there are discrepancies, what could be the cause? Is the area variable intensive or extensive? Is its agr equal to constant, identity or aggregate? Find the name of the county that contains POINT(-78.34046 35.017) Find the names of all counties with boundaries that touch county Sampson. List the names of all counties that are less than 50 km away from county Sampson. References "],
["rs.html", "Chapter 7 Reference Systems 7.1 Units of measurement 7.2 Temporal Reference Systems 7.3 Coordinate Reference Systems", " Chapter 7 Reference Systems “Data are not just numbers, they are numbers with a context”; “In data analysis, context provides meaning” (Cobb and Moore 1997) 7.1 Units of measurement 7.1.1 Quantities The VIM (International vocabulary of metrology, BIPM et al. (2012)) defines a quantity as a “property of a phenomenon, body, or substance, where the property has a magnitude that can be expressed as a number and a reference”, where “[a] reference can be a measurement unit, a measurement procedure, a reference material, or a combination of such.” One could argue whether all data is constitued of quantities, but there is no need to argue that proper data handling requires that numbers are accompanied by information on what the numbers mean, and what they are about. A measurement system consist of base units for base quantities, and derived units for derived quantities. The SI system of units (Bureau International des Poids et Mesures 2006) consist of the seven base units length (metre, m), mass (kilogram, kg), time (second, s), electric current (ampere, A), thermodynamic temperature (kelvin, K), amount of substance (mole, mol), and luminous intensity (candela, cd). Derived units are composed of products of integer powers of base units; exampes are speed (\\(\\mbox{m}~\\mbox{s}^{-1}\\)) and density (\\(\\mbox{kg}~\\mbox{m}^{-3}\\)). Many data variables have units that are not expressed as SI base units or derived units. Hand (2004) discusses many such measurement scales, e.g. those used to measure intelligence in social sciences, in the context of measurement units. 7.1.2 Unitless measures The special case of unitless units can refer to either cases where units cancelled out (e.g. mass fraction: kg/kg, or angle measured in rad: m/m) or to cases where objects or events were counted (e.g. 5 apples). Adding an angle to a count of apples would not make sense; adding 5 apples to 3 oranges may make sense if the result is reinterpreted, e.g. as pieces of fruit. Flater (2018) discusses systems for proper handling of unitless quantities; handling counts could for instance link to domain-specific ontologies pointing out which things were counted, and perhaps identifying super-classes, like fruit. 7.1.3 Units in R The units R package (E. Pebesma, Mailund, and Hiebert 2016b) provides units of measurement support for R, by interfacing the udunits2 units database and C library. It allows for setting, converting and deriving units: library(units) (a = set_units(1:3, m)) #&gt; Units: [m] #&gt; [1] 1 2 3 a_km = set_units(1:3, km) a + a_km #&gt; Units: [m] #&gt; [1] 1001 2002 3003 b = set_units(4:6, s) a / b #&gt; Units: [m/s] #&gt; [1] 0.25 0.40 0.50 and raises errors in case of meaningless operations a + b #&gt; Error: cannot convert s into m 7.1.4 Datum For many quantities, the natural origin of values is zero. This works for amounts, and differences between amounts results in meaningful negative values. For locations and times, differences have a natural zero interpretation: length and duration. Absolute location (position) and time need a fixed origin, from which we can meaningfully measure other absolute space-time points: a datum. For space, a datum involves more than one dimension. The combination of a datum and a measurement unit (scale) is a a reference system. The next two sections will deal with temporal and spatial reference systems, and how they are handled in R. 7.2 Temporal Reference Systems R has two native classes for time-related data: POSIXt and Date, which are used for specifying dates, and times. 7.2.1 Date Date objects are numeric vectors of class Date, which contain the number of days since (or in case negative: before) Jan 1, 1970: (d = as.Date(&quot;1970-02-01&quot;)) #&gt; [1] &quot;1970-02-01&quot; as.numeric(d) #&gt; [1] 31 We see that the print method as well as the as.Date method use ISO 8601 (ISO8601 2000) character strings, which is standard used to read and write dates. We can also modify this to local conventions by specifying a format: (d = as.Date(&quot;01.02.1970&quot;, format = &quot;%d.%m.%Y&quot;)) #&gt; [1] &quot;1970-02-01&quot; format(d, format = &quot;%d.%m.%Y&quot;) #&gt; [1] &quot;01.02.1970&quot; The default for format may depend on the locale settings of the computer used. The help page of ?as.Date contains further discussion of date systems, and calendars used. 7.2.2 POSIXt POSIXt is an R native class for specifying times. It has two subclasses: POSIXct represents time as a numeric, representing decimal seconds since 1970-01-01 00:00 UTC, and POSIXlt contains the same information as a list with all time components (second, minute, hour, day of month, month, year) in list components: t = as.POSIXct(&quot;1970-01-01 01:00:00&quot;, tz = &quot;UTC&quot;) as.numeric(t) #&gt; [1] 3600 names(unclass(as.POSIXlt(t))) #&gt; [1] &quot;sec&quot; &quot;min&quot; &quot;hour&quot; &quot;mday&quot; &quot;mon&quot; &quot;year&quot; &quot;wday&quot; &quot;yday&quot; &quot;isdst&quot; unclass(as.POSIXlt(t))$hour #&gt; [1] 1 7.2.3 Time zones Time zones can be seen as local modifiers of time: where time as numeric value is stored with respect to UTC (universal coordinated time), a local time zone is used to format it, and a time zone can be specified for creation, and formatting: (t = as.POSIXct(&quot;1970-01-01 00:00:00&quot;, tz = &quot;PST&quot;)) #&gt; [1] &quot;1970-01-01 PST&quot; this adds a time zone modifier to t that redefines the time origin, as as.numeric(t) #&gt; [1] 0 To convert POSIXt to Date we can use as.Date; this converts to the local date. Converting back to POSIXct looses the time of day and time zone information. (t = as.POSIXct(&quot;1970-01-01 23:00:00&quot;, tz = &quot;PST&quot;)) #&gt; [1] &quot;1970-01-01 23:00:00 PST&quot; as.Date(t) #&gt; [1] &quot;1970-01-01&quot; format(as.POSIXct(as.Date(t)), tz = &quot;UTC&quot;) #&gt; [1] &quot;1970-01-01&quot; Working with local time zones is sometimes confusing when the data we work with were not referenced to this time zone. It may help to set Sys.setenv(TZ=&quot;UTC&quot;) at the start of an R script. The effect is, at lease on some platforms, that R thinks it is working in a UTC time zone. This way, the scripts will produce identical outputs, no matter in which time zone it is run. 7.3 Coordinate Reference Systems We follow Lott (2015) when defining the following concepts (italics indicate literal quoting): a coordinate system is a set of mathematical rules for specifying how coordinates are to be assigned to points a datum is a parameter or set of parameters that define the position of the origin, the scale, and the orientation of a coordinate system, a geodetic datum is a datum describing the relationship of a two- or three-dimensional coordinate system to the Earth, and a coordinate reference system is a coordinate system that is related to an object by a datum; for geodetic and vertical datums, the object will be the Earth. A readable text that further explains these concepts is Iliffe and Lott (2008). Essentially it boils down to the Earth not following a regular shape. The topography of the Earth is of course known to vary strongly, but also the surface formed by constant gravity at mean sea level, the geoid, is irregular. A commonly used model that is fit to the geoid is an ellipsoid of revolution, which is an ellipse with two identical minor axes. This model can be fit locally to be highly precise, can be fixed for particular tectonic plates (like ETRS89), or can be globally fit (like WGS84). The definitions above state that coordinates in degrees longitude and latitude can only have a meaning, i.e. can only be understood as Earth coordinates when the datum they relate to is given. Recomputing coordinates in a new datum is called coordinate transformation. 7.3.1 Projections When we look at geographical data on a paper map or a screen, or on any flat device, we see the values projected – they are no longer arranged on an a sphere or ellipsoid. Even if we plot degrees longitude/latitude data on a flat x/y plane with unit aspect ratio, we use a projection, called plate carrée. Note that even for projected data, the data that were projected are associated with a reference ellipsoid (datum). Going from one projection to the other without changing datum is called coordinate conversion, and usually passes through the geodetic coordinates of the datum involved; up to numerical errors this process is lossless and invertible. 7.3.2 Describing Coordinate Reference Systems Lott (2015) describes a standard for encoding coordinate reference system using well known text; the standard is referred to as WKT2. GDAL and PROJ support this encoding (FIXME: verify this is true by the time this book goes into print). Traditionally, PROJ used a string representation to encode coordinate reference systems (datums) and projections, called the proj4string. Some of these come from a catalogue (originally) compiled by the European Petroleum Survey Group (now: International Association of Oil &amp; Gas Producers), and have a so-called epsg code. Package sf provides a crs class which is initialised either by an epsg code, like st_crs(4326) #&gt; Coordinate Reference System: #&gt; EPSG: 4326 #&gt; proj4string: &quot;+proj=longlat +datum=WGS84 +no_defs&quot; or by a PROJ string, like st_crs(&quot;+proj=longlat&quot;) #&gt; Coordinate Reference System: #&gt; EPSG: 4326 #&gt; proj4string: &quot;+proj=longlat +ellps=WGS84 +no_defs&quot; and can be missing valued st_crs() #&gt; Coordinate Reference System: NA A number of methods are available for crs objects: methods(class = &quot;crs&quot;) #&gt; [1] $ is.na Ops print st_as_text st_crs #&gt; see &#39;?methods&#39; for accessing help and source code The Ops methods are convenience functions, e.g. st_crs(4326) == st_crs(&quot;+proj=longlat&quot;) #&gt; [1] FALSE but there not all cases semantically identical crs objects will yield equality in this test. st_as_text converts a crs object into WKT, we print it using cat: cat(st_as_text(st_crs(4326), pretty = TRUE)) #&gt; GEOGCS[&quot;WGS 84&quot;, #&gt; DATUM[&quot;WGS_1984&quot;, #&gt; SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563, #&gt; AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]], #&gt; AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]], #&gt; PRIMEM[&quot;Greenwich&quot;,0, #&gt; AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]], #&gt; UNIT[&quot;degree&quot;,0.0174532925199433, #&gt; AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]], #&gt; AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]] It should be noted that at the time of writing this, a new draft standard for WKT (informally called WKT2, Lott (2015)) is rapidly being implemented in GDAL and PROJ, and can be expected in R once these changes appear in released versions. st_crs is also a generic, with methods for methods(st_crs) #&gt; [1] st_crs.bbox* st_crs.character* st_crs.crs* #&gt; [4] st_crs.CRS* st_crs.default* st_crs.numeric* #&gt; [7] st_crs.Raster* st_crs.sf* st_crs.sfc* #&gt; [10] st_crs.Spatial* st_crs.stars* #&gt; see &#39;?methods&#39; for accessing help and source code The method for sfc objects can drill further into the underlying data, by adding an argument; a few of these are printed by st_crs(st_sfc(crs = 4326), parameters = TRUE)[c(1:4, 8)] #&gt; $SemiMajor #&gt; 6378137 [m] #&gt; #&gt; $SemiMinor #&gt; 6356752 [m] #&gt; #&gt; $InvFlattening #&gt; [1] 298 #&gt; #&gt; $units_gdal #&gt; [1] &quot;degree&quot; #&gt; #&gt; $ud_unit #&gt; 1 [°] where we see that udunits and GDAL units are integrated. The major axis lengths (SemiMajor) and inverse flattening are used e.g. to compute great circle distances on the ellipsoid, using the algorithm from Karney (2013). This algorithm is implemented in PROJ, and interfaced by lwgeom::st_geod_distance, which is called from sf::st_distance when objects have geodetic coordinates. References "],
["plotting.html", "Chapter 8 Plotting spatial data 8.1 Every plot is a projection 8.2 Plotting points, lines, polygons, grid cells 8.3 Class intervals 8.4 Poles and datelines 8.5 Graticules and other navigation aids", " Chapter 8 Plotting spatial data Together with timelines, maps belong to the most powerful graphs, perhaps because we can immediately relate where we are, or have been, on the space of the plot. Two recent books on visualisation (Healy 2018, Wilke (2019)) contain chapters on visualising geospatial data or maps. Here, we will not try to preach the do’s and don’ts of maps, but rather point out a number of possibilities how to do things, point out challenges along the way and ways to mitigate them. 8.1 Every plot is a projection The world is round, but plotting devices are flat. As mentioned in section 7.3.1, any time we visualise, in any way, the world on a flat device, we project: we convert angular, geodetic coordinates into Cartesian coordinates. This includes the cases where we think we “do nothing” (figure 8.1, left), or where show the world “as it is”, e.g. as seen from space (figure 8.1, right). Figure 8.1: Earth country boundaries; left: mapping long/lat to x and y; right: as seen from space The left plot of figure 8.1 was obtained by library(sf) library(rnaturalearth) w &lt;- ne_countries(scale = &quot;medium&quot;, returnclass = &quot;sf&quot;) plot(st_geometry(w)) and we see that this is the default projection for data with geodetic coordinates, as indicated by st_crs(w) #&gt; Coordinate Reference System: #&gt; EPSG: 4326 #&gt; proj4string: &quot;+proj=longlat +datum=WGS84 +no_defs&quot; The projection taken here is the equirectangular (or equidistant cylindrical) projection, which maps longitude and latitude linear to the x and y axis, keeping an aspect ratio of 1. Were we to do this for smaller areas not on the equator, it makes sense to choose a plot ratio such that one distance unit E-W equals one distance unit N-S on the center of the plotted area. We can also carry out this projection before plotting. Say we want to do this for Germany, then after loading the (rough) country outline, we use st_transform to project: DE = st_geometry(ne_countries(country = &quot;germany&quot;, returnclass = &quot;sf&quot;)) DE.eqc = st_transform(DE, &quot;+proj=eqc +lat_ts=51.14 +lon_0=90w&quot;) st_transform takes an sf or sfc object, and as second argument the projection. This can either be a number of a known EPSG projection, e.g. listed at http://spatialreference.org/ref/epsg/ , or a string describing the projection (+proj=...) with further parameters. The parameter here is lat_ts, the latitude of true scale (i.e., one length unit N-S equals one length unit E-W), which was here chosen as the middle of the bounding box latitudes mean(st_bbox(DE)[c(&quot;ymin&quot;, &quot;ymax&quot;)]) #&gt; [1] 51.1 When we now plot both maps (figure 8.2), they look the same up to their values along the axes: degrees for geodetic (left), and metres for Cartesian coordinates. par(mfrow = c(1, 2)) plot(DE, axes = TRUE) plot(DE.eqc, axes = TRUE) Figure 8.2: Germany in equidistant cylindrical projection: left with degrees, right with metres along the axes 8.1.1 What is a good projection for my data? There is unfortunately no silver bullet here. Projections that maintain all distances do not exist; only globes do. The mostly used projections try to preserve areas (equal area), directions (conformal, e.g. Mercator), some properties of distances (e.g. equirectangular preserves distances along meridians, azimuthal equidistant preserves distances to a central point) or some compromise of these. Parameters of projections decide what is shown in the center of a map and what on the fringes, which areas are up and which are down, and which areas are most enlarged. All these choices are in the end political decisions. It is often entertaining and at times educational to play around with the different projections and understand their consequences. When the primary purpose of the map however is not to entertain or educate projection varieties, it may be preferrable to choose a well-known or less surprising projection, and move the discussion which projection should be preferred to a decision process on its own. 8.1.2 Does projection always work? No. Look for instance at the figure 8.1, right. Countries like the USA are half out-of-sight. Where is the California coast line drawn? The PROJ string used here was &quot;+proj=ortho +lat_0=30 +lon_0=-10&quot; and we can easily check what happens to a polygon that crosses the visible area by setting both parameters to 0: sq = rbind(c(-89,0), c(-89,1), c(-91,1), c(-91,0), c(-89,0)) pol = st_sfc(st_polygon(list(sq)), crs = 4326) (pol.o = st_transform(pol, &quot;+proj=ortho +lat_0=0 +lon_0=0&quot;))[[1]] #&gt; POLYGON ((-6377166 0, -6376194 111314, -6377166 0)) st_is_valid(pol.o, NA_on_exception=FALSE) #&gt; Error in CPL_geos_is_valid(st_geometry(x), as.logical(NA_on_exception)): Evaluation error: IllegalArgumentException: Invalid number of points in LinearRing found 3 - must be 0 or &gt;= 4. where we see that the polygon is not nicely cut along the visibility line, but that the invisible points are simply dropped. This leads in this case to an invalid geometry, and may in the case of 8.2 lead to straight lines that do not follow the map border circle. How was figure 8.2 created? By using a rather ugly script that used a projected half-sphere circle to first cooky-cut the part of the countries that would remain visible on this projection. The script is available, and so is its output. 8.2 Plotting points, lines, polygons, grid cells Since maps are just a special form of plots of statistical data, the usual rules hold. Frequently occuring challenges include: polygons may be very small, and vanish when plotted depending on the data, polygons for different features may well overlap, and be visible only partially; using transparent fill colors may help indentify them when points are plotted with symbols, they may easily overlap and be hidden; density maps (chapter 13) may be more helpful lines may be hard to read when coloured and may overlap regardless line width When plotting polygons filled with colors, one has the choice to plot polygon boundaries, or to suppress these. If polygon boundaries draw too much attention, an alternative is to colour them in a grey tone, or another color that doesn’t interfere with the fill colors. When suppressing boundaries entirely, polygons with (nearly) identical colors will melt together. If the property indicating the fill color is constant over the region, such as land cover type, this is OK. If the property is an aggregation, the region over which it was aggregated gets lost. Especially for extensive variables, e.g. the amount of people living in a polygon, this strongly misleads. But even with polygon boundaries, using filled polygons for such variables may not be a good idea. The use of continuous color scales for continuously varying variables may look attractive, but is often more fancy than useful: it impracticle to match a color on the map with a legend value colors ramps often stretch non-linearly over the value range Only for cases where the identification of values is less important than the continuity of the map, such as the coloring of a high resolution digital terrain model, it does serve its goal. 8.3 Class intervals When plotting continuous geometry attributes using a limited set of colors (or symbols), classes need to be made from the data. The R package classInt (R. Bivand 2019a) provides a number of methods to do so. Using it is quite simple: library(classInt) r = rnorm(100) classIntervals(r) #&gt; style: quantile #&gt; one of 1.49e+10 possible partitions of this variable into 8 classes #&gt; [-2.61,-1.26) [-1.26,-0.356) [-0.356,-0.131) [-0.131,0.091) #&gt; 13 12 13 12 #&gt; [0.091,0.433) [0.433,0.623) [0.623,1.11) [1.11,2.76] #&gt; 12 13 12 13 classIntervals(r)$brks #&gt; [1] -2.612 -1.257 -0.356 -0.131 0.091 0.433 0.623 1.113 2.755 and it takes argument n for the number of intervals, and a style that can be one of “fixed”, “sd”, “equal”, “pretty”, “quantile”, “kmeans”, “hclust”, “bclust”, “fisher”, or “jenks”. Style “pretty” may not obey n; if if n is missing, ‘nclass.Sturges’ is used; two other methods are available for choosing n automatically. 8.4 Poles and datelines Given the linestring (ls = st_sfc(st_linestring(rbind(c(-179.5, 52), c(179.5, 52))), crs = 4326)) #&gt; Geometry set for 1 feature #&gt; geometry type: LINESTRING #&gt; dimension: XY #&gt; bbox: xmin: -180 ymin: 52 xmax: 180 ymax: 52 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; LINESTRING (-180 52, 180 52) How long a distance does it span? Let’s see: st_length(ls) #&gt; 68677 [m] which seems sensible. But does ls actually intersect with the dateline? dateline = st_sfc(st_linestring(rbind(c(180, 51), c(180, 53))), crs = 4326) st_intersects(ls, dateline) #&gt; although coordinates are longitude/latitude, st_intersects assumes that they are planar #&gt; Sparse geometry binary predicate list of length 1, where the predicate was `intersects&#39; #&gt; 1: (empty) … it seems not? How can this be? The warning said it all: if ls is not in spherical coordinates, it means it follows the 52th parallel, crossing (0, 52) half way. This is as if we drew a straight line between the two points on the left figure of 8.1, almost across the complete 52N parallel. Where do these inconsistencies come from? The software sf is built upon (see the C/C++ libraries box in figure 1.4) is a GIS software stack that originally targeted flat, 2D geometries. The simple feature standard assumes straight lines between points, but great circle segments are not straight. The functions that deal with spherical geometry, such as st_length, use PostGIS extensions in liblwgeom that were added later on to PostGIS, without rewriting the entire geometry core for geodetic coordinates. More recent systems, including Google’s S21, BigQuery GIS2 and Ubers H33 were written from scratch with global data in mind, and work exclusively with geodetic coordinates. 8.4.1 st_wrap_dateline The st_wrap_dateline function can be convenient, (ls.w = st_wrap_dateline(ls))[[1]] #&gt; MULTILINESTRING ((-180 52, -180 52), (180 52, 180 52)) as it cuts any geometry crossing the dateline into MULTI-geometries of which the sub-geometries touch on, but no longer cross the dateline. This is in particular convenient for plotting geodetic coordinates using naive approaches such as that of figure 8.1 left, where they would have crossed the entire plotting area. Note that by cutting the line at (180,52), st_wrap_dateline does not follow a great circle; for this, it should be preceded by st_segmentize, as e.g. in (ls.w2 = st_wrap_dateline(st_segmentize(ls, units::set_units(30, km))))[[1]] #&gt; MULTILINESTRING ((-180 52, -180 52, -180 52), (180 52, 180 52, 180 52)) Also note that bounding boxes like st_bbox(ls.w) #&gt; xmin ymin xmax ymax #&gt; -180 52 180 52 simply take the coordinate ranges, and are pretty much meaningless as descriptors of the extent of a geometry for geometries that cross the dateline. Similar notions hold for the poles; a polygon enclosing the North pole pole = st_sfc(st_polygon(list(rbind(c(0,80), c(120,80), c(240,80), c(0,80)))), crs = 4326) does not include the pole st_intersects(pole, st_sfc(st_point(c(0,90)), crs = 4326)) #&gt; although coordinates are longitude/latitude, st_intersects assumes that they are planar #&gt; Sparse geometry binary predicate list of length 1, where the predicate was `intersects&#39; #&gt; 1: (empty) (Cartesian interpretation) but has a positive area st_area(pole) #&gt; 1.63e+12 [m^2] indicating again a geodetic interpretation. 8.5 Graticules and other navigation aids Graticules are lines on a map that follow constant latitude or longitude values. On figure 8.1 left they are drawn in grey. Graticules are often drawn in maps to give reference where something is. In our first map in figure 1.1 we can read that the area plotted is near 35\\(^o\\) North and 80\\(^o\\) West. Had we plotted the lines in the projected coordinate system, they would have been straight and their actual numbers would not have been very informative, apart from giving an interpretation of size or distances when the unit is known, and familiar to the map reader. Graticules, by that, also shed light on which projection was used: equirectangular or Mercator projections will have straight vertical and horizontal lines, conic projections have straight but diverging meridians, equal area may have curved meridians The real navigation aid on figure 8.1 and most other maps are geographical features like the state outline, country outlines, coast lines, rivers, roads, railways and so on. If these are added sparsely and sufficiently, graticules can as well be omitted. In such cases, maps look good without axes, tics, and labels, leaving up a lot of plotting space to be filled with actual map data. References "],
["plot.html", "Chapter 9 Base and grid plots 9.1 Base plots 9.2 Combining base plots 9.3 Grid plots and viewports", " Chapter 9 Base and grid plots With base plots we mean the plot methods as offered by base R. Higher-level plots created with ggplot2, which are built on top of the grid package, are discussed in chapter 10. 9.1 Base plots The nice thing about base plots is that they work relatively simple, and can be built incrementally: after plotting a first element, it is possible to add further elements, like titles, axes, legends, annotations, and so on. Making more complex plot such as faceted plots is more of a challenge with base plot. They may also be faster than e.g. ggplot2. This chapter focuses on the base plot methods for sf and stars objects, and in particular where they deviate from what one expects from base R plots: default colors, color key placement, and multiple maps. 9.1.1 plot.sf defaults When we plot an sf object with multiple attributes, we get to see multiple attributes, as shown in figure 9.1; the warning indicates that the number of attributes plotted has been limited (max.plot), that by default no key is shown when multiple attributes are plotted, and that factor or character columns are plotted with a categorical scale, and numeric variables by a continuous color scale. system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;) %&gt;% read_sf -&gt; nc plot(nc) #&gt; Warning: plotting the first 10 out of 14 attributes; use max.plot = 14 to #&gt; plot all Figure 9.1: default plot with multiple attributes When the attributes values have a common reference system it makes sense to plot a key, and this can be done by suppressPackageStartupMessages(library(dplyr)) nc %&gt;% select(SID74, SID79) %&gt;% plot(key.pos = 4) # 1: below; 4: right Figure 9.2: two attributes sharing a key 9.1.2 Controlling color, color breaks, key The color of each feature geometry, be it a point, line or polygon, can be controlled by passing a single value to argument col, or to pass a vector with colors of length equal to the number of features. In that case, one does not get an automatic key, since the mapping of values to the key is unclear. One can get further control over the key by specifying either of breaks, to one of the classInt::classIntervals styles (“fixed”, “sd”, “equal”, “pretty”, “quantile”, “kmeans”, “hclust”, “bclust”, “fisher”, or “jenks”), or to the numeric vector of (increasing) class break values, nbreaks to set the number of color breaks, and pal, to pass a function that generates a palette, e.g. rainbow or viridis::viridis. logz which causes legend values to be plotted as 10-powers at which controls the values plotted along the key key.pos plots the key beneath (1), left (2), over (3) or to the right (4) of the map(s), and is omitted if NULL. An example is shown in figure 9.3. plot(nc[&quot;SID74&quot;], logz = TRUE, pal = viridis::viridis, breaks = c(0,.5,1,1.5,2,2.5), at = c(0,.5,1,1.5,2,2.5), key.width = lcm(1.3), key.length = 1) Figure 9.3: controlling key color and breaks; log transform causes zero values to remain uncolored; custom key size parameters For factor variables, the key shows factor levels, and key.length and/or key.width may need to be further controlled to get them look good. 9.1.3 Incrementally adding plot elements We can add elements to maps without keys by simply adding add = TRUE. When a map has a key, the initial plot command needs to have reset = FALSE. The reason for this is as follows. When plotting a map with a key on the side, in base plot one needs to cut up the plotting region in a region for the map, and a region for the key, similar to using par(mfrow = c(1,2)). In order to not keep the plot region splitting active for subsequent, unrelated plots, it is removed before the plot function returns. This means that by default one cannot add elements to a plot. In case we want to add elements to a plot, we need to instruct plot not to reset, and then can add elements if we set add = TRUE, as shown in figure 9.4. plot(nc[&quot;SID74&quot;], pal = viridis::viridis, reset = FALSE, graticule = TRUE, axes = TRUE) plot(st_point(c(-81.498,36.434)), col = &#39;red&#39;, pch = 1, cex = 4, lwd = 3, add = TRUE) layout(matrix(1)) # manually reset the plotting region split-up Figure 9.4: adding plot elements to a map with a key; graticule and axes 9.1.4 plotting graticules and axes A graticule (section 8.5) is added if graticule = TRUE is set; figure @(fig:figadd) gives an example. Axes are by default omitted: they take space, and often other map elements, such as boundaries or coast lines, are sufficient for orientation. Axes can be added with axes = TRUE. They are given in values of the coordinate reference system of the data, unless a graticule is added, in which case they correspond to the graticule values. 9.1.5 plot.stars defaults The base plot method for stars objects follows a number of parameters that were also discussed above for plot.sf. These are: key.pos, logz, axes, reset, key.width, key.length nbreaks, and breaks. The default for breaks is &quot;quantile&quot;; along with the default join_zlim = TRUE this results in quantile values taken from all of the images shown in a composite plot to yeild color breaks. The result of this is shown in figure 4.1, the default stars plot for a multi-band raster. If join_zlim is FALSE, color breaks are computed separately for each subplot, and a joint legend is not possible, hence, no legend is drawn. For large datasets with more than 10000 pixels, a (regular) sample of size 10000 is taken to determine quantiles, rather than using the entire dataset. The col argument provides the pallette used, and not a color vector for each pixel; by default a linear grey scale is used. box_col is the color used for the boxes around subplots. With the default of downsample = TRUE, plot.stars does not many more pixels than can be seen on the plotting device. As shown in the left subfigure of 4.2, the rgb parameter can be used to specify three bands for creating rgb colors. maxColorValue, by default the maximum of all bands, can be used to specify the maximum pixel values for scaling pixel values into rgb components. The effect of setting text_values and interpolate are shown in figure 9.5 showing a 10 x 10 cell raster. text_values prints cell values as text, e.g. for numeric examples. interpolate interpolates rgb values, and seems to limit its extent to the raster cell centres, rather than corners. tif = system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;) library(stars) x = read_stars(tif) par(mfrow = c(1, 2)) plot(x[,1:10,1:10,1], text_values=TRUE, key.pos = NULL, col = viridis::viridis(10), reset = FALSE) plot(x[,1:10,1:10,1:3], rgb=1:3, interpolate = TRUE, reset = FALSE) Figure 9.5: stars plots with text values (left) and interpolated rgb colors (right); both maps plot a 10 x 10 raster Some rasters cannot be plotted by R’s image, but have to be plotted as features (polygons or points); examples are rotated, sheared and curvilinear grids. Figure 9.6 shows a sheared grid, and illustrates st_as_sf for conversion to simple features, merge for whether to merge polygons with identical values, and as_points to plot raster cells as points. par(mfrow = c(1, 3)) xs = adrop(x[,1:10,1:10,1]) attr(attr(xs, &quot;dimensions&quot;), &quot;raster&quot;)$affine = c(1, 3) plot(xs, col = viridis::viridis(10), key.pos = NULL, reset = FALSE) plot(st_as_sf(xs, as_points = FALSE, merge = FALSE), pal = viridis::viridis, key.pos = NULL, reset = FALSE) plot(st_as_sf(xs, as_points = TRUE, merge = FALSE), key.pos = NULL, pal = viridis::viridis, pch = 16, reset = FALSE) Figure 9.6: sheared rasters, plotted as simple features; left: with merged polygon features, middle: with seperate polygons, right: as point features 9.2 Combining base plots Figure 9.6 is an exmaple of multiple plots of stars and sf objects in one plot. They could be combined because they lack the automatic legend key. The explanation about the need for parameter reset and the examples above with multiple subplots setting it to FALSE already show the limitations of this approach: if we want a key next to each map, we cannot easily combine maps in subplots, using sf‘s or stars’ plot methods. If we can do without the automatic legend, all is fine. A more flexible approach to combine maps and map elements into a composite is using package grid as shown in the next section, or ggplot facet plots (which also use grid) as will be shown in chapter 10 about ggplot2. 9.3 Grid plots and viewports Package grid, one of the R base packages, takes a more structured approach to building plots than base plot does. It allows the creation of graphic objects (objects of class grob, or grobs) that contain all the information for plotting, and the definition of viewports, plotting subregions within which coordinate systems can easily be redefined. Packages like ggplot2 (chapter 10) and lattice are built upon the logic provided by grid. Package sf contains st_as_grob methods for all feature geometry classes. Figure 9.7 shows a simple plot of all the nc counties drawn using grid functions. library(grid) system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;) %&gt;% read_sf -&gt; nc st_viewport(nc) %&gt;% pushViewport st_geometry(nc) %&gt;% lapply(st_as_grob) %&gt;% lapply(grid.draw) -&gt; x Figure 9.7: simple plot created with grid functions The st_as_grob methods are exported by sf and used by ggplot2::geom_sf to convert geometries into plot-able objects; this way, ggplot2 needs no knowledge on how geometries are stored in R objects. "],
["ggplot2.html", "Chapter 10 ggplot2 10.1 geom_sf 10.2 geom_stars", " Chapter 10 ggplot2 Function ggplot from package ggplot2 (Wickham 2016) provides a high-level interface to creating graphs, essentially by composing all their ingredients and constraints in a single expression. It implements the “grammar for graphics” by Wilkinson (2006), and is the plotting package of choice in the tidyverse. Before ggplot 3.0.0 came out, the approach to plotting spatial data was to fortify it, meaning encode all the geometries as atomic vectors in a data.frame, keeping an ID column to register which coordinate belonged to which geometry, and repeating all non-geometry attributes for each geometry coordinate. This worked reasonably well for points and lines, but not very well for polygons with holes. Since ggplot 3.0.0 and package sf, this runs much smoother; ggplot has received a geom_sf function that could take an sf object and calls st_as_grob on each feature geometry to get an object that can directly be added to the plot. In addition to that, it takes care of automated datum transformations or projections if different objects have differing coordinate reference systems, and adds by default a graticule and degree axis tic labels. Moreno and Basille (2018a), Moreno and Basille (2018b) and Moreno and Basille (2018c) published three guest blogs on r-spatial.org explaining the capabilities of ggplot for making beautiful maps with sf and ggplot2. 10.1 geom_sf We will introduce the affordances of geom_sf here step by step. We use a projected version of nc library(tidyverse) library(sf) system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;) %&gt;% read_sf() %&gt;% st_transform(32119) -&gt; nc.32119 and create a first ggplot by ggplot() + geom_sf(data = nc.32119) It is attractive to think that ggplot(nc.32119) + geom_sf() would also work, but it doesn’t – it only works if the geometry column is named geometry, which is not always the case (sf objects may have more than one geometry column). If we want to get rid of the axis tics and grid lines, we could use ggplot() + geom_sf(data = nc.32119) + theme_void() + theme(panel.grid.major = element_line(color = &quot;white&quot;)) A first ggplot2 plot with polygons colored by attributes (as in figure 1.2) is created by ggplot() + geom_sf(data = nc.32119) + aes(fill = BIR74) + scale_fill_gradientn(colors = viridis::viridis(20)) 10.1.1 facet plots Facet plots are a powerful means to compare maps, because they keep all the plotting parameters constant (spatial extent, scale, color breaks). One would perhaps wish that it were possible to directly plot multiple attribute columns over facet maps. For this, we first have to reorganise the data such that the target variable is a single column, another column indicates the facet, and geometries are repeated accordingly. For this, tidyr::gather can be used; an example is given in figure 1.3. 10.1.2 multiple geometries in a single map Multiple geometries with geom_sf can be created by adding consecutive geom_sf geometries; we can add for instance the county centroids to a plot by ggplot() + geom_sf(data = nc.32119) + geom_sf(data = st_centroid(nc.32119)) #&gt; Warning in st_centroid.sf(nc.32119): st_centroid assumes attributes are #&gt; constant over geometries of x When subsequent sf objects have a different coordinate reference system from the first object, geom_sf will transform them to the reference system of the first object. 10.1.3 Fine tuning In case sf objects have multiple geometry list-columns, the “active” list column is selected by default. This can be overriden by supplying another column name in aes(geometry = my_column) ` To add labels to geometries, one can use geom_sf_label. From the examples of geom_sf: nc &lt;- sf::st_read(system.file(&quot;gpkg/nc.gpkg&quot;, package = &quot;sf&quot;), quiet = TRUE) nc_3857 &lt;- sf::st_transform(nc, &quot;+init=epsg:3857&quot;) ggplot() + geom_sf(data = nc_3857[1:3, ], aes(fill = AREA)) + geom_sf_label(data = nc_3857[1:3, ], aes(label = NAME)) geom_sf_text can be used to add simple text annotations without decoration. Graticules are drawn by default. Since they are different for every projection, it is hard to anticipate how strongly they will be curved. geom_sf takes a parameter ndiscr, by default set to 100, which can be increased when graticulas show up unexpectedly as non-smooth lines. label_graticules can be used to control which graticules are labeled. In addition, label_axes controls on which axes particular graticules will be labeled. 10.2 geom_stars Package stars comes with a geom_stars function that is much more limited in scope than geom_sf. In essence, it creates a call to geom_raster in case of raster data with a regular grid, to geom_tile for other raster data, or to geom_sf if the stars object has a simple feature geometry dimension rather than raster dimensions, or has a curvilinear raster geom_raster also creates the mapping of dimension names to x and y-coordinates and set the first attribute name as the fill variable. This means that the aspect ratio still needs to be controlled (coord_equal()) and that a facet_wrap is needed to display multiple rasters. An example is shown in figure 10.1. library(stars) library(ggplot2) library(viridis) #&gt; Loading required package: viridisLite system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;) %&gt;% read_stars() -&gt; x ggplot() + geom_stars(data = x) + coord_equal() + facet_wrap(~band) + scale_fill_viridis() + theme_void() + scale_x_discrete(expand=c(0,0)) + scale_y_discrete(expand=c(0,0)) Figure 10.1: example of geom_stars geom_stars has a parameter, downsample, which can be used to downsample particular dimensions. Here we downsample a 90m x 90m raster to a 900m x 900m raster: ggplot() + geom_stars(data = x, downsample = c(10,10,1)) + coord_equal() + facet_wrap(~band) + scale_fill_viridis() + theme_void() + scale_x_discrete(expand=c(0,0))+ scale_y_discrete(expand=c(0,0)) library(spacetime) data(air) # this loads several datasets in .GlobalEnv d = st_dimensions(station = st_as_sfc(stations), time = dates) aq = st_as_stars(list(PM10 = air), dimensions = d) # ggplot() + geom_stars(data = aq[,,3000]) aq.sf = st_as_sf(aq[,,3000], long=TRUE) ggplot() + geom_sf(data = st_as_sf(DE_NUTS1)) + geom_sf(data = aq.sf, mapping = aes(col = PM10)) + ggtitle(aq.sf$time[1]) References "],
["interactive-maps.html", "Chapter 11 Interactive Maps", " Chapter 11 Interactive Maps base plot: identify, locator leaflet, tmap, mapview mapedit? "],
["summarizing-geometries.html", "Chapter 12 Summarizing Geometries", " Chapter 12 Summarizing Geometries Properties: dimension, length, area, etc, if not earlier in Ch 3? counts, density, intensity (units; meaningful) "],
["pp.html", "Chapter 13 Point Pattern Analysis", " Chapter 13 Point Pattern Analysis Basics PP, beyond counting; basic steps in PPA sf - spatstat interface; rasters; "],
["manipulating-attributes-summarise-aggregate-union-sample.html", "Chapter 14 Manipulating attributes: summarise, aggregate, union, sample", " Chapter 14 Manipulating attributes: summarise, aggregate, union, sample "],
["up-and-downscaling.html", "Chapter 15 Up- and Downscaling", " Chapter 15 Up- and Downscaling sampling largest sub-geometry area-weighted interpolation "],
["interpolation.html", "Chapter 16 Interpolation and Geostatistics 16.1 Preparing the air quality dataset 16.2 Sample variogram 16.3 Fitting variogram models 16.4 Kriging interpolation 16.5 Areal means: block kriging 16.6 Conditional and unconditional simulation 16.7 Trend models 16.8 Multivariable geostatistics 16.9 Spatiotemporal interpolation", " Chapter 16 Interpolation and Geostatistics Geostatistics is concerned with the modelling, prediction and simulation of spatially continuous phenomena. The typical problem is a missing value problem: we observe a property of a phenomenon \\(Z(s)\\) at a limited number of sample locations \\(s_i, i = 1,...,n\\), and are interested in the property value at all locations \\(s_0\\), so we have to predict it for unobserved locations. This is also called kriging, or Gaussian process prediction. In case \\(Z(s)\\) contains a white noise component \\(\\epsilon\\), as in \\(Z(s)=S(s)+\\epsilon(s)\\), an alternative but similar goal is to predict \\(S(s)\\), which may be called filtering or smoothing. In this chapter we will show simple approaches for handling geostatistical data, will demonstrate simple interpolation methods, explore modelling spatial correlation, spatial prediction and simulation. We will use package gstat (Pebesma and Graeler 2019, Pebesma (2004)), which offers a fairly wide palette of models and options for geostatistical analysis. 16.1 Preparing the air quality dataset The dataset we work with is an air quality dataset obtained from the European Environmental Agency (EEA). European member states report air quality measurements to this Agency. So-called validated data are quality controlled by member states, and are reported on a yearly basis. They form the basis for policy compliancy evaluations. The EEA’s air quality e-reporting website gives access to the data reported by European member states. We decided to download hourly (time series) data, which is the data primarily measured. A web form helps convert simple selection criteria into an http request. The following URL https://fme.discomap.eea.europa.eu/fmedatastreaming/AirQualityDownload/AQData_Extract.fmw?CountryCode=DE&amp;CityName=&amp;Pollutant=8&amp;Year_from=2017&amp;Year_to=2017&amp;Station=&amp;Samplingpoint=&amp;Source=E1a&amp;Output=TEXT&amp;UpdateDate= was created to select all validated (Source=E1a) \\(NO_2\\) (Pollutant=8) data for 2017 (Year_from, Year_to) from Germany (CountryCode=DE). It returns a text file with a set of URLs to CSV files, each containing the hourly values for the whole period for a single measurement station. These files were downloaded and converted to the right encoding using the dos2unix command line utility. In the following, we will read all the files into a list, files = list.files(&quot;aq&quot;, pattern = &quot;*.csv&quot;, full.names = TRUE) r = lapply(files[-1], function(f) read.csv(f)) then convert the time variable into a POSIXct variable, and time order them Sys.setenv(TZ = &quot;UTC&quot;) # make sure times are not interpreted as DST r = lapply(r, function(f) { f$t = as.POSIXct(f$DatetimeBegin) f[order(f$t), ] } ) and we deselect smaller datasets that do not contain hourly data: r = r[sapply(r, nrow) &gt; 1000] names(r) = sapply(r, function(f) unique(f$AirQualityStationEoICode)) length(r) == length(unique(names(r))) #&gt; [1] TRUE and then combine all files using xts::cbind, so that they are matched based on time: library(xts) r = lapply(r, function(f) xts(f$Concentration, f$t)) aq = do.call(cbind, r) We can now select those stations for which we have 75% of the hourly values measured, i.e. drop those with more than 25% hourly values missing: # remove stations with more than 75% missing values: sel = apply(aq, 2, function(x) sum(is.na(x)) &lt; 0.75 * 365 * 24) aqsel = aq[, sel] # stations are in columns Next, the station metadata was read and filtered for rural background stations in Germany by library(tidyverse) read.csv(&quot;aq/AirBase_v8_stations.csv&quot;, sep = &quot;\\t&quot;, stringsAsFactors = FALSE) %&gt;% as_tibble %&gt;% filter(country_iso_code == &quot;DE&quot;, station_type_of_area == &quot;rural&quot;, type_of_station == &quot;Background&quot;) -&gt; a2 These stations contain coordinates, and an sf object with (static) station metadata is created by library(sf) library(stars) a2.sf = st_as_sf(a2, coords = c(&quot;station_longitude_deg&quot;, &quot;station_latitude_deg&quot;), crs = 4326) We now subset the air quality data to stations that are of type rural background: sel = colnames(aqsel) %in% a2$station_european_code aqsel = aqsel[, sel] We can compute station means, and join these to stations locations by tb = tibble(NO2 = apply(aqsel, 2, mean, na.rm = TRUE), station_european_code = colnames(aqsel)) crs = 32632 right_join(a2.sf, tb) %&gt;% st_transform(crs) -&gt; no2.sf #&gt; Joining, by = &quot;station_european_code&quot; # load German boundaries data(air, package = &quot;spacetime&quot;) de &lt;- st_transform(st_as_sf(DE_NUTS1), crs) ggplot() + geom_sf(data = de) + geom_sf(data = no2.sf, mapping = aes(col = NO2)) 16.2 Sample variogram In order to make spatial predictions using geostatistical methods, we first need to identify a model for the mean and for the spatial correlation. In the simplest model, \\(Z(s) = m + e(s)\\), the mean is an unknown constant \\(m\\), and in this case the spatial correlation can be modelled using the variogram, \\(\\gamma(h) = 0.5 E (Z(s)-Z(s+h))^2\\). For processes with a finite variance \\(C(0)\\), the variogram is related to the covariogram or covariance function through \\(\\gamma(h) = C(0)-C(h)\\). The sample variogram is obtained by computing estimates of \\(\\gamma(h)\\) for distance intervals, \\(h_i = [h_{i,0},h_{i,1}]\\): \\[ \\hat{\\gamma}(h_i) = \\frac{1}{2N(h_i)}\\sum_{j=1}^{N(h_i)}(z(s_i)-z(s_i+h&#39;))^2, \\ \\ h_{i,0} \\le h&#39; &lt; h_{i,1} \\] with \\(N(h_i)\\) the number of sample pairs available for distance interval \\(h_i\\). Function gstat::variogram computes sample variograms. library(gstat) v = variogram(NO2~1, no2.sf) plot(v, plot.numbers = TRUE) This chooses default maximum distance (cutoff: one third of the length of the bounding box diagonal) and (constant) interval widths (width: cutoff divided by 15). These defaults can be changed, e.g. by library(gstat) v0 = variogram(NO2~1, no2.sf, cutoff = 100000, width = 10000) plot(v0, plot.numbers = TRUE) Note that the formula NO2~1 is used to select the variable of interest from the data file (NO2), and to specify the mean model: ~1 refers to an intercept-only (unknown, constant mean) model. 16.3 Fitting variogram models In order to progress toward spatial predictions, we need a variogram model \\(\\gamma(h)\\) for (potentially) all distances \\(h\\), rather than the set of estimates derived above: in case we would for instance connect these estimates with straight lines, or assume they reflect constant values over their respective distance intervals, this would lead to statisical models with non-positive covariance matrices, which is a complicated way of expressing that you are in a lot of trouble. To avoid these troubles we fit parametric models \\(\\gamma(h)\\) to the estimates \\(\\hat{\\gamma}(h_i)\\), where we take \\(h_i\\) as the mean value of all the \\(h&#39;\\) values involved in estimating \\(\\hat{\\gamma}(h_i)\\). For this, when we fit a model like the exponential variogram, v.m = fit.variogram(v, vgm(1, &quot;Exp&quot;, 50000, 1)) plot(v, v.m, plot.numbers = TRUE) the fitting is done by minimizing \\(\\sum_{i=1}^{n}w_i(\\gamma(h_i)-\\hat{\\gamma}(h_i))^2\\), with \\(w_i\\) by default equal to \\(N(h_i)/h^2\\), other fitting schemes are available through argument fit.method. 16.4 Kriging interpolation Kriging involves the prediction of \\(Z(s_0)\\) at arbitrary locations \\(s_0\\). Typically, when we interpolate a variable, we do that on points on a regular grid covering the target area. When we take the grid points such that they align with the points of our graphics device (screen), then nobody will notice as long as the graphic device is not resized. We first create a stars object with a raster covering the target area, and NA’s outside it: # build a grid over Germany: bb = st_bbox(de) dx = seq(bb[1], bb[3], 10000) dy = seq(bb[4], bb[2], -10000) # decreases! st_as_stars(matrix(0, length(dx), length(dy))) %&gt;% st_set_dimensions(1, dx) %&gt;% st_set_dimensions(2, dy) %&gt;% st_set_dimensions(names = c(&quot;x&quot;, &quot;y&quot;)) %&gt;% st_set_crs(crs) -&gt; grd i = st_intersects(grd, de) grd[[1]][lengths(i)==0] = NA grd #&gt; stars object with 2 dimensions and 1 attribute #&gt; attribute(s): #&gt; A1 #&gt; Min. :0 #&gt; 1st Qu.:0 #&gt; Median :0 #&gt; Mean :0 #&gt; 3rd Qu.:0 #&gt; Max. :0 #&gt; NA&#39;s :1793 #&gt; dimension(s): #&gt; from to offset delta refsys point values #&gt; x 1 65 280741 10000 +proj=utm +zone=32 +datum... FALSE NULL [x] #&gt; y 1 87 6101239 -10000 +proj=utm +zone=32 +datum... FALSE NULL [y] Then, we can krige by using gstat::krige, with the model for the trend, the data, the prediction grid, and the variogram model: k = krige(NO2~1, no2.sf, grd, v.m) #&gt; [using ordinary kriging] ggplot() + geom_stars(data = k, aes(fill = var1.pred, x = x, y = y)) + geom_sf(data = st_cast(de, &quot;MULTILINESTRING&quot;)) + geom_sf(data = no2.sf) 16.5 Areal means: block kriging a = aggregate(no2.sf[&quot;NO2&quot;], de, mean) b = krige(NO2~1, no2.sf, de, v.m) #&gt; [using ordinary kriging] b$sample = a$NO2 b$kriging = b$var1.pred b %&gt;% select(sample, kriging) %&gt;% gather(var, NO2, -geometry) -&gt; b2 ggplot() + geom_sf(data = b2, mapping = aes(fill = NO2)) + facet_wrap(~var) + scale_fill_gradientn(colors = sf.colors(20)) SE = function(x) sqrt(var(x)/length(x)) a = aggregate(no2.sf[&quot;NO2&quot;], de, SE) b$sample = a$NO2 b$kriging = sqrt(b$var1.var) b %&gt;% select(sample, kriging) %&gt;% gather(var, NO2, -geometry) -&gt; b2 ggplot() + geom_sf(data = b2, mapping = aes(fill = NO2)) + facet_wrap(~var) + scale_fill_gradientn(colors = sf.colors(20)) 16.6 Conditional and unconditional simulation library(viridis) s = krige(NO2~1, no2.sf, grd, v.m, nmax = 30, nsim = 10) #&gt; drawing 10 GLS realisations of beta... #&gt; [using conditional Gaussian simulation] g = ggplot() + coord_equal() + scale_fill_viridis() + theme_void() + scale_x_discrete(expand=c(0,0)) + scale_y_discrete(expand=c(0,0)) g + geom_stars(data = s[,,,1:6]) + facet_wrap(~sample) 16.7 Trend models Kriging and conditional simulation, as used so far in this chapter, assume that all spatial variability is a random process, characterized by a spatial covariance model. In case we have other variables that are meaningfully correlated with the target variable, we can use them in a linear regression model for the trend, \\[ Z(s) = \\sum_{j=0}^p \\beta_j X_p(s) + e(s) \\] with \\(X_0(s) = 1\\) and \\(\\beta_0\\) an intercept, but with the other \\(\\beta_j\\) regression coefficients. This typically reduces both the spatial correlation in the residual \\(e(s)\\), as well as its variance, and leads to more accurate predictions and more similar conditional simulations. 16.7.1 A population grid As a potential predictor for NO2 in the air, we use population density. NO2 is mostly caused by traffic, and traffic is stronger in more densely populated areas. Population density is obtained from the 2011 census, and is downloaded as a csv file with the number of inhabitants per 100 m grid cell. We can aggregate these data to the target grid cells by summing the inhabitants: library(vroom) v = vroom(&quot;aq/pop/Zensus_Bevoelkerung_100m-Gitter.csv&quot;) #&gt; Observations: 35,785,840 #&gt; Variables: 4 #&gt; chr [1]: Gitter_ID_100m #&gt; dbl [3]: x_mp_100m, y_mp_100m, Einwohner #&gt; #&gt; Call `spec()` for a copy-pastable column specification #&gt; Specify the column types with `col_types` to quiet this message v %&gt;% filter(Einwohner &gt; 0) %&gt;% select(-Gitter_ID_100m) %&gt;% st_as_sf(coords = c(&quot;x_mp_100m&quot;, &quot;y_mp_100m&quot;), crs = 3035) %&gt;% st_transform(st_crs(grd)) -&gt; b a = aggregate(b, st_as_sf(grd, na.rm = FALSE), sum) Now we have the population counts per grid cell in a. To get to population density, we need to find the area of each cell; for cells crossing the country border, this will be less than 10 x 10 km: grd$ID = 1:prod(dim(grd)) # so we can find out later which grid cell we have ii = st_intersects(grd[&quot;ID&quot;], st_cast(st_union(de), &quot;MULTILINESTRING&quot;)) grd_sf = st_as_sf(grd[&quot;ID&quot;], na.rm = FALSE)[lengths(ii) &gt; 0,] iii = st_intersection(grd_sf, st_union(de)) #&gt; Warning: attribute variables are assumed to be spatially constant #&gt; throughout all geometries grd$area = st_area(grd)[[1]] + units::set_units(grd$A1, m^2) # NA&#39;s grd$area[iii$ID] = st_area(iii) Instead of doing the two-stage procedure above: first finding cells that have a border crossing it, then computing its area, we could also directly use st_intersection on all cells, but that takes considerably longer. From the counts and areas we can compute densities, and verify totals: grd$pop_dens = a$Einwohner / grd$area sum(grd$pop_dens * grd$area, na.rm = TRUE) # verify #&gt; [1] 80323301 sum(b$Einwohner) #&gt; [1] 80324282 g + geom_stars(data = grd, aes(fill = sqrt(pop_dens), x = x, y = y)) We need to divide the number of inhabitants by the number of 100 m grid points contributing to it, in order to convert population counts into population density. To obtain population density values at monitoring network stations, we can use (a = aggregate(grd[&quot;pop_dens&quot;], no2.sf, mean)) #&gt; stars object with 1 dimensions and 1 attribute #&gt; attribute(s): #&gt; pop_dens #&gt; Min. :0.000 #&gt; 1st Qu.:0.000 #&gt; Median :0.000 #&gt; Mean :0.000 #&gt; 3rd Qu.:0.000 #&gt; Max. :0.002 #&gt; NA&#39;s :1 #&gt; dimension(s): #&gt; from to offset delta refsys point #&gt; geometry 1 74 NA NA +proj=utm +zone=32 +datum... TRUE #&gt; values #&gt; geometry POINT (439814 5938977),...,POINT (456668 5436135) no2.sf$pop_dens = st_as_sf(a)[[1]] summary(lm(NO2~sqrt(pop_dens), no2.sf)) #&gt; #&gt; Call: #&gt; lm(formula = NO2 ~ sqrt(pop_dens), data = no2.sf) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -7.87 -1.82 -0.49 1.56 8.11 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 4.627 0.693 6.67 4.6e-09 *** #&gt; sqrt(pop_dens) 321.802 49.660 6.48 1.0e-08 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3.13 on 71 degrees of freedom #&gt; (1 observation deleted due to missingness) #&gt; Multiple R-squared: 0.372, Adjusted R-squared: 0.363 #&gt; F-statistic: 42 on 1 and 71 DF, p-value: 1.04e-08 and the corresponding scatterplot is shown in 16.1. Figure 16.1: Scatter plot of 2017 annual mean NO2 concentration against population density, for rural background air quality stations Prediction under this new model involves first modelling a residual variogram, by no2.sf = no2.sf[!is.na(no2.sf$pop_dens),] vr = variogram(NO2~sqrt(pop_dens), no2.sf) vr.m = fit.variogram(vr, vgm(1, &quot;Exp&quot;, 50000, 1)) plot(vr, vr.m, plot.numbers = TRUE) and subsequently, kriging prediction is done by kr = krige(NO2~sqrt(pop_dens), no2.sf, grd[&quot;pop_dens&quot;], vr.m) #&gt; [using universal kriging] k$kr1 = k$var1.pred k$kr2 = kr$var1.pred st_redimension(k[c(&quot;kr1&quot;, &quot;kr2&quot;)], along = list(what = c(&quot;kriging&quot;, &quot;residual kriging&quot;))) %&gt;% setNames(&quot;NO2&quot;) -&gt; km g + geom_stars(data = km, aes(fill = NO2, x = x, y = y)) + geom_sf(data = st_cast(de, &quot;MULTILINESTRING&quot;)) + geom_sf(data = no2.sf) + facet_wrap(~what) #&gt; Coordinate system already present. Adding new coordinate system, which will replace the existing one. where, critically, the pop_dens values are now available for prediction locations in object grd. We see some clear differences: the map with population density in the trend follows the extremes of the population density rather than those of the measurement stations, and has a range that extends that of the former. It should be taken with a large grain of salt however, since the stations used were filtered for the category “rural background”, indicating that they represent conditions of lower populations density. The scatter plot of Figure 16.1 reveals that the the population density at the locations of stations is much more limited than that in the population density map, and hence the right-hand side map is based on strongly extrapolating the relationship shown in 16.1. 16.8 Multivariable geostatistics Multivariable geostatics involves the joint modelling, prediction and simulation of multiple variables, \\[Z_1(s) = X_1 \\beta_1 + e_1(s)\\] \\[...\\] \\[Z_n(s) = X_n \\beta_n + e_n(s).\\] In addition to having observations, trend models, and variograms for each variable, the cross variogram for each pair of residual variables, describing the covariance of \\(e_i(s), e_j(s+h)\\), is required. If this cross covariance is non-zero, knowledge of \\(e_j(s+h)\\) may help predict (or simulate) \\(e_i(s)\\). This is especially true if \\(Z_j(s)\\) is more densely sample than \\(Z_i(s)\\). Prediction and simulation under this model are called cokriging and cosimulation. Examples using gstat are found when running the demo scripts library(gstat) demo(cokriging) demo(cosimulation) and are illustrated and discussed in (Bivand, Pebesma, and Gomez-Rubio 2013). In case the different variables considered are observed at the same set of locations, for instance different air quality parameters, then the statistical gain of using cokriging as opposed to direct (univariable) kriging is often modest, when not negligible. A gain may however be that the prediction is truly multivariable: in addition to the prediction vector \\(\\hat{Z(s_0)}=(\\hat{Z}_1(s_0),...,\\hat{Z}_n(s_0))\\) we get the full covariance matrix of the prediction error (Ver Hoef and Cressie 1993). This means for instance that if we are interested in some linear combination of \\(\\hat{Z}(s_0)\\), such as \\(\\hat{Z}_2(s_0) - \\hat{Z}_1(s_0)\\), that we can get the standard error of that combination because we have the correlations between the prediction errors. Although sets of direct and cross variograms can be computed and fitted automatically, multivariable geostatistical modelling becomes quickly hard to manage when the number of variables gets large, because the number of direct and cross variograms required is \\(n(n+1)/2\\). In case different variables refer to the same variable take at different time steps, one could use a multivariable (cokriging) prediction approach, but this would not allow for e.g. interpolation between two time steps. For this, and for handling the case of having data observed at many time instances, one can also model its variation as a function of continuous space and time, as of \\(Z(s,t)\\), which we will do in the next section. 16.9 Spatiotemporal interpolation Spatiotemporal geostatistical processes are modelled as variables having a value everywhere in space and time, \\(Z(s,t)\\), with \\(s\\) and \\(t\\) the continuously indext space and time index. Given observations \\(Z(s_i,t_j)\\) and a variogram (covariance) model \\(\\gamma(s,t)\\) we can predict \\(Z(s_0,t_0)\\) at arbitrary space/time locations \\((s_0,t_0)\\) using standard Gaussian process theory. Several books have been written recently about modern approaches to handling and modelling spatiotemporal geostatistical data, including (Wikle, Zammit-Mangion, and Cressie 2019) and (Blangiardo and Cameletti 2015). Here, we will use (Gräler, Pebesma, and Heuvelink 2016) and give some simple examples building upon the dataset used throughout this chapter. 16.9.1 A spatiotemporal variogram model Starting with the spatiotemporal matrix of NO\\(_2\\) data in aq constructed at the beginning of this chapter, we will first select the rural background stations: aqx = aq[,colnames(aq) %in% a2$station_european_code] sfc = st_geometry(a2.sf)[match(colnames(aqx), a2.sf$station_european_code)] st_as_stars(NO2 = as.matrix(aqx)) %&gt;% st_set_dimensions(names = c(&quot;time&quot;, &quot;station&quot;)) %&gt;% st_set_dimensions(&quot;time&quot;, index(aqx)) %&gt;% st_set_dimensions(&quot;station&quot;, sfc) -&gt; no2.st v.st = variogramST(NO2~1, no2.st[,1:(24*31)], tlags = 0:48) v1 = plot(v.st) v2 = plot(v.st, map = FALSE) print(v1, split = c(1,1,2,1), more = TRUE) print(v2, split = c(2,1,2,1), more = FALSE) # product-sum prodSumModel &lt;- vgmST(&quot;productSum&quot;, space=vgm(150, &quot;Exp&quot;, 200, 0), time= vgm(20, &quot;Sph&quot;, 40, 0), k=2) StAni = estiStAni(v.st, c(0,200000)) (fitProdSumModel &lt;- fit.StVariogram(v.st, prodSumModel, fit.method = 7, stAni = StAni, method = &quot;L-BFGS-B&quot;, control = list(parscale = c(1,10,1,1,0.1,1,10)), lower = rep(0.0001, 7))) #&gt; space component: #&gt; model psill range #&gt; 1 Nug 26.3 0 #&gt; 2 Exp 140.5 432 #&gt; time component: #&gt; model psill range #&gt; 1 Nug 1.21 0.0 #&gt; 2 Sph 15.99 40.1 #&gt; k: 0.0322468133959116 plot(v.st, fitProdSumModel, wireframe=FALSE, all=TRUE, scales=list(arrows=FALSE), zlim=c(0,150)) which can also be plotted as wireframes, by plot(v.st, model=fitProdSumModel, wireframe=TRUE, all=TRUE, scales=list(arrows=FALSE), zlim=c(0,185)) Hints about the fitting strategy and alternative models for spatiotemporal variograms are given in (Gräler, Pebesma, and Heuvelink 2016). With this fitted model, and given the observations, we can carry out kriging or simulation at arbitrary points in space and time. For instance, we could estimate (or simulate) values in the time series that are now missing: this occurs regularly, and in section 16.4 we used means over time series based on simply ignoring up to 25% of the observations: substituting these with estimated or simulated values based on neigbouring (in space and time) observations before computing yearly mean values seems a more reasonable approach. More in general, we can estimate at arbitrary locations and time points, and we will illustrate this with predicting time series at particular locations, and and predicting spatial slices. We can create a stars object to denote two points and all time instances with set.seed(123) pt = st_sample(de, 2) t = st_get_dimension_values(no2.st, 1) st_as_stars(list(pts = matrix(1, length(t), length(pt)))) %&gt;% st_set_dimensions(names = c(&quot;time&quot;, &quot;station&quot;)) %&gt;% st_set_dimensions(&quot;time&quot;, t) %&gt;% st_set_dimensions(&quot;station&quot;, pt) -&gt; new_pt no2.st &lt;- st_transform(no2.st, crs) new_ts &lt;- krigeST(NO2~1, data = no2.st[&quot;NO2&quot;], newdata = new_pt, nmax = 50, stAni = StAni, modelList = fitProdSumModel, progress = FALSE) plot(xts(t(new_ts[[2]]), t), type = &#39;l&#39;) t4 = t[(1:4 - 0.5) * (3*24*30)] d = dim(grd) st_as_stars(pts = array(1, c(d[1], d[2], time=length(t4)))) %&gt;% st_set_dimensions(&quot;time&quot;, t4) %&gt;% st_set_dimensions(&quot;x&quot;, st_get_dimension_values(grd, &quot;x&quot;)) %&gt;% st_set_dimensions(&quot;y&quot;, st_get_dimension_values(grd, &quot;y&quot;)) %&gt;% st_set_crs(crs) -&gt; grd.st new_int &lt;- krigeST(NO2~1, data = no2.st[&quot;NO2&quot;], newdata = grd.st, nmax = 50, stAni = StAni, modelList = fitProdSumModel, progress = FALSE) plot(new_int[2,,,1], reset = FALSE) plot(de, col = NA, border = &#39;red&#39;, add = TRUE) #&gt; Warning in plot.sf(de, col = NA, border = &quot;red&quot;, add = TRUE): ignoring all #&gt; but the first attribute plot(st_geometry(no2.sf), col = &#39;green&#39;, add = TRUE, pch = 16) We can estimate have a single model for all correlations predict, or simulate, missing values; better approach than taking averages while ignoring if 0-25% are missing alternative multivariate arima would have been enough/better? Alternative: pkg gapfill (remote sensing oriented; Florian Gerber) https://cran.r-project.org/web/packages/gapfill/ predict a sequence of hours smooth in between, e.g. by 10-min interpolations, but note that measurements are hourly integrals, not values at time instances, which is what we assume References "],
["area-data-and-spatial-autcorrelation.html", "Chapter 17 Area Data and Spatial Autcorrelation 17.1 Spatial autocorrelation 17.2 Spatial weights matrices 17.3 Measures of spatial autocorrelation 17.4 Spatial heterogeneity", " Chapter 17 Area Data and Spatial Autcorrelation Areal units of observation are very often used when simultaneous observations are aggregated within non-overlapping boundaries. The boundaries may be those of administrative entities, and may be related to underlying spatial processes, such as commuting flows, but are usually arbitrary. If they do not match the underlying and unobserved spatial processes in one or more variables of interest, proximate areal units will contain parts of the underlying processes, engendering spatial autocorrelation. This is at least in part because the aggregated observations are driven by factors which may or may not themselves have been observed. It is possible to represent the support of areal data by a point, despite the fact that the data have polygonal support. The centroid of the polygon may be taken as a representative point, or the centroid of the largest polygon in a multi-polygon object. When data with intrinsic point support are treated as areal data, the change of support goes the other way, from the known point to a non-overlapping tesselation such as a Voronoi diagram or Dirichlet tessellation or Theissen polygons often through a Delaunay triangulation when using a Euclidean plane. Here, different metrics may also be chosen, or distances measured on a network rather than on the plane. There is also a literature using weighted Voronoi diagrams in local spatial analysis (see for example Boots and Okabe 2007; Okabe et al. 2008; She et al. 2015). When the intrinsic support of the data is as points, but the underlying process is between proximate observations rather than driven chiefly by distance however measured between observations, the data may be aggregate counts or totals (polling stations, retail turnover) or represent a directly observed characteristic of the observation (opening hours of the polling station). Obviously, the risk of mis-representing the footprint of the underlying spatial processes remains in all of these cases, not least because the observations are taken as encompassing the entirety of the underlying process in the case of tesselation of the whole area of interest. This is distinct from the geostatistical setting in which observations are rather samples taken using some scheme within the area of interest. It is also partly distinct from the practice of taking areal sample plots within the area of interest but covering only a small proportion of the area, typically used in ecological and environmental research. This chapter then considers a subset of the methods potentially available for exploring spatial autocorrelation in areal data, or data being handled as areal, where the spatial processes are considered as working through proximity understood in the first instance as contiguity, as a graph linking observations taken as neighbours. This graph is typically undirected and unweighted, but may be directed and/or weighted in certain settings, which then lead to further issues with regard to symmetry. In principle, proximity would be expected to operate symmetrically in space, that is that the influence of \\(i\\) on \\(j\\) and of \\(j\\) on \\(i\\) based on their relative positions should be equivalent. Edge effects are not considered in standard treatments. 17.1 Spatial autocorrelation When analysing areal data, it has long been recognised that, if present, spatial autocorrelation changes how we may infer, relative to the default position of independent observations. In the presence of spatial autocorrelation, we can predict the values of observation \\(i\\) from the values observed at \\(j \\in N_i\\), the set of its proximate neighbours. Early results (Moran 1948; Geary 1954), entered into research practice gradually, for example the social sciences (Duncan, Cuzzort, and Duncan 1961). These results were then collated and extended to yield a set of basic tools of analysis (Cliff and Ord 1973; Cliff and Ord 1981). Cliff and Ord (1973) generalised and extended the expression of the spatial weights matrix representation as part of the framework for establishing the distribution theory for join count, Moran’s \\(I\\) and Geary’s \\(C\\) statistics. This development of what have become known as global measures, returning a single value of autocorrelation for the total study area, has been supplemented by local measures returning values for each areal unit (getis+ord:92; Anselin 1995). 17.2 Spatial weights matrices Handling spatial autocorrelation using relationships to neighbours on a graph takes the graph as given, chosen by the analyst. This differs from the geostatistical approach in which the analyst chooses the binning of the empirical variogram and function used, and then the way the fitted variogram is fitted. Both involve a priori choices, but represent the underlying correlation in different ways (Wall 2004). In Bavaud (1998) and work citing his contribution, attempts have been made to place graph-based neighbours in a broader context. One issue arising in the creation of objects representing neighbourhood relationships is that of no-neighbour areal units (Bivand and Portnov 2004). Islands or units separated by rivers may not be recognised as neighbours when the units have areal support and when using topological relationships such as shared boundaries. In some settings, for example mrf (Markov Random Field) terms in mgcv::gam() and similar model fitting functions that require undirected connected graphs, a requirement also violated when there are disconnected subgraphs. No-neighbour observations can also occur when a distance threshold is used between points, where the threshold is smaller than the maximum nearest neighbour distance. Shared boundary contiguities are not affected by using geographical, unprojected coordinates, but all point-based approaches use distance in one way or another, and need to calculate distances in an appropriate way. The spdep package provides an nb class for neighbours, a list of length equal to the number of observations, with integer vector components. No-neighbours are encoded as an integer vector with a single element 0L, and observations with neighbours as sorted integer vectors containing values in 1L:n pointing to the neighbouring observations. This is a typical row-oriented sparse representation of neighbours. spdep provides many ways of constructing nb objects, and the representation and construction functions are widely used in other packages. spdep builds on the nb representation (undirected or directed graphs) with the listw object, a list with three components, an nb object, a matching list of numerical weights, and a single element character vector containing the single letter name of the way in which the weights were calculated. The most frequently used approach in the social sciences is calculating weights by row standardization, so that all the non-zero weights for one observation will be the inverse of the cardinality of its set of neighbours (1/card(nb[[i]]). We will be using election data from the 2015 Polish Presidential election in this chapter, with 2495 municipalities and Warsaw boroughs, and complete count data from polling stations aggregated to these areal units. The data are an sf sf object: library(sf) data(pol_pres15, package=&quot;spDataLarge&quot;) head(pol_pres15[, c(1, 4, 6)]) #&gt; Simple feature collection with 6 features and 3 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: 235000 ymin: 367000 xmax: 281000 ymax: 413000 #&gt; epsg (SRID): NA #&gt; proj4string: +proj=tmerc +lat_0=0 +lon_0=18.99999999999998 +k=0.9993 +x_0=500000 +y_0=-5300000 +ellps=GRS80 +towgs84=0,0,0 +units=m +no_defs #&gt; TERYT name types geometry #&gt; 1 020101 BOLESŁAWIEC Urban MULTIPOLYGON (((261089 3855... #&gt; 2 020102 BOLESŁAWIEC Rural MULTIPOLYGON (((254150 3837... #&gt; 3 020103 GROMADKA Rural MULTIPOLYGON (((275346 3846... #&gt; 4 020104 NOWOGRODZIEC Urban/rural MULTIPOLYGON (((251770 3770... #&gt; 5 020105 OSIECZNICA Rural MULTIPOLYGON (((263424 4060... #&gt; 6 020106 WARTA BOLESŁAWIECKA Rural MULTIPOLYGON (((267031 3870... library(tmap) tm_shape(pol_pres15) + tm_fill(&quot;types&quot;) Figure 17.1: Polish municipality types 2015 Between early 2002 and April 2019, spdep contained functions for constructing and handling neighbour and spatial weights objects, tests for spatial autocorrelation, and model fitting functions. The latter have been split out into spatialreg, and will be discussed in the next chapter. spdep now accommodates objects represented using sf classes and sp classes directly, going beyond the explorations made in this vignette. library(spdep) #&gt; Loading required package: sp #&gt; Loading required package: spData #&gt; #&gt; Attaching package: &#39;spData&#39; #&gt; The following objects are masked _by_ &#39;.GlobalEnv&#39;: #&gt; #&gt; x, y 17.2.1 Contiguous neighbours The poly2nb() function in spdep takes the boundary points making up the polygon boundaries in the object passed as the pl= argument, and for each observation checks whether at least one (queen=TRUE, default), or at least two (rook, queen=FALSE) points are within snap= distance units of each other. The distances are planar in the raw coordinate units, ignoring geographical projections. Once the required number of sufficiently close points is found, the search is stopped. args(poly2nb) #&gt; function (pl, row.names = NULL, snap = sqrt(.Machine$double.eps), #&gt; queen = TRUE, useC = TRUE, foundInBox = NULL) #&gt; NULL The geometry column should be either of class &quot;sfc_MULTIPOLYGON&quot; or &quot;sfc_POLYGON&quot;, not &quot;sfc_GEOMETRY&quot;; if need be cast to &quot;MULTIPOLYGON&quot; if there are mixed &quot;POLYGON&quot; and &quot;MULTIPOLYGON&quot; objects. class(st_geometry(pol_pres15)) #&gt; [1] &quot;sfc_MULTIPOLYGON&quot; &quot;sfc&quot; table(sapply(st_geometry(pol_pres15), function(x) class(x)[2])) #&gt; #&gt; MULTIPOLYGON #&gt; 2495 system.time(nb_q &lt;- poly2nb(pol_pres15, queen=TRUE)) #&gt; user system elapsed #&gt; 1.11 0.00 1.13 nb_q #&gt; Neighbour list object: #&gt; Number of regions: 2495 #&gt; Number of nonzero links: 14242 #&gt; Percentage nonzero weights: 0.229 #&gt; Average number of links: 5.71 The rgeos gUnarySTRtreeQuery() function also reduces polygons to their boundary points before searching for overlapping “envelopes”, the bounding boxes of the derived multi-point objects. Here, pre-finding candidate contiguous neighbours only shaves off a little run time, but may be helpful with larger objects. system.time({ fB &lt;- rgeos::gUnarySTRtreeQuery(as(pol_pres15, &quot;Spatial&quot;)) nb_q1 &lt;- poly2nb(pol_pres15, queen=TRUE, foundInBox=fB) }) #&gt; user system elapsed #&gt; 0.920 0.000 0.929 all.equal(nb_q, nb_q1, check.attributes=FALSE) #&gt; [1] TRUE We might consider using the contiguity of the polygon boundaries, the fifth element of the DE-9IM vector, setting the first element to FALSE, to find Queen neighbours using GEOS functions in sf, but it takes longer than simply treating the boundaries as points: st_queen &lt;- function(a, b = a) st_relate(a, b, pattern = &quot;F***T****&quot;) as.nb.sgbp &lt;- function(x, ...) { attrs &lt;- attributes(x) x &lt;- lapply(x, function(i) { if(length(i) == 0L) 0L else i } ) attributes(x) &lt;- attrs class(x) &lt;- &quot;nb&quot; x } system.time(nb_sf_q &lt;- as.nb.sgbp(st_queen(pol_pres15))) #&gt; user system elapsed #&gt; 3.9 0.0 3.9 all.equal(nb_q, nb_sf_q, check.attributes=FALSE) #&gt; [1] TRUE The same effect as using rgeos::gUnarySTRtreeQuery can be obtained without the overhead of converting to the equivalent sp class and using rgeos to make the equivalent tree search using sf, by finding intersecting bounding boxes, removing self-intersections and duplicate intersections (contiguities are by definition symmetric, if i is a neighbour of j, then j is a neighbour of i). system.time({ fB1 &lt;- st_intersects(st_as_sfc(lapply(st_geometry(pol_pres15), function(x) { bb &lt;- st_bbox(x) mat &lt;- cbind(c(bb[1], bb[1], bb[3], bb[3], bb[1]), c(bb[2], bb[4], bb[4], bb[2], bb[2])) st_polygon(list(mat)) }))) fB1a &lt;- lapply(seq_along(fB1), function(i) fB1[[i]][fB1[[i]] &gt; i]) fB1a &lt;- fB1a[-length(fB1a)] nb_sf_q1 &lt;- poly2nb(pol_pres15, queen=TRUE, foundInBox=fB1a) }) #&gt; user system elapsed #&gt; 0.637 0.000 0.637 all.equal(nb_q, nb_sf_q1, check.attributes=FALSE) #&gt; [1] TRUE Much of the work involved in finding contiguous neighbours is spent on finding candidate neighbours with intersecting bounding boxes. Note that nb objects record both symmetric neighbour relationships, because these objects admit asymmetric relationships as well, but these duplications are not needed for object construction. Most of the spdep functions for constructing neighbour objects take a row.names= argument, the value of which is stored as a region.id attribute. If not given, the values are taken from row.names() of the first argument. These can be used to check that the neighbours object is in the same order as data. If nb objects are subsetted, the indices change to continue to be within 1:length(subsetted_nb), but the region.id attribute values point back to the object from which it was constructed. We can also check that this undirected graph is connected using the n.comp.nb() function: n.comp.nb(nb_q)$nc #&gt; [1] 1 Neighbour objects may be exported and imported in GAL format for exchange with other software, using write.nb.gal() and read.gal(): tf &lt;- tempfile(fileext=&quot;.gal&quot;) write.nb.gal(nb_q, tf) Using reticulate, it is possible to interoperate with the PySAL family of Python packages, first libpysal providing the basic weights handling infrastructure. As we can see, the percentage of non-zero neighbours is the same in both software systems. library(reticulate) use_python(python=&#39;/usr/bin/python3&#39;) np &lt;- import(&quot;numpy&quot;) libpysal &lt;- import(&quot;libpysal&quot;) nb_gal_ps &lt;- libpysal$io$open(tf)$read() nb_gal_ps$pct_nonzero #&gt; [1] 0.229 17.2.2 Graph-based neighbours If areal units are an appropriate representation, but only points have been observed, contiguity relationships may be approximated using graph-based neighbours. In this case, the imputed boundaries tesselate the plane such that points closer to one observation than any other fall within its polygon. The simplest form is by using triangulation, here using the deldir() function in the deldir package. Because the function returns from and to identifiers, it is easy to construct a long representation of a listw object, as used in the S-Plus SpatialStats module and the sn2listw() function internally to construct an nb object (ragged wide representation). Alternatives often fail to return sufficient information to permit the neighbours to be identified. args(tri2nb) #&gt; function (coords, row.names = NULL) #&gt; NULL The soi.graph() function takes triangulated neighbours and prunes off neighbour relationships represented by unusually long edges, especially around the convex hull, but may render the output object asymmetric. Other graph-based approaches include relativeneigh() and gabrielneigh(). args(soi.graph) #&gt; function (tri.nb, coords, quadsegs = 10) #&gt; NULL The output of these functions is then converted to the nb representation using graph2nb(), with the possible use of the sym= argument to coerce to symmetry. args(graph2nb) #&gt; function (gob, row.names = NULL, sym = FALSE) #&gt; NULL We take the centroids of the largest component polygon for each observation as the point representation; population-weighted centroids might have been a better choice if they were available: coords &lt;- st_centroid(st_geometry(pol_pres15), of_largest_polygon=TRUE) suppressMessages(nb_tri &lt;- tri2nb(coords)) nb_tri #&gt; Neighbour list object: #&gt; Number of regions: 2495 #&gt; Number of nonzero links: 14930 #&gt; Percentage nonzero weights: 0.24 #&gt; Average number of links: 5.98 The average number of neighbours is similar to the Queen boundary contiguity case, but if we look at the distribution of edge lengths using nbdists(), we can see that although the upper quartile is about 15 km, the maximum is almost 300 km, an edge along much of one side of the convex hull. The short minimum distance is also of interest, as many centroids of urban municipalities are very close to the centroids of their surrounding rural counterparts. summary(unlist(nbdists(nb_tri, coords))) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 247 9847 12151 13485 14994 296974 Using the card() function to return a vector of neighbour counts by observation, we see that there are relatively many such surrounded urban municipalities with only one neighbour. table(pol_pres15$types, card(nb_q)) #&gt; #&gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 #&gt; Rural 0 5 41 183 413 465 271 105 55 16 8 0 1 #&gt; Urban 70 58 55 34 22 25 15 13 6 0 3 1 1 #&gt; Urban/rural 2 2 15 36 109 173 162 80 23 8 1 0 0 #&gt; Warsaw Borough 0 0 0 2 3 6 4 3 0 0 0 0 0 For obvious reasons, triangulated units seldom have few neighbours, not only because they are located on the convex hull: table(pol_pres15$types, card(nb_tri)) #&gt; #&gt; 3 4 5 6 7 8 9 10 11 #&gt; Rural 2 57 455 632 328 75 12 2 0 #&gt; Urban 1 41 118 93 33 11 5 1 0 #&gt; Urban/rural 0 22 116 251 158 52 10 0 2 #&gt; Warsaw Borough 0 1 5 6 5 1 0 0 0 Triangulated neighbours also yield a connected graph: n.comp.nb(nb_tri)$nc #&gt; [1] 1 The sphere of influence graph trims a neighbour object such as nb_tri to remove edges that seem long in relation to typical neighbours (Avis and Horton 1985). nb_soi &lt;- graph2nb(soi.graph(nb_tri, coords)) nb_soi #&gt; Neighbour list object: #&gt; Number of regions: 2495 #&gt; Number of nonzero links: 12792 #&gt; Percentage nonzero weights: 0.205 #&gt; Average number of links: 5.13 Unpicking the triangulated neighbours does however remove the connected character of the underlying graph: n_comp &lt;- n.comp.nb(nb_soi) n_comp$nc #&gt; [1] 16 The SoI algorithm has stripped out longer edges leading to urban and rural municpalities where their centroids are very close to each other, giving 15 pairs of neighours unconnected to the main graph: table(n_comp$comp.id) #&gt; #&gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #&gt; 2465 2 2 2 2 2 2 2 2 2 2 2 2 2 2 #&gt; 16 #&gt; 2 The largest length edges along the convex hull have been removed, but “holes” have appeared where the unconnected pairs of neighbours have appeared. The differences between nb_tri and nb_soi are shown in orange in the figure. summary(unlist(nbdists(nb_soi, coords))) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 247 9507 11651 11880 14142 30005 opar &lt;- par(mar=c(0,0,0,0)+0.5) plot(st_geometry(pol_pres15), border=&quot;grey&quot;, lwd=0.5) plot(nb_soi, coords=st_coordinates(coords), add=TRUE, points=FALSE, lwd=0.5) plot(diffnb(nb_tri, nb_soi), coords=st_coordinates(coords), col=&quot;orange&quot;, add=TRUE, points=FALSE, lwd=0.5) par(opar) Figure 17.2: Triangulated and sphere of influence neighbours 17.2.3 Distance-based neighbours Distance-based neighbours can be constructed using dnearneigh(), with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument. If unprojected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid. args(dnearneigh) #&gt; function (x, d1, d2, row.names = NULL, longlat = NULL, bounds = c(&quot;GT&quot;, #&gt; &quot;LE&quot;)) #&gt; NULL The knearneigh() function for \\(k\\)-nearest neighbours returns a knn object, converted to an nb object using knn2nb(). It can also use great circle distances, not least because nearest neighbours may differ when uprojected coordinates are treated as planar. k= should be a small number. For projected coordinates, the RANN package is used to compute nearest neighbours more efficiently. Note that nb objects constructed in this way are most unlikely to be symmetric, hence knn2nb() has a sym= argument to permit the imposition of symmetry, which will mean that all units have at least k= neighbours, not that all units will have exactly k= neighbours. args(knearneigh) #&gt; function (x, k = 1, longlat = NULL, RANN = TRUE) #&gt; NULL args(knn2nb) #&gt; function (knn, row.names = NULL, sym = FALSE) #&gt; NULL The nbdists() function returns the length of neighbour relationship edges in the units of the coordinates if the coordinates are projected, in km otherwise. args(nbdists) #&gt; function (nb, coords, longlat = NULL) #&gt; NULL In order to set the upper limit for distance bands, one may first find the maximum first nearest neighbour distance, using unlist() to remove the list structure of the returned object. k1 &lt;- knn2nb(knearneigh(coords)) k1dists &lt;- unlist(nbdists(k1, coords)) summary(k1dists) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 247 6663 8538 8275 10124 17979 Here the largest first nearest neighbour distance is just under 18 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour: nb_d18 &lt;- dnearneigh(coords, 0, 18000) nb_d18 #&gt; Neighbour list object: #&gt; Number of regions: 2495 #&gt; Number of nonzero links: 20358 #&gt; Percentage nonzero weights: 0.327 #&gt; Average number of links: 8.16 However, even though there are no no-neighbour observations (their presence is reported by the print method for nb objects), the graph is not connected, as a pair of observations are each others’ only neighbours. n_comp &lt;- n.comp.nb(nb_d18) n_comp$nc #&gt; [1] 2 table(n_comp$comp.id) #&gt; #&gt; 1 2 #&gt; 2493 2 Adding 300 m to the threshold gives us a neighbour object with no no-neighbour units, and all units can be reached from all others across the graph. nb_d183 &lt;- dnearneigh(coords, 0, 18300) nb_d183 #&gt; Neighbour list object: #&gt; Number of regions: 2495 #&gt; Number of nonzero links: 21086 #&gt; Percentage nonzero weights: 0.339 #&gt; Average number of links: 8.45 n_comp &lt;- n.comp.nb(nb_d183) n_comp$nc #&gt; [1] 1 One characteristic of distance-based neighbours is that more densely settled areas, with units which are smaller in terms of area (Warsaw boroughs are much smaller on average, but have almost 30 neighbours). Having many neighbours smoothes the neighbour relationship across more neighbours: table(pol_pres15$types, card(nb_d183)) #&gt; #&gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #&gt; Rural 5 18 51 87 117 156 193 227 222 166 114 62 43 29 #&gt; Urban 3 6 9 12 28 27 29 21 35 30 20 12 17 11 #&gt; Urban/rural 10 30 28 47 55 62 94 89 60 45 35 10 15 7 #&gt; Warsaw Borough 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; #&gt; 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #&gt; Rural 21 12 13 9 7 7 0 1 0 1 2 0 0 0 #&gt; Urban 5 3 8 5 12 5 1 0 0 2 0 1 1 0 #&gt; Urban/rural 6 3 7 3 1 1 1 0 2 0 0 0 0 0 #&gt; Warsaw Borough 0 0 0 0 0 0 0 0 0 1 2 2 5 3 #&gt; #&gt; 29 30 31 #&gt; Rural 0 0 0 #&gt; Urban 0 0 0 #&gt; Urban/rural 0 0 0 #&gt; Warsaw Borough 1 2 2 arha &lt;- units::set_units(st_area(pol_pres15), hectare) aggregate(arha, list(pol_pres15$types), mean) #&gt; Group.1 x #&gt; 1 Rural 12500 [] #&gt; 2 Urban 4497 [] #&gt; 3 Urban/rural 16850 [] #&gt; 4 Warsaw Borough 2886 [] For use later, we also construct a neighbour object with no-neighbour units, using a threshold of 16 km: nb_d16 &lt;- dnearneigh(coords, 0, 16000) nb_d16 #&gt; Neighbour list object: #&gt; Number of regions: 2495 #&gt; Number of nonzero links: 15850 #&gt; Percentage nonzero weights: 0.255 #&gt; Average number of links: 6.35 #&gt; 7 regions with no links: #&gt; 569 1371 1522 2374 2385 2473 2474 It is possible to control the numbers of neighbours directly using \\(k\\)-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry: knn_k6 &lt;- knearneigh(coords, k=6) knn2nb(knn_k6) #&gt; Neighbour list object: #&gt; Number of regions: 2495 #&gt; Number of nonzero links: 14970 #&gt; Percentage nonzero weights: 0.24 #&gt; Average number of links: 6 #&gt; Non-symmetric neighbours list nb_k6s &lt;- knn2nb(knn_k6, sym=TRUE) nb_k6s #&gt; Neighbour list object: #&gt; Number of regions: 2495 #&gt; Number of nonzero links: 16810 #&gt; Percentage nonzero weights: 0.27 #&gt; Average number of links: 6.74 Here the size of k= is sufficient to ensure connectedness, although the graph is not planar as edges cross at locations other than nodes, which is not the case for contiguous or graph-based neighbours. n_comp &lt;- n.comp.nb(nb_k6s) n_comp$nc #&gt; [1] 1 17.2.4 Weights specification Once neighbour objects are available, further choices need to made in specifying the weights objects. The nb2listw() function is used to create a listw weights object with an nb object, a matching list of weights vectors, and a style specification. Because handling no-neighbour observations now begins to matter, the zero.policy= argument is introduced. By default, this is FALSE, indicating that no-neighbour observations will cause an error, as the spatially lagged value for an observation with no neighbours is not available. By convention, zero is substituted for the lagged value, as the cross product of a vector of zero-valued weights and a data vector, hence the name of zero.policy. args(nb2listw) #&gt; function (neighbours, glist = NULL, style = &quot;W&quot;, zero.policy = NULL) #&gt; NULL We will be using the helper function spweights.constants() below to show some consequences of varing style choices. It returns constants for a listw object, \\(n\\) is the number of observations, n1 to n3 are \\(n-1, \\ldots\\), nn is \\(n^2\\) and \\(S_0\\), \\(S_1\\) and \\(S_2\\) are constants, \\(S_0\\) being the sum of the weights. There is a full discussion of the constants in Bivand and Wong (2018). args(spweights.constants) #&gt; function (listw, zero.policy = NULL, adjust.n = TRUE) #&gt; NULL The &quot;B&quot; binary style gives a weight of unity to each neighbour relationship, and typically upweights units with no boundaries on the edge of the study area. lw_q_B &lt;- nb2listw(nb_q, style=&quot;B&quot;) unlist(spweights.constants(lw_q_B)) #&gt; n n1 n2 n3 nn S0 S1 S2 #&gt; 2495 2494 2493 2492 6225025 14242 28484 357280 The &quot;W&quot; row-standardized style upweights units around the edge of the study area that necessarily have fewer neighbours. This style first gives a weight of unity to each neighbour relationship, then divides these weights by the per unit sums of weights. Naturally this leads to division by zero where there are no neighbours, a not-a-number result, unless the chosen policy is to permit no-neighbour observations. We can see that \\(S_0\\) is now equal to \\(n\\). lw_q_W &lt;- nb2listw(nb_q, style=&quot;W&quot;) unlist(spweights.constants(lw_q_W))[c(1,6:8)] #&gt; n S0 S1 S2 #&gt; 2495 2495 958 10406 An &quot;S&quot; style attempts to balance the tendencies of the &quot;B&quot; and &quot;W&quot; styles to downweight or upweight units on the edge of the study area (Tiefelsdorf, Griffith, and Boots 1999). Other styles are variants of &quot;B&quot;, &quot;C&quot; sets the weights such that they sum (\\(S_0\\)) to \\(n\\), and &quot;U&quot; to unity. The main change in the &quot;S&quot; style is to \\(S_1\\) for this configuration of contiguities. lw_q_S &lt;- nb2listw(nb_q, style=&quot;S&quot;) unlist(spweights.constants(lw_q_S))[c(1,6:8)] #&gt; n S0 S1 S2 #&gt; 2495 2495 886 10646 Inverse distance weights are used in a number of scientific fields. Some use dense inverse distance matrices, but many of the inverse distances are close to zero, so have little practical contribution, especially as the spatial process matrix is itself dense. Inverse distance weights may be constructed by taking the lengths of edges, changing units to avoid most weights being too large or small (here from m to km), taking the inverse, and passing through the glist= argument to nb2listw(): gwts &lt;- lapply(nbdists(nb_d183, coords), function(x) 1/(x/1000)) lw_d183_idw_B &lt;- nb2listw(nb_d183, glist=gwts, style=&quot;B&quot;) unlist(spweights.constants(lw_d183_idw_B))[c(1,6:8)] #&gt; n S0 S1 S2 #&gt; 2495 1841 534 7265 No-neighbour handling is by default to prevent the construction of a weights object, making the analyst take a position on how to proceed. try(lw_d16_B &lt;- nb2listw(nb_d16, style=&quot;B&quot;)) #&gt; Error in nb2listw(nb_d16, style = &quot;B&quot;) : Empty neighbour sets found Use can be made of the zero.policy= argument to many functions used with nb and listw objects. lw_d16_B &lt;- nb2listw(nb_d16, style=&quot;B&quot;, zero.policy=TRUE) unlist(spweights.constants(lw_d16_B, zero.policy=TRUE))[c(1,6:8)] #&gt; n S0 S1 S2 #&gt; 2488 15850 31700 506480 It is also possible to store the chosen zero policy for the running session using set.ZeroPolicyOption(); the function returns the value it held before being set. If the zero.policy= argument takes its default value of NULL, the stored option value is used, initialized to FALSE at the beginning of the session. zO &lt;- set.ZeroPolicyOption(TRUE) lw_d16_B &lt;- nb2listw(nb_d16, style=&quot;B&quot;) unlist(spweights.constants(lw_d16_B))[c(1,6:8)] #&gt; n S0 S1 S2 #&gt; 2488 15850 31700 506480 invisible(set.ZeroPolicyOption(zO)) Note that by default the adjust.n= argument is set by default to TRUE, subtracting the count of no-neighbour observations from the observation count, so \\(n\\) is smaller with possible consequences for inference. The complete count can be retrieved by changing the argument. unlist(spweights.constants(lw_d16_B, zero.policy=TRUE, adjust.n=FALSE))[c(1,6:8)] #&gt; n S0 S1 S2 #&gt; 2495 15850 31700 506480 17.3 Measures of spatial autocorrelation Measures of spatial autcorrelation unfortunately pick up other mis-specifications in the way that we model data (Waller and Gotway 2004; McMillen 2003). First we test a random variable using the Moran test, here under the normality assumption (argument randomisation=FALSE, default TRUE): set.seed(1) x &lt;- rnorm(nrow(pol_pres15)) moran.test(x, lw_q_B, randomisation=FALSE)$estimate #&gt; Moran I statistic Expectation Variance #&gt; -0.004772 -0.000401 0.000140 It however fails to detect a missing trend in the data as a missing variable problem, finding spatial autocorrelation instead: beta &lt;- 0.5e-02 t &lt;- st_coordinates(coords)[,1]/1000 x_t &lt;- x + beta*t moran.test(x_t, lw_q_B, randomisation=FALSE)$estimate #&gt; Moran I statistic Expectation Variance #&gt; 0.355771 -0.000401 0.000140 If we test the residuals of a linear model including the trend, the apparent spatial autocorrelation disappears: lm.morantest(lm(x_t ~ t), lw_q_B)$estimate #&gt; Observed Moran I Expectation Variance #&gt; -0.004777 -0.000789 0.000140 A comparison of implementations of measures of spatial autocorrelation shows that a wide range of measures is available in R in a number of packages, chiefly in the spdep package, and that differences from other implementations can be attributed to design decisions (Bivand and Wong 2018). The spdep package also includes the only implementations of exact and Saddlepoint approximations to global and local Moran’s I for regression residuals (Tiefelsdorf 2002; Bivand, Müller, and Reder 2009). 17.3.1 Global measures We will begin by examining join count statistics, where joincount.test() takes a factor vector of values fx= and a listw object, and returns a list of htest (hypothesis test) objects defined in the stats package, one htest object for each level of the fx= argument. The observed counts are of neighbours with the same factor levels, known as same-colour joins. args(joincount.test) #&gt; function (fx, listw, zero.policy = NULL, alternative = &quot;greater&quot;, #&gt; sampling = &quot;nonfree&quot;, spChk = NULL, adjust.n = TRUE) #&gt; NULL The function takes an alternative= argument for hypothesis testing, a sampling= argument showing the basis for the construction of the variance of the measure, where the default &quot;nonfree&quot; choice corresponds to analytical permutation; the spChk= argument is retained for backward compatibility. For reference, the counts of factor levels for the type of municipality or Warsaw borough are: table(pol_pres15$types) #&gt; #&gt; Rural Urban Urban/rural Warsaw Borough #&gt; 1563 303 611 18 Since there are four levels, we re-arrange the list of htest objects to give a matrix of estimated results. The observed same-colour join counts are tabulated with their expectations based on the counts of levels of the input factor, so that few joins would be expected between for example Warsaw Boroughs, because there are very few of them. The variance calculation uses the underlying constants of the chosen listw object and the counts of levels of the input factor. The z-value is obtained in the usual way by dividing the difference between the observed and expected join counts by the square root of the variance. The global tests in spdep return htest objects with a print() method in the stats package. To save space here, we’ll use a glance function if the broom::tidy() method has not been updated to use estimate names. glance_htest &lt;- function(ht) c(ht$estimate, &quot;Std deviate&quot;=unname(ht$statistic)) jcl &lt;- joincount.test(pol_pres15$types, listw=lw_q_B) broom_ok &lt;- FALSE if (names(broom::tidy(jcl[[1]]))[1] == &quot;Same colour statistic&quot;) broom_ok &lt;- TRUE if (broom_ok) { nm &lt;- tibble::enframe(sapply(jcl, function(t) { nm &lt;- substring(names(t$statistic), 18) paste0(nm, &quot;:&quot;, nm) })) mat &lt;- cbind(nm, do.call(&quot;rbind&quot;, (lapply(jcl, broom::tidy)))) names(mat)[3] &lt;- &quot;Joincount&quot; names(mat)[6] &lt;- &quot;Std.deviate&quot; names(mat)[2] &lt;- &quot;&quot; mat[,2:6] } else { mat &lt;- t(sapply(jcl, glance_htest)) colnames(mat)[1] &lt;- &quot;Joincount&quot; rownames(mat) &lt;- sapply(jcl, function(t) { nm &lt;- substring(names(t$statistic), 18) paste0(nm, &quot;:&quot;, nm) }) mat } #&gt; Joincount Expectation Variance Std deviate #&gt; Rural:Rural 3087 2793.92 1126.534 8.732 #&gt; Urban:Urban 110 104.72 93.299 0.547 #&gt; Urban/rural:Urban/rural 656 426.53 331.759 12.599 #&gt; Warsaw Borough:Warsaw Borough 41 0.35 0.347 68.965 The join count test was subsequently adapted for multi-colour join counts (Upton and Fingleton 1985). The implementation as joincount.mult() in spdep returns a table based on nonfree sampling, and does not report p-values. args(joincount.multi) #&gt; function (fx, listw, zero.policy = FALSE, spChk = NULL, adjust.n = TRUE) #&gt; NULL The first four lines of results for same-colour join counts are the same as those from jointcount.test() above, also under nonfree sampling. The remaining lines report results for multi-colour join counts on the same basis, and finally the “Jtot” statistic summarising the totality of join counts for the variable and listw object chosen. joincount.multi(pol_pres15$types, listw=lw_q_B) #&gt; Joincount Expected Variance z-value #&gt; Rural:Rural 3087.000 2793.920 1126.534 8.73 #&gt; Urban:Urban 110.000 104.719 93.299 0.55 #&gt; Urban/rural:Urban/rural 656.000 426.526 331.759 12.60 #&gt; Warsaw Borough:Warsaw Borough 41.000 0.350 0.347 68.96 #&gt; Urban:Rural 668.000 1083.941 708.209 -15.63 #&gt; Urban/rural:Rural 2359.000 2185.769 1267.131 4.87 #&gt; Urban/rural:Urban 171.000 423.729 352.190 -13.47 #&gt; Warsaw Borough:Rural 12.000 64.393 46.460 -7.69 #&gt; Warsaw Borough:Urban 9.000 12.483 11.758 -1.02 #&gt; Warsaw Borough:Urban/rural 8.000 25.172 22.354 -3.63 #&gt; Jtot 3227.000 3795.486 1496.398 -14.70 So far, we have used binary weights, so the sum of join counts multiplied by the weight on that join remains integer. If we change to row standardised weights, where the weights are not unity in all cases, the counts, expectations and variances change, but there are few major changes in the z-values. joincount.multi(pol_pres15$types, listw=lw_q_W) #&gt; Joincount Expected Variance z-value #&gt; Rural:Rural 521.6476 489.4559 22.8856 6.73 #&gt; Urban:Urban 20.9023 18.3452 2.8822 1.51 #&gt; Urban/rural:Urban/rural 106.1765 74.7213 9.3498 10.29 #&gt; Warsaw Borough:Warsaw Borough 6.7363 0.0613 0.0116 61.89 #&gt; Urban:Rural 165.2283 189.8913 18.2987 -5.77 #&gt; Urban/rural:Rural 389.6002 382.9162 36.1501 1.11 #&gt; Urban/rural:Urban 32.5048 74.2314 10.6416 -12.79 #&gt; Warsaw Borough:Rural 1.8554 11.2807 1.1075 -8.96 #&gt; Warsaw Borough:Urban 1.6171 2.1868 0.3775 -0.93 #&gt; Warsaw Borough:Urban/rural 1.2315 4.4098 0.6810 -3.85 #&gt; Jtot 592.0373 664.9162 43.1245 -11.10 Using an inverse distance based listw object does, however, change the z-values markedly, because closer centroids are upweighted relatively strongly: joincount.multi(pol_pres15$types, listw=lw_d183_idw_B) #&gt; Joincount Expected Variance z-value #&gt; Rural:Rural 3.46e+02 3.61e+02 4.93e+01 -2.10 #&gt; Urban:Urban 2.90e+01 1.35e+01 2.23e+00 10.39 #&gt; Urban/rural:Urban/rural 4.65e+01 5.51e+01 9.61e+00 -2.79 #&gt; Warsaw Borough:Warsaw Borough 1.68e+01 4.53e-02 6.61e-03 206.38 #&gt; Urban:Rural 2.02e+02 1.40e+02 2.36e+01 12.73 #&gt; Urban/rural:Rural 2.25e+02 2.83e+02 3.59e+01 -9.59 #&gt; Urban/rural:Urban 3.65e+01 5.48e+01 8.86e+00 -6.14 #&gt; Warsaw Borough:Rural 5.65e+00 8.33e+00 1.73e+00 -2.04 #&gt; Warsaw Borough:Urban 9.18e+00 1.61e+00 2.54e-01 15.01 #&gt; Warsaw Borough:Urban/rural 3.27e+00 3.25e+00 5.52e-01 0.02 #&gt; Jtot 4.82e+02 4.91e+02 4.16e+01 -1.38 The implementation of Moran’s \\(I\\) in spdep in the moran.test() function has similar arguments to those of joincount.test(), but sampling= is replaced by randomisation= to indicate the underlying analytical approach used for calculating the variance of the measure. It is also possible to use ranks rather than numerical values (Cliff and Ord 1981, 46). The drop.EI2= agrument may be used to reproduce results where the final component of the variance term is omitted. args(moran.test) #&gt; function (x, listw, randomisation = TRUE, zero.policy = NULL, #&gt; alternative = &quot;greater&quot;, rank = FALSE, na.action = na.fail, #&gt; spChk = NULL, adjust.n = TRUE, drop.EI2 = FALSE) #&gt; NULL The default for the randomisation= argument is TRUE, but here we will simply show that the test under normality is the same as a test of least squares residuals with only the intercept used in the mean model. The spelling of randomisation is that of Cliff and Ord (1973). mt &lt;- moran.test(pol_pres15$I_turnout, listw=lw_q_B, randomisation=FALSE) if (broom_ok) broom::tidy(mt)[,1:4] else glance_htest(mt) #&gt; Moran I statistic Expectation Variance Std deviate #&gt; 0.691434 -0.000401 0.000140 58.461349 The lm.morantest() function also takes a resfun= argument to set the function used to extract the residuals used for testing, and clearly lets us model other salient features of the response variable (Cliff and Ord 1981, 203). args(lm.morantest) #&gt; function (model, listw, zero.policy = NULL, alternative = &quot;greater&quot;, #&gt; spChk = NULL, resfun = weighted.residuals, naSubset = TRUE) #&gt; NULL To compare with the standard test, we are only using the intercept here, and as can be seen, the results are the same. ols &lt;- lm(I_turnout ~ 1, pol_pres15) lmt &lt;- lm.morantest(ols, listw=lw_q_B) if (broom_ok) broom::tidy(lmt)[,1:4] else glance_htest(lmt) #&gt; Observed Moran I Expectation Variance Std deviate #&gt; 0.691434 -0.000401 0.000140 58.461349 Following the elaboration of the global Moran’s I test for least squares residuals, further work was done to develop an exact test (Tiefelsdorf and Boots 1995; Tiefelsdorf and Boots 1997; Hepple 1998). The lm.morantest.sad() function implements a Saddlepoint approximation, and lm.morantest.exact() the exact test, using dense matrix calculations unsuitable for large \\(n\\) (Tiefelsdorf 2002; Bivand, Müller, and Reder 2009). The additional arguments are used in the computation of the exact standard deviate. args(lm.morantest.exact) #&gt; function (model, listw, zero.policy = NULL, alternative = &quot;greater&quot;, #&gt; spChk = NULL, resfun = weighted.residuals, zero.tol = 1e-07, #&gt; Omega = NULL, save.M = NULL, save.U = NULL, useTP = FALSE, #&gt; truncErr = 1e-06, zeroTreat = 0.1) #&gt; NULL We can easily see that the exact standard deviate is an order of magnitude smaller for this response, mean model and spatial weights specification, compared to the standard deviate based on the variance calculated using the normality assumption. suppressWarnings(lmte &lt;- lm.morantest.exact(ols, listw=lw_q_B)) names(lmte$estimate) &lt;- &quot;Moran I statistic&quot; class(lmte) &lt;- c(class(lmte), &quot;htest&quot;) if (broom_ok) broom::tidy(lmte)[,1:2] else glance_htest(lmte) #&gt; Moran I statistic Std deviate #&gt; 0.691 5.832 The only difference between tests under normality and randomisation is that an extra term is added if the kurtosis of the variable of interest indicates a flatter or more peaked distribution, where the measure used is the classical measure of kurtosis. all.equal(3+e1071::kurtosis(pol_pres15$I_turnout, type=1), moran(pol_pres15$I_turnout, listw=lw_q_B, n=nrow(pol_pres15), S0=Szero(lw_q_B))$K) #&gt; [1] TRUE Under the default randomisation assumption of analytical randomisation, the results are largely unchanged. mtr &lt;- moran.test(pol_pres15$I_turnout, listw=lw_q_B) if (broom_ok) (tmtr &lt;- broom::tidy(mtr)[,1:4]) else (tmtr &lt;- glance_htest(mtr)) #&gt; Moran I statistic Expectation Variance Std deviate #&gt; 0.691434 -0.000401 0.000140 58.459835 The PySAL esda package contains the Moran function reporting the same results, here under randomisation. The function returns results under normality, randomisation and by permutation simulation. Similar compatisons may be made for other global measures; for details see Bivand and Wong (2018). esda &lt;- import(&quot;esda&quot;) np$random$seed(1L) mi &lt;- esda$Moran(pol_pres15$I_turnout, nb_gal_ps, transformation=&quot;B&quot;, permutations=999L, two_tailed=FALSE) all.equal(c(mi$I, mi$EI, mi$VI_rand, mi$z_rand), unname(tmtr)) #&gt; [1] TRUE Of course, from the very beginning, interest was shown in Monte Carlo testing, also known as a Hope-type test and as a permutation bootstrap. By default, moran.mc() retrurns a &quot;htest&quot; object, but may simply use boot::boot() internally and return a &quot;boot&quot; object when return_boot=TRUE. args(moran.mc) #&gt; function (x, listw, nsim, zero.policy = NULL, alternative = &quot;greater&quot;, #&gt; na.action = na.fail, spChk = NULL, return_boot = FALSE, adjust.n = TRUE) #&gt; NULL In addition the number of simulations of the variable of interest by permutation, that is shuffling the values across the observations at random, needs to be given as nsim=. set.seed(1) mmc &lt;- moran.mc(pol_pres15$I_turnout, listw=lw_q_B, nsim=999, return_boot = TRUE) The bootstrap permutation retains the outcomes of each of the random permutations, reporting the observed value of the statistic, here Moran’s \\(I\\), the difference between this value and the mean of the simulations under randomisation (equivalent to \\(E(I)\\)), and the standard deviation of the simulations under randomisation. glance_boot &lt;- function(bt) c(original=bt$t0, bias=mean(bt$t)-bt$t0, std.error=sd(bt$t), zvalue=(bt$t0-mean(bt$t))/sd(bt$t)) if (broom_ok) broom::tidy(mmc) else glance_boot(mmc) #&gt; original bias std.error zvalue #&gt; 0.691 -0.691 0.012 57.592 We can extract the equivalent results from object returned by the PySAL Moran() object, with the Numpy random number generator: c(statistic=mi$I, bias=mi$EI_sim-mi$I, std.error=mi$seI_sim) #&gt; statistic bias std.error #&gt; 0.6914 -0.6922 0.0123 If we compare the Monte Carlo and analytical variances of \\(I\\) under randomisation, we typically see few differences, arguably rendering Monte Carlo testing unnecessary. c(&quot;Permutation bootstrap&quot;=var(mmc$t), &quot;Analytical randomisation&quot;=unname(mtr$estimate[3])) #&gt; Permutation bootstrap Analytical randomisation #&gt; 0.000144 0.000140 There is also a permutation bootstrap approach to the approximate profile likelihood estimator (APLE) measure (Li, Calder, and Cressie 2007; Li, Calder, and Cressie 2012) using row-standardised weights, in which the variable of interest must be centred on zero first: apmc &lt;- aple.mc(c(scale(pol_pres15$I_turnout, scale=FALSE)), listw=lw_q_W, nsim=999) #&gt; Registered S3 methods overwritten by &#39;spatialreg&#39;: #&gt; method from #&gt; residuals.stsls spdep #&gt; deviance.stsls spdep #&gt; coef.stsls spdep #&gt; print.stsls spdep #&gt; summary.stsls spdep #&gt; print.summary.stsls spdep #&gt; residuals.gmsar spdep #&gt; deviance.gmsar spdep #&gt; coef.gmsar spdep #&gt; fitted.gmsar spdep #&gt; print.gmsar spdep #&gt; summary.gmsar spdep #&gt; print.summary.gmsar spdep #&gt; print.lagmess spdep #&gt; summary.lagmess spdep #&gt; print.summary.lagmess spdep #&gt; residuals.lagmess spdep #&gt; deviance.lagmess spdep #&gt; coef.lagmess spdep #&gt; fitted.lagmess spdep #&gt; logLik.lagmess spdep #&gt; fitted.SFResult spdep #&gt; print.SFResult spdep #&gt; fitted.ME_res spdep #&gt; print.ME_res spdep #&gt; print.lagImpact spdep #&gt; plot.lagImpact spdep #&gt; summary.lagImpact spdep #&gt; HPDinterval.lagImpact spdep #&gt; print.summary.lagImpact spdep #&gt; print.sarlm spdep #&gt; summary.sarlm spdep #&gt; residuals.sarlm spdep #&gt; deviance.sarlm spdep #&gt; coef.sarlm spdep #&gt; vcov.sarlm spdep #&gt; fitted.sarlm spdep #&gt; logLik.sarlm spdep #&gt; anova.sarlm spdep #&gt; predict.sarlm spdep #&gt; print.summary.sarlm spdep #&gt; print.sarlm.pred spdep #&gt; as.data.frame.sarlm.pred spdep #&gt; residuals.spautolm spdep #&gt; deviance.spautolm spdep #&gt; coef.spautolm spdep #&gt; fitted.spautolm spdep #&gt; print.spautolm spdep #&gt; summary.spautolm spdep #&gt; logLik.spautolm spdep #&gt; print.summary.spautolm spdep #&gt; print.WXImpact spdep #&gt; summary.WXImpact spdep #&gt; print.summary.WXImpact spdep #&gt; predict.SLX spdep if (broom_ok) broom::tidy(apmc) else glance_boot(apmc) #&gt; original bias std.error zvalue #&gt; 0.7852 -0.7847 0.0326 24.0760 Geary’s global \\(C\\) is implemented in geary.test() largely following the same argument structure as moran.test(). args(geary.test) #&gt; function (x, listw, randomisation = TRUE, zero.policy = NULL, #&gt; alternative = &quot;greater&quot;, spChk = NULL, adjust.n = TRUE) #&gt; NULL Because \\(C\\) is based on the similarity of neighbouring values, small values of \\(C\\) indicate small differences, here much smaller than the expected standardised difference of unity. gt &lt;- geary.test(pol_pres15$I_turnout, listw=lw_q_B) if (broom_ok) broom::tidy(gt)[,1:4] else glance_htest(gt) #&gt; Geary C statistic Expectation Variance Std deviate #&gt; 3.04e-01 1.00e+00 2.14e-04 4.76e+01 The Getis-Ord \\(G\\) test includes extra arguments to accommodate differences between implementations, as Bivand and Wong (2018) found multiple divergences from the original definitions, often to omit no-neighbour observations generated when using distance band neighbours. args(globalG.test) #&gt; function (x, listw, zero.policy = NULL, alternative = &quot;greater&quot;, #&gt; spChk = NULL, adjust.n = TRUE, B1correct = TRUE, adjust.x = TRUE, #&gt; Arc_all_x = FALSE) #&gt; NULL With contiguity neighbours, global \\(G\\) yields a much smaller standard deviate than when using a 16 km distance band neighbour definition: ggt &lt;- globalG.test(pol_pres15$I_turnout, listw=lw_q_B) if (broom_ok) broom::tidy(ggt)[,1:4] else glance_htest(ggt) #&gt; Global G statistic Expectation Variance #&gt; 2.31e-03 2.29e-03 1.73e-11 #&gt; Std deviate #&gt; 5.08e+00 Because this neighbour definition leaves a few observations without neighbours, we need to use the zero.policy= argument. ggt &lt;- globalG.test(pol_pres15$I_turnout, listw=lw_d16_B, zero.policy=TRUE) if (broom_ok) broom::tidy(ggt)[,1:4] else glance_htest(ggt) #&gt; Global G statistic Expectation Variance #&gt; 2.79e-03 2.56e-03 5.51e-11 #&gt; Std deviate #&gt; 3.05e+01 The Mantel test is implemented without its analytical variance as sp.mantel.mc(), rather using permutation bootstrap like moran.mc(). It takes a type= argument, which can be set to &quot;moran&quot;, &quot;geary&quot; or &quot;sokal&quot;. The output is scaled differently, but gives similar inferences. set.seed(1) mamc &lt;- sp.mantel.mc(pol_pres15$I_turnout, listw=lw_q_B, nsim=999, type=&quot;moran&quot;, return_boot = TRUE) if (broom_ok) broom::tidy(mamc) else glance_boot(mamc) #&gt; original bias std.error zvalue #&gt; 9843.5 -9844.3 170.9 57.6 Finally, the empirical Bayes Moran’s \\(I\\) takes account of the denominator in assessing spatial autocorrelation in rates data (Assunção and Reis 1999). Until now, we have considered the proportion of valid votes cast in relation to the numbers entitled to vote by spatial entity, but using EBImoran.mc() we can try to accommodate uncertainty in extreme rates in entities with small numbers entitled to vote. There is, however, little impact on the outcome in this case. set.seed(1) suppressMessages(ebimc &lt;- EBImoran.mc(n=pol_pres15$I_valid_votes, x=pol_pres15$I_entitled_to_vote, listw=lw_q_B, nsim=999, return_boot=TRUE)) if (broom_ok) broom::tidy(ebimc) else glance_boot(ebimc) #&gt; original bias std.error zvalue #&gt; 0.693 -0.693 0.012 57.742 A similar measure may also be achieved using global empirical Bayes estimators with EBest() to calculate a numerical vector of empirical Bayes estimates, but both are only limited ways of handling mis-specification in the mean model. Global measures of spatial autocorrelation using spatial weights objects based on graphs of neighbours are, as we have seen, rather blunt tools, which for interpretation depend critically on a reasoned mean model of the variable in question. If the mean model is just the intercept, the global measures will respond to all kinds of mis-specification, not only spatial autocorrelation. A key source of mis-specification will typically also include the choice of entities for aggregation of data. 17.3.2 Local measures Building on insights from the weaknesses of global measures, local indicators of spatial association began to appear in the first half of the 1990s (Anselin 1995; Getis and Ord 1992; Getis and Ord 1996). In addition, the Moran plot was introduced, plotting the values of the variable of interest against their spatially lagged values, typically using row-standardised weights to make the axes more directly comparable (Anselin 1996). The moran.plot() function also returns an influence measures object used to label observations exerting more than propotional influence on the slope of the line representing global Moran’s \\(I\\). In this case, we can see that there are many spatial entities exerting such influence. These pairs of observed and lagged observed values make up in aggregate the global measure, but can also be explored in detail. The quadrants of the Moran plot also show low-low pairs in the lower left quadrant, high-high in the upper right quadrant, and fewer low-high and high-low pairs in the upper left and lower right quadrants. infl_W &lt;- moran.plot(pol_pres15$I_turnout, listw=lw_q_W, labels=pol_pres15$TERYT, cex=1, pch=&quot;.&quot;, xlab=&quot;I round turnout&quot;, ylab=&quot;lagged turnout&quot;) (#fig:moran_plot)Moran plot of I round turnout, row standardised weights If we extract the hat value influence measure from the returned object, a map suggests that some edge entities exert more than proportional influence, as do entities in or near larger urban areas. pol_pres15$hat_value &lt;- infl_W$infmat[,6] tm_shape(pol_pres15) + tm_fill(&quot;hat_value&quot;) (#fig:moran_hat)Moran plot hat values, row standardised neighbours Bivand and Wong (2018) discuss issues impacting the use of local indicators, such as local Moran’s \\(I\\) and local Getis-Ord \\(G\\). Some issues affect the calculation of the local indicators, others inference from their values. Because \\(n\\) statistics may be being calculated from the same number of observations, there are multiple comparison problems that need to be addressed. Although the apparent detection of hotspots from values of local indicators has been quite widely adopted, it remains fraught with difficulty because adjustment of the inferential basis to accommodate multiple comparisons is not often chosen, and as in the global case, mis-specification also remains a source of confusion. Further, interpreting local spatial autocorrelation in the presence of global spatial autocorrelation is challenging (Ord and Getis 2001; Tiefelsdorf 2002; Bivand, Müller, and Reder 2009). The mlvar= and adjust.x= arguments to localmoran() are discussed in Bivand and Wong (2018), and permit matching with other implementations. The p.adjust.method= argument uses an untested speculation that adjustment should only take into account the cardinality of the neighbour set of each observation when adjusting for multiple comparisons; using stats::p.adjust() is preferable. args(localmoran) #&gt; function (x, listw, zero.policy = NULL, na.action = na.fail, #&gt; alternative = &quot;greater&quot;, p.adjust.method = &quot;none&quot;, mlvar = TRUE, #&gt; spChk = NULL, adjust.x = FALSE) #&gt; NULL Taking &quot;two.sided&quot; p-values because these local indicators when summed and divided by the sum of the spatial weights, and thus positive and negative local spatial autocorrelation may be present, we obtain: locm &lt;- localmoran(pol_pres15$I_turnout, listw=lw_q_B, alternative=&quot;two.sided&quot;) all.equal(sum(locm[,1])/Szero(lw_q_B), unname(moran.test(pol_pres15$I_turnout, lw_q_B)$estimate[1])) #&gt; [1] TRUE Using stats::p.adjust() to adjust for multiple comparisons, we see that almost 29% of the local measures have p-values \\(\\lt 0.05\\) if no adjustment is applied, but only 12% using Bonferroni correction. pvs &lt;- cbind(locm[,5], p.adjust(locm[,5], &quot;bonferroni&quot;), p.adjust(locm[,5],&quot;fdr&quot;), p.adjust(locm[,5], &quot;BY&quot;)) colnames(pvs) &lt;- c(&quot;none&quot;, &quot;bonferroni&quot;, &quot;fdr&quot;, &quot;BY&quot;) apply(pvs, 2, function(x) sum(x &lt; 0.05)) #&gt; none bonferroni fdr BY #&gt; 715 297 576 424 The localmoran.sad() and localmoran.exact() functions take a fitted linear model, a select= argument to specify which observations to test (choosing only one removes the need to adjust for multiple comparisons), arguments like nb2listw() to build single observation star graphs of neighbours and their weights, and arguments needed for numerical estimation internally. They return lists of objects like &quot;htest&quot; objects, which may be coerced to data.frame objects. args(localmoran.sad) #&gt; function (model, select, nb, glist = NULL, style = &quot;W&quot;, zero.policy = NULL, #&gt; alternative = &quot;greater&quot;, spChk = NULL, resfun = weighted.residuals, #&gt; save.Vi = FALSE, tol = .Machine$double.eps^0.5, maxiter = 1000, #&gt; tol.bounds = 1e-04, save.M = FALSE, Omega = NULL) #&gt; NULL If we use localmoran.sad() to make saddlepoint approximations of the standard deviates of the local measures, and make the same corrections, we drop from 26% of the p-values \\(\\lt 0.05\\) with no correction to under 2% using Bonferroni correction: locms &lt;- localmoran.sad(ols, nb=nb_q, style=&quot;B&quot;, alternative=&quot;two.sided&quot;) locms &lt;- as.data.frame(locms) pvss &lt;- cbind(locms[,5], p.adjust(locms[,5], &quot;bonferroni&quot;), p.adjust(locms[,5], &quot;fdr&quot;), p.adjust(locms[,5], &quot;BY&quot;)) colnames(pvss) &lt;- c(&quot;none&quot;, &quot;bonferroni&quot;, &quot;fdr&quot;, &quot;BY&quot;) apply(pvss, 2, function(x) sum(x &lt; 0.05)) #&gt; none bonferroni fdr BY #&gt; 646 44 314 87 Should we think it appropriate to take possible global spatial autocorrelation into account, we may fit a spatial regression model first. We can find the interval for the line search for the spatial coefficient \\(\\lambda\\) from the inverse of the extreme eigenvalues of the spatial weights, here using spatialreg::lextrB(): ints &lt;- 1/c(spatialreg::lextrB(lw_q_B)) ints #&gt; lambda_n lambda_1 #&gt; -0.293 0.157 From the output of a simultaneous autoregressive spatial error model, we see that the coefficient is close to its upper bound: SEM &lt;- spatialreg::errorsarlm(I_turnout ~ 1, data=pol_pres15, listw=lw_q_B, method=&quot;Matrix&quot;, interval=ints) spatialreg::coef.sarlm(SEM) #&gt; lambda (Intercept) #&gt; 0.139 0.459 The spatial error model is clearly a better description of the data than a mean model with just the intercept using a likelihood ratio test for comparison: spatialreg::LR1.sarlm(SEM) #&gt; #&gt; Likelihood Ratio diagnostics for spatial dependence #&gt; #&gt; data: #&gt; Likelihood ratio = 2111, df = 1, p-value &lt;2e-16 #&gt; sample estimates: #&gt; Log likelihood of spatial error model #&gt; 4386 #&gt; Log likelihood of OLS fit y #&gt; 3330 Reconstructing a linear model after filtering out the global spatial autocorrelation \\((I - \\lambda W)\\) as lm.target, we can also construct \\((I - \\lambda W)^{-1}\\) representing the global spatial process as input to the Omega= argument. This is used internally to re-balance the matrix products of the per-observation star weights matrices, so involves substantial amounts of numerical linear algebra, including solving a dense matrix eigeneproblem, for each chosen observation. So if we take global autocorrelation into account, in this case the no-adjustment count of observations with p-values \\(\\lt 0.05\\) is under 7%, and using Bonferroni correction, we drop to 0.2%. lm.target &lt;- lm(SEM$tary ~ SEM$tarX - 1) Omega &lt;- invIrW(lw_q_B, rho=SEM$lambda) locmsO &lt;- localmoran.sad(lm.target, nb=nb_q, style=&quot;B&quot;, alternative=&quot;two.sided&quot;, Omega=Omega) locmsO &lt;- as.data.frame(locmsO) pvssO &lt;- cbind(locmsO[,5], p.adjust(locmsO[,5], &quot;bonferroni&quot;), p.adjust(locmsO[,5], &quot;fdr&quot;), p.adjust(locmsO[,5], &quot;BY&quot;)) colnames(pvssO) &lt;- c(&quot;none&quot;, &quot;bonferroni&quot;, &quot;fdr&quot;, &quot;BY&quot;) apply(pvssO, 2, function(x) sum(x &lt; 0.05)) #&gt; none bonferroni fdr BY #&gt; 167 5 13 4 In the global measure case, bootstrap permutations could be used as an alternative to analytical methods for possible inference. In the local case, conditional permutation may be used, retaining the value at observation \\(i\\) and randomly sampling from the remaining \\(n-1\\) values to find randomised values at neighbours. x &lt;- pol_pres15$I_turnout lw &lt;- lw_q_B xx &lt;- mean(x) z &lt;- x - xx s2 &lt;- sum(z^2)/length(x) crd &lt;- card(lw$neighbours) nsim &lt;- 999 res_p &lt;- numeric(nsim) mns &lt;- numeric(length(x)) sds &lt;- numeric(length(x)) set.seed(1) for (i in seq(along=x)) { wtsi &lt;- lw$weights[[i]] zi &lt;- z[i] z_i &lt;- z[-i] crdi &lt;- crd[i] if (crdi &gt; 0) { for (j in 1:nsim) { sz_i &lt;- sample(z_i, size=crdi) lz_i &lt;- sum(sz_i*wtsi) res_p[j] &lt;- (zi/s2)*lz_i } mns[i] &lt;- mean(res_p) sds[i] &lt;- sd(res_p) } else { mns[i] &lt;- as.numeric(NA) sds[i] &lt;- as.numeric(NA) } } This approach is not provided as a function in spdep. The outcome is that almost 32% of observations have two sided p-values \\(\\lt 0.05\\) without multiple comparison correction, and under 3% with Bonferroni correction. Since it is not advisable to use conditional permutation with regression residuals in the presence of spatial autocorrelation, localmoran.sad() and localmoran.exact() provide wys of calculating more robust standard deviates with the added flexibility of a possibly richer mean model. perm_Zi &lt;- (locm[,1] - mns)/sds pv &lt;- 2 * pnorm(abs(perm_Zi), lower.tail = FALSE) pvsp &lt;- cbind(pv, p.adjust(pv, &quot;bonferroni&quot;), p.adjust(pv, &quot;fdr&quot;), p.adjust(pv, &quot;BY&quot;)) colnames(pvsp) &lt;- c(&quot;none&quot;, &quot;bonferroni&quot;, &quot;fdr&quot;, &quot;BY&quot;) apply(pvsp, 2, function(x) sum(x &lt; 0.05)) #&gt; none bonferroni fdr BY #&gt; 789 68 477 160 In order to compare the results from localmoran() with the PySAL function in esda, we need first to re-run dividing by \\(n-1\\) instead of \\(n\\) in parts of the calculation, by setting the argument mlvar=FALSE. locm_nml &lt;- localmoran(pol_pres15$I_turnout, listw=lw_q_B, alternative=&quot;two.sided&quot;, mlvar=FALSE) Once this is done, the local estimates of Moran’s \\(I\\) agree within machine precision. np$random$seed(1L) loc_I_ps &lt;- esda$Moran_Local(pol_pres15$I_turnout, nb_gal_ps, transformation=&quot;B&quot;, permutations=999L) all.equal(unname(locm_nml[,1]), c(loc_I_ps$Is)) #&gt; [1] TRUE The PySAL outcomes after adjusting for multiple comparisons are also similar to those shown above: pv &lt;- 2 * pnorm(abs(loc_I_ps$z_sim), lower.tail = FALSE) pvsp &lt;- cbind(pv, p.adjust(pv, &quot;bonferroni&quot;), p.adjust(pv, &quot;fdr&quot;), p.adjust(pv, &quot;BY&quot;)) colnames(pvsp) &lt;- c(&quot;none&quot;, &quot;bonferroni&quot;, &quot;fdr&quot;, &quot;BY&quot;) apply(pvsp, 2, function(x) sum(x &lt; 0.05)) #&gt; none bonferroni fdr BY #&gt; 787 73 480 157 The figure shows the breadth of the density distribution of the standard deviates calculated using the analytical randomisation approach, compared with those of Saddlepoint approximation with and without the accommodation of global spatial autocorrelation, and with conditional permutation. df &lt;- data.frame(std_deviate=c(locm[,4], locms[,4], locmsO[,4], perm_Zi, loc_I_ps$z_sim), method=rep(c(&quot;Analytical randomisation&quot;, &quot;Saddlepoint approximation&quot;, &quot;Saddlepoint approximation (Omega)&quot;, &quot;Conditional permutation&quot;, &quot;Conditional permutation (PySAL)&quot;), each=2495)) library(ggplot2) ggplot(df) + geom_density(aes(x=std_deviate, fill=method), alpha=0.2) + xlim(c(-6, 12)) #&gt; Warning: Removed 26 rows containing non-finite values (stat_density). (#fig:localmoran_Zi)Local Moran’s I standard deviates by method The figure and the tabulated summaries of the standard deviates by method show that in this case the conditional permutations shift the medians and third quartiles of the distributions rightwards compared to the analytical standard deviates, yielding larger numbers of apparently “significantly” locally autocorelated areal units than, say, the Saddlepoint approximation. print(do.call(&quot;rbind&quot;, tapply(df$std_deviate, list(df$method), summary)), digits=3) #&gt; Min. 1st Qu. Median Mean 3rd Qu. #&gt; Analytical randomisation -3.90 0.0701 0.691 1.629 2.3183 #&gt; Conditional permutation -3.07 0.2656 1.235 1.305 2.2613 #&gt; Conditional permutation (PySAL) -3.11 0.2652 1.244 1.307 2.2635 #&gt; Saddlepoint approximation -2.66 0.1213 0.950 1.159 2.0047 #&gt; Saddlepoint approximation (Omega) -5.66 -0.8934 -0.379 -0.462 0.0361 #&gt; Max. #&gt; Analytical randomisation 22.47 #&gt; Conditional permutation 7.52 #&gt; Conditional permutation (PySAL) 7.86 #&gt; Saddlepoint approximation 6.67 #&gt; Saddlepoint approximation (Omega) 2.89 Taking global spatial autocorrelation into account leaves most of the areal units with “significant” local residual spatial autocorrelation below zero, indicating negative dependencies probably associated with urban/rural contrasts. The figure shows the dominance of positive local autocorrelation before the removal of global autocorrelation, and the speckled appearance of the local indocators after its removal using Omega, the global process model. pol_pres15$loc_I_SA &lt;- locms[,4] pol_pres15$loc_I_Omega &lt;- locmsO[,4] tm_shape(pol_pres15) + tm_fill(c(&quot;loc_I_SA&quot;, &quot;loc_I_Omega&quot;), midpoint=0, title=&quot;Std. deviate&quot;) + tm_facets(free.scales=FALSE) + tm_borders(lwd=0.3, alpha=0.4) + tm_layout(panel.labels=c(&quot;Saddlepoint approximation&quot;, &quot;Omega&quot;)) (#fig:localmoran_Zi_map)Local Moran’s I standard deviates The local Getis-Ord \\(G\\) measure is reported as a standard deviate, and may also take the \\(G^*\\) form where self-neighbours are inserted into the neighbour object using include.self(). The observed and expected values of local \\(G\\) with their analytical variances may also be returned if return_internals=TRUE. The GeoDa= argument changes summation behaviour when no-neighbour observations are present, dropping values of x from sums. args(localG) #&gt; function (x, listw, zero.policy = NULL, spChk = NULL, return_internals = FALSE, #&gt; GeoDa = FALSE) #&gt; NULL locG &lt;- localG(pol_pres15$I_turnout, lw_q_B, return_internals=TRUE) Once again we face the problem of multiple comparisons, with the count of areal unit p-values \\(\\lt 0.5\\) being reduced by an order of magnitude when employing Bonferroni correction: pv &lt;- 2 * pnorm(abs(c(locG)), lower.tail = FALSE) pvsp &lt;- cbind(pv, p.adjust(pv, &quot;bonferroni&quot;), p.adjust(pv, &quot;fdr&quot;), p.adjust(pv, &quot;BY&quot;)) colnames(pvsp) &lt;- c(&quot;none&quot;, &quot;bonferroni&quot;, &quot;fdr&quot;, &quot;BY&quot;) apply(pvsp, 2, function(x) sum(x &lt; 0.05)) #&gt; none bonferroni fdr BY #&gt; 789 69 468 156 The PySAL esda G_Local() function returns the same basic values, resulting in the same local standard deviates: np$random$seed(1L) loc_G_ps &lt;- esda$G_Local(pol_pres15$I_turnout, nb_gal_ps, transform=&quot;B&quot;, permutations=999L) all.equal(c(locG), c(loc_G_ps$Zs)) #&gt; [1] TRUE Code for conditional permutation is not included in spdep, but is not difficult to construct: x &lt;- pol_pres15$I_turnout lw &lt;- lw_q_B crd &lt;- card(lw$neighbours) nsim &lt;- 999 res &lt;- matrix(nrow=length(x), ncol=nsim) set.seed(1) x_star &lt;- sum(x[crd &gt; 0]) for (i in seq(along=x)) { wtsi &lt;- lw$weights[[i]] xi &lt;- x[i] x_i &lt;- x[-i] crdi &lt;- crd[i] if (crdi &gt; 0) { for (j in 1:nsim) { sx_i &lt;- sample(x_i, size=crdi) lx_i &lt;- sum(sx_i*wtsi) res[i, j] &lt;- lx_i/(x_star-xi) } } } If we follow esda$G_Local() and construct the permutation standard deviates from the average and standard deviation of the complete set of simulated local \\(G\\) values, it seems that the distribution of values is much closer to zero; the R case: G_obs &lt;- attr(locG, &quot;internals&quot;)[,1] G_Zi &lt;- (G_obs - mean(res))/sd(res) # code follows PySAL esda$G_Local() pv &lt;- 2 * pnorm(abs(c(G_Zi)), lower.tail = FALSE) pvsp &lt;- cbind(pv, p.adjust(pv, &quot;bonferroni&quot;), p.adjust(pv, &quot;fdr&quot;), p.adjust(pv, &quot;BY&quot;)) colnames(pvsp) &lt;- c(&quot;none&quot;, &quot;bonferroni&quot;, &quot;fdr&quot;, &quot;BY&quot;) apply(pvsp, 2, function(x) sum(x &lt; 0.05)) #&gt; none bonferroni fdr BY #&gt; 210 5 8 4 and the PySAL case: pv &lt;- 2 * pnorm(abs(c(loc_G_ps$z_sim)), lower.tail = FALSE) pvsp &lt;- cbind(pv, p.adjust(pv, &quot;bonferroni&quot;), p.adjust(pv, &quot;fdr&quot;), p.adjust(pv, &quot;BY&quot;)) colnames(pvsp) &lt;- c(&quot;none&quot;, &quot;bonferroni&quot;, &quot;fdr&quot;, &quot;BY&quot;) apply(pvsp, 2, function(x) sum(x &lt; 0.05)) #&gt; none bonferroni fdr BY #&gt; 210 5 8 4 If however we take the by entity local permutation outputs, averages and standard deviations by row of the simulation matrix, we get back to the relative proportions seen in the analytical standard deviates in the R case: G_Zi_1 &lt;- (G_obs - apply(res, 1, mean))/apply(res, 1, sd) pv &lt;- 2 * pnorm(abs(c(G_Zi_1)), lower.tail = FALSE) pvsp &lt;- cbind(pv, p.adjust(pv, &quot;bonferroni&quot;), p.adjust(pv, &quot;fdr&quot;), p.adjust(pv, &quot;BY&quot;)) colnames(pvsp) &lt;- c(&quot;none&quot;, &quot;bonferroni&quot;, &quot;fdr&quot;, &quot;BY&quot;) apply(pvsp, 2, function(x) sum(x &lt; 0.05)) #&gt; none bonferroni fdr BY #&gt; 789 68 477 160 and the PySAL case: mns_1 &lt;- apply(loc_G_ps$rGs, 1, mean) sds_1 &lt;- apply(loc_G_ps$rGs, 1, sd) z_sim_1 &lt;- (loc_G_ps$Gs - mns_1) / sds_1 pv &lt;- 2 * pnorm(abs(z_sim_1), lower.tail = FALSE) pvsp &lt;- cbind(pv, p.adjust(pv, &quot;bonferroni&quot;), p.adjust(pv, &quot;fdr&quot;), p.adjust(pv, &quot;BY&quot;)) colnames(pvsp) &lt;- c(&quot;none&quot;, &quot;bonferroni&quot;, &quot;fdr&quot;, &quot;BY&quot;) apply(pvsp, 2, function(x) sum(x &lt; 0.05)) #&gt; none bonferroni fdr BY #&gt; 787 73 478 157 (https://github.com/pysal/esda/issues/53). pol_pres15$loc_G_Zi_perm &lt;- G_Zi_1 pol_pres15$loc_G_Zi &lt;- c(locG) tm_shape(pol_pres15) + tm_fill(c(&quot;loc_G_Zi&quot;, &quot;loc_G_Zi_perm&quot;), midpoint=0, title=&quot;Std. deviate&quot;) + tm_facets(free.scales=FALSE) + tm_borders(lwd=0.3, alpha=0.4) + tm_layout(panel.labels=c(&quot;Analytical&quot;, &quot;Permutation&quot;)) (#fig:local_Gi_map)Local G standard deviates 17.4 Spatial heterogeneity Over and above local and global spatial autocorrelation, areal data are affected by heterogeneity. In some cases, this heterogeneity may be modelled in the mean model through included covariates, or possibly using unstructured random effects, which may also reflect the differing scales of spatial process footprints. Here we will only mention two specifically spatial approaches, the local spatial heteroscedasticity (LOSH) statistic and Moran eigenvectors. 17.4.1 Local spatial heteroscedasticity (LOSH) statistic Local spatial heteroscedasticity (LOSH) statistics were introduced fairly recently, and an implementation was contributed to spdep even more recently, so there is as yet little experience with the approach (Ord and Getis 2012). It has been extended to provide bootstrap p-values for the measures of heterogeneity (Xu, Mei, and Yan 2014). The a= argument takes a default value of \\(2\\), giving a Chi-squared interpretation to output. args(LOSH) #&gt; function (x, listw, a = 2, var_hi = TRUE, zero.policy = NULL, #&gt; na.action = na.fail, spChk = NULL) #&gt; NULL lh &lt;- LOSH(pol_pres15$I_turnout, listw=lw_q_B) It is also possible to map local spatially weighted mean values derived from the local measures, showing a smoothing effect pol_pres15$x_bar_i &lt;- lh[,5] tm_shape(pol_pres15) + tm_fill(c(&quot;I_turnout&quot;, &quot;x_bar_i&quot;), title=&quot;Turnout&quot;, n=6, style=&quot;fisher&quot;) + tm_facets(free.scales=FALSE) + tm_borders(lwd=0.3, alpha=0.4) + tm_layout(panel.labels=c(&quot;Turnout&quot;, &quot;Weighted means&quot;)) (#fig:local_Hi_map)Local spatially weighted mean values These approaches have been further extended but as yet without implementations in R (Westerholt, Resch, and Zipf 2015; Westerholt et al. 2018). 17.4.2 Moran eigenvectors There is a close connection between Moran’s \\(I\\) and the eigenproblem of the doubly centred weights matrix, and again with the spatial pattern of a variable of interest and combinations of eigenvectors of the same matrix (Griffith 2003; Griffith 2010; Chun and Griffith 2013). There is a literature in numerical ecology using the term of principle coordinates of neighbour matrices for the same approach. The value of global Moran’s \\(I\\) can be calculated using the hat matrix to centre the variable of interest in taking the cross products which yield Moran’s \\(I\\). n &lt;- nrow(pol_pres15) ones &lt;- rep(1, n) hat &lt;- (diag(n) - tcrossprod(ones) / n) Cmat &lt;- hat %*% listw2mat(lw_q_B) %*% hat x &lt;- pol_pres15$I_turnout I_MCM &lt;- c((n/Szero(lw_q_B) * crossprod(x, Cmat %*% x)/crossprod(x, hat %*% x))) I_MCM #&gt; [1] 0.691 The solution to the eigenproblem of matrix product gives us the eigenvectors, providing key information about the linkage structure of the graph of neighbours. EV &lt;- eigen(Cmat)$vectors pol_pres15$EV1 &lt;- EVa[,1] pol_pres15$EV2 &lt;- EVa[,2] pol_pres15$EV3 &lt;- EVa[,3] pol_pres15$EV4 &lt;- EVa[,4] tm_shape(pol_pres15) + tm_fill(c(&quot;EV1&quot;, &quot;EV2&quot;, &quot;EV3&quot;, &quot;EV4&quot;), midpoint=0, title=&quot;Eigenvectors&quot;, n=6, style=&quot;fisher&quot;) + tm_facets(free.scales=FALSE) + tm_borders(lwd=0.3, alpha=0.4) + tm_layout(panel.labels=c(&quot;EV 1&quot;, &quot;EV 2&quot;, &quot;EV 3&quot;, &quot;EV 4&quot;)) (#fig:EV_map)First four eigenvectors References "],
["spatial-regression.html", "Chapter 18 Spatial Regression 18.1 Spatial regression with spatial weights 18.2 Estimators 18.3 Implementation details 18.4 Markov random field and multilevel models with spatial weights", " Chapter 18 Spatial Regression Even though it may be tempting to focus on interpreting the map pattern of a response variable of interest, the pattern may largely derive from covariates (and their functional forms), as well as the respective spatial footprints of the variables in play. Spatial autoregressive models in two dimensions began without covariates and with clear links to time series (Whittle 1954). Extensions included tests for spatial autocorrelation in linear model residuals, and models applying the autoregressive component to the response or the residuals, where the latter matched the tests for residuals (Cliff and Ord 1972; Cliff and Ord 1973). These “lattice” models of areal data typically express the dependence between observations using a graph of neighbours in the form of a contiguity matrix. A division has grown up, possibly unhelpfully, between scientific fields using conditional autoregressive (CAR) models (Besag 1974), and simultaneous autoregressive models (SAR) (Ord 1975; Hepple 1976). Although CAR and SAR models are closely related, these fields have found it difficult to share experience of applying similar models, often despite referring to key work summarising the models (Ripley 1981; Ripley 1988; Cressie 1993). More recent books expounding the theoretical bases for modelling with areal data simply point out the similarities in relevant chapters (Gaetan and Guyon 2010; Lieshout 2019); the interested reader is invited to consult these sources for background information and examples using the functions described below. Of course, handling a spatial correlation structure in a generalised least squares model or a (generalised) linear or nonlinear mixed effects model such as those provided in the nlme and many other packages does not have to use a graph of neighbours (Pinheiro and Bates 2000). These models are also spatial regression models, using functions of the distance between observations, and fitted variograms to model the spatial autocorrelation present; such models have been held to yield a clearer picture of the underlying processes (Wall 2004), building on geostatistics. For example, the glmmTMB package successfully uses this approach to spatial regression (Brooks et al. 2017). Here we will only consider spatial regression using spatial weights, chiefly as implemented in the spatialreg package recently split out of the spdep package which had grown unnecessarily large, covering too many aspects of spatial dependence. 18.1 Spatial regression with spatial weights Spatial autoregression models using spatial weights matrices were described in some detail using maximum likelihood estimation some time ago (Cliff and Ord 1973; Cliff and Ord 1981). A family of models were elaborated in spatial econometric terms extending earlier work, and in many cases using the simultaneous autoregressive framework and row standardization of spatial weights (Anselin 1988). The simultaneous and conditional autoregressive frameworks can be compared, and both can be supplemented using case weights to reflect the relative importance of different observations (Waller and Gotway 2004). Here we shall use the Boston housing data set, which has been restructured and furnished with census tract boundaries (Bivand 2017). The original data set used 506 census tracts and a hedonic model to try to estimate willingness to pay for clean air. The response was constructed from counts of ordinal answers to a 1970 census question about house value; the response is left and right censored in the census source. The key covariate was created from a calibrated meteorological model showing the annual nitrogen oxides (NOX) level for a smaller number of model output zones. The numbers of houses responding also varies by tract and model output zone. There are several other covariates, some measured at the tract level, some by town only, where towns broadly correspond to the air pollution model output zones. library(sf) boston_506 &lt;- st_read(system.file(&quot;shapes/boston_tracts.shp&quot;, package=&quot;spData&quot;)[1]) #&gt; Reading layer `boston_tracts&#39; from data source `/home/edzer/R/x86_64-pc-linux-gnu-library/3.6/spData/shapes/boston_tracts.shp&#39; using driver `ESRI Shapefile&#39; #&gt; Simple feature collection with 506 features and 36 fields #&gt; geometry type: POLYGON #&gt; dimension: XY #&gt; bbox: xmin: -71.5 ymin: 42 xmax: -70.6 ymax: 42.7 #&gt; epsg (SRID): 4267 #&gt; proj4string: +proj=longlat +datum=NAD27 +no_defs nb_q &lt;- spdep::poly2nb(boston_506) lw_q &lt;- spdep::nb2listw(nb_q, style=&quot;W&quot;) We can start by reading in the 506 tract data set from spData, and creating a contiguity neighbour object and from that again a row standardized spatial weights object. If we examine the median house values, we find that they have been assigned as missing values, and that 17 tracts are affected. table(boston_506$censored) #&gt; #&gt; left no right #&gt; 2 489 15 summary(boston_506$median) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; 5600 16800 21000 21749 24700 50000 17 Next, we can subset to the remaining 489 tracts with non-censored house values, and the neighbour object to match. The neighbour object now has one observation with no neighbours. boston_489 &lt;- boston_506[!is.na(boston_506$median),] nb_q_489 &lt;- spdep::poly2nb(boston_489) lw_q_489 &lt;- spdep::nb2listw(nb_q_489, style=&quot;W&quot;, zero.policy=TRUE) The NOX_ID variable specifies the upper level aggregation, letting us aggregate the tracts to air pollution model output zones. We can create aggregate neighbour and row standardized spatial weights objects, and aggregate the NOX variable taking means, and the CHAS Charles River dummy variable for observations on the river. agg_96 &lt;- list(as.character(boston_506$NOX_ID)) boston_96 &lt;- aggregate(boston_506[, &quot;NOX_ID&quot;], by=agg_96, unique) boston_96 &lt;- st_cast(boston_96, &quot;MULTIPOLYGON&quot;) nb_q_96 &lt;- spdep::poly2nb(boston_96) lw_q_96 &lt;- spdep::nb2listw(nb_q_96) boston_96$NOX &lt;- aggregate(boston_506$NOX, agg_96, mean)$x boston_96$CHAS &lt;- aggregate(as.integer(boston_506$CHAS)-1, agg_96, max)$x The response is aggregated using the weightedMedian() function in matrixStats, and midpoint values for the house value classes. Counts of houses by value class were punched to check the published census values, which can be replicated using weightedMedian() at the tract level. Here we find two output zones with calculated weighted medians over the upper census question limit of USD 50,000, and remove them subsequently as they also are affected by not knowing the appropriate value to insert for the top class by value. nms &lt;- names(boston_506) ccounts &lt;- 23:31 for (nm in nms[c(22, ccounts, 36)]) { boston_96[[nm]] &lt;- aggregate(boston_506[[nm]], agg_96, sum)$x } br2 &lt;- c(3.50, 6.25, 8.75, 12.50, 17.50, 22.50, 30.00, 42.50, 60.00)*1000 counts &lt;- as.data.frame(boston_96)[, nms[ccounts]] f &lt;- function(x) matrixStats::weightedMedian(x=br2, w=x, interpolate=TRUE) boston_96$median &lt;- apply(counts, 1, f) is.na(boston_96$median) &lt;- boston_96$median &gt; 50000 summary(boston_96$median) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; 9009 20417 23523 25263 30073 49496 2 Before subsetting, we aggregate the remaining covariates by weighted mean using the tract population counts punched from the census (Bivand 2017). boston_94 &lt;- boston_96[!is.na(boston_96$median),] nb_q_94 &lt;- spdep::subset.nb(nb_q_96, !is.na(boston_96$median)) lw_q_94 &lt;- spdep::nb2listw(nb_q_94, style=&quot;W&quot;) We now have two data sets each at the lower, census tract level and the upper, air pollution model output zone level, one including the censored observations, the other excluding them. The original model related the log of median house values by tract to the square of NOX values, including other covariates usually related to house value by tract, such as aggregate room counts, aggregate age, ethnicity, social status, distance to downtown and to the nearest radial road, a crime rate, and town-level variables reflecting land use (zoning, industry), taxation and education (Bivand 2017). This structure will be used here to exercise issues raised in fitting spatial regression models, including the presence of multiple levels. form &lt;- formula(log(median) ~ CRIM + ZN + INDUS + CHAS + I((NOX*10)^2) + I(RM^2) + AGE + log(DIS) + log(RAD) + TAX + PTRATIO + I(BB/100) + log(I(LSTAT/100))) Before moving to presentations of issues raised in fitting spatial regression models, it is worth making a few further points. A recent review of spatial regression in a spatial econometrics setting is given by Kelejian and Piras (2017); note that their usage is to call the spatial coefficient of the lagged response \\(\\lambda\\) and that of the lagged residuals \\(\\rho\\), the reverse of other usage (Anselin 1988; LeSage and Pace 2009). One interesting finding is that relatively dense spatial weights matrices may downweight model estimates, suggesting that sparser weights are preferable (Smith 2009). Another useful finding is that the presence of residual spatial autocorrelation need not bias the estimates of variance of regression coefficients, provided that the covariates themselves do not exhibit spatial autocorrelation (Smith and Lee 2012). In general, however, the footprints of the spatial processes of the response and covariates may not be aligned, and if covariates and the residual are autocorrelated, it is likely that the estimates of variance of regression coefficients will be biassed downwards if attempts are not made to model the spatial processes. In trying to model these spatial processes, we may choose to model the spatial autocorrelation in the residual with a spatial error model (SEM). If the processes in the covariates and the response match, we should find little difference between the coefficients of a least squares and a SEM, but very often they diverge, suggesting that a Hausman test for this condition should be employed (Pace and LeSage 2008). This may be related to earlier discussions of a spatial equivalent to the unit root and cointegration where spatial processes match (Fingleton 1999). Work reviewed by Mur and Angulo (2006) on the Durbin model, including the spatially lagged covariates in the model, permits a shared spatial process to be viewed and tested for as a Common Factor (Burridge 1981; Bivand 1984). The inclusion of spatially lagged covariates lets us check whether the same spatial process is manifest in the response and the covariates (SEM), whether they are different processes, or whether no process is detected. A model with a spatial process in the response only is termed a spatial lag model (SLM, often SAR - spatial autoregressive), and with different processes in the response and covariates a spatial Durbin model (SDM) (LeSage and Pace 2009). If we extend this family with processes in the covariates and the residual, we get a spatial error Durbin model (SDEM). If it is chosen to admit a spatial process in the residuals in addition to a spatial process in the response, again two models are formed, a general nested model (GNM) nesting all the others, and a model without spatially lagged covariates (SAC, also known as SARAR). If neither the residuals nor the residual are modelled with spatial processes, spatially lagged covariates may be added to a linear model, as a spatially lagged X model (SLX) (Elhorst 2010; Bivand 2012; Halleck Vega and Elhorst 2015). Although making predictions for new locations for which covariates are observed was raised as an issue some time ago, it has many years to make progress in reviewing the possibilities (Bivand 2002; Goulard, Laurent, and Thomas-Agnan 2017). The prediction method for SLM, SDM, SEM, SDEM, SAC and GNM models fitted with maximum likelihood were contributed as a Google Summer of Coding project by Martin Gubri. This work, and work on similar models with missing data (Suesse 2018) is also relevant for exploring censored median house values in the Boston data set. Work on prediction also exposed the importance of the reduced form of these models, in which the spatial process in the response interacts with the regression coefficients in the SLM, SDM, SAC and GNM models. The consequence of these interactions is that a unit change in a covariate will only impact the response as the value of the regression coefficient if the spatial coefficient of the lagged response is zero. Where it is non-zero, global spillovers, impacts, come into play, and these impacts should be reported rather than the regression coefficients (LeSage and Pace 2009; Elhorst 2010; Bivand 2012; Halleck Vega and Elhorst 2015). Local impacts may be reported for SDEM and SLX models, using linear combination to calculate standard errors for the total impacts of each covariate (sums of coefficients on the covariates and their spatial lags). Current work in the spatialreg package is focused on refining the handling of spatially lagged covariates using a consistent Durbin= argument taking either a logical value or a formula giving the subset of covariates to add in spatially lagged form. There is a speculation that some covariates, for example some dummy variables, should not be added in spatially lagged form. This then extends to handling these included spatially lagged covariates appropriately in calculating impacts. This work applies to cross-sectional models fitted using MCMC or maximum likelihood, and will offer facilities to spatial panel models. It is worth mentioning the almost unexplored issues of functional form assumptions, for which flexible structures are useful, including spatial quantile regression presented in the McSpatial package (McMillen 2013). There are further issues with discrete response variables, covered by some functions in McSpatial, and in the spatialprobit and ProbitSpatial packages (Wilhelm and Matos 2013; Martinetti and Geniaux 2017); the MCMC implementations of the former are based on LeSage and Pace (2009). Finally, Wagner and Zeileis (2019) show how an SLM model may be used in the setting of recursive partitioning, with an implementation using spatialreg::lagsarlm() in the lagsarlmtree package. 18.2 Estimators The review of cross-sectional maximum likelihood and generalized method of moments (GMM) estimators in spatialreg and sphet for spatial econometrics style spatial regression models by Bivand and Piras (2015) is still largely valid. In the review, estimators in these R packages were compared with alternative implementations available in other programming languages elsewhere. The review did not cover Bayesian spatial econometrics style spatial regression. More has changed with respect to spatial panel estimators described in Millo and Piras (2012), but will not be covered here. 18.2.1 Maximum likelihood For models with single spatial coefficients (SEM and SDEM using errorsarlm(), SLM and SDM using lagsarlm()), the methods initially described by Ord (1975) are used. Both estimating functions take similar arguments, where the first two, formula= and data= are shared by most model estimating functions. The third argument is a listw spatial weights object, while na.action= behaves as in other model estimating functions if the spatial weights can reasonably be subsetted to avoid observations with missing values. The weights= argument may be used to provide weights indicating the known degree of per-observation variability in the variance term - this is not available for lagsarlm(). library(spatialreg) #&gt; Loading required package: Matrix #&gt; #&gt; Attaching package: &#39;Matrix&#39; #&gt; The following object is masked from &#39;package:tidyr&#39;: #&gt; #&gt; expand #&gt; #&gt; Attaching package: &#39;spatialreg&#39; #&gt; The following objects are masked from &#39;package:spdep&#39;: #&gt; #&gt; anova.sarlm, as_dgRMatrix_listw, as_dsCMatrix_I, #&gt; as_dsCMatrix_IrW, as_dsTMatrix_listw, as.spam.listw, #&gt; bptest.sarlm, can.be.simmed, cheb_setup, coef.gmsar, #&gt; coef.sarlm, coef.spautolm, coef.stsls, create_WX, #&gt; deviance.gmsar, deviance.sarlm, deviance.spautolm, #&gt; deviance.stsls, do_ldet, eigen_pre_setup, eigen_setup, eigenw, #&gt; errorsarlm, fitted.gmsar, fitted.ME_res, fitted.sarlm, #&gt; fitted.SFResult, fitted.spautolm, get.ClusterOption, #&gt; get.coresOption, get.mcOption, get.VerboseOption, #&gt; get.ZeroPolicyOption, GMargminImage, GMerrorsar, #&gt; griffith_sone, gstsls, Hausman.test, HPDinterval.lagImpact, #&gt; impacts, intImpacts, Jacobian_W, jacobianSetup, l_max, #&gt; lagmess, lagsarlm, lextrB, lextrS, lextrW, lmSLX, #&gt; logLik.sarlm, logLik.spautolm, LR.sarlm, LR1.sarlm, #&gt; LR1.spautolm, LU_prepermutate_setup, LU_setup, Matrix_J_setup, #&gt; Matrix_setup, mcdet_setup, MCMCsamp, ME, mom_calc, #&gt; mom_calc_int2, moments_setup, powerWeights, predict.sarlm, #&gt; predict.SLX, print.gmsar, print.ME_res, print.sarlm, #&gt; print.sarlm.pred, print.SFResult, print.spautolm, print.stsls, #&gt; print.summary.gmsar, print.summary.sarlm, #&gt; print.summary.spautolm, print.summary.stsls, residuals.gmsar, #&gt; residuals.sarlm, residuals.spautolm, residuals.stsls, #&gt; sacsarlm, SE_classic_setup, SE_interp_setup, #&gt; SE_whichMin_setup, set.ClusterOption, set.coresOption, #&gt; set.mcOption, set.VerboseOption, set.ZeroPolicyOption, #&gt; similar.listw, spam_setup, spam_update_setup, #&gt; SpatialFiltering, spautolm, spBreg_err, spBreg_lag, #&gt; spBreg_sac, stsls, subgraph_eigenw, summary.gmsar, #&gt; summary.sarlm, summary.spautolm, summary.stsls, trW, #&gt; vcov.sarlm, Wald1.sarlm args(errorsarlm) #&gt; function (formula, data = list(), listw, na.action, weights = NULL, #&gt; Durbin, etype, method = &quot;eigen&quot;, quiet = NULL, zero.policy = NULL, #&gt; interval = NULL, tol.solve = .Machine$double.eps, trs = NULL, #&gt; control = list()) #&gt; NULL args(lagsarlm) #&gt; function (formula, data = list(), listw, na.action, Durbin, type, #&gt; method = &quot;eigen&quot;, quiet = NULL, zero.policy = NULL, interval = NULL, #&gt; tol.solve = .Machine$double.eps, trs = NULL, control = list()) #&gt; NULL The Durbin= argument replaces the earlier type= and etype= arguments, and if not given is taken as FALSE. If given, it may be FALSE, TRUE in which case all spatially lagged covariates are included, or a one-sided formula specifying which spatially lagged covariates should be included. The method= argument gives the method for calculating the log determinant term in the log likelihood function, and defaults to &quot;eigen&quot;, suitable for moderate sized data sets. The interval= argument gives the bounds of the domain for the line search using stats::optimize() used for finding the spatial coefficient. The tol.solve() argument, passed through to base::solve(), was needed to handle data sets with differing numerical scales among the coefficients which hindered inversion of the variance-covariance matrix; the default value in base::solve() used to be much larger. The control= argument takes a list of control values to permit more careful adjustment of the running of the estimation function. The spautolm() function also fits spatial regressions with the spatial process in the residuals, and takes a possibly poorly named family= argument, taking the values of &quot;SAR&quot; for the simultaneous autoregressive model (like errorsarlm()), &quot;CAR&quot; for the conditional autoregressive model, and &quot;SMA&quot; for the spatial moving average model. args(spautolm) #&gt; function (formula, data = list(), listw, weights, na.action, #&gt; family = &quot;SAR&quot;, method = &quot;eigen&quot;, verbose = NULL, trs = NULL, #&gt; interval = NULL, zero.policy = NULL, tol.solve = .Machine$double.eps, #&gt; llprof = NULL, control = list()) #&gt; NULL The sacsarlm() function may take second spatial weights and interval arguments if the spatial weights used to model the two spatial processes in the SAC and GNM specifications differ. By default, the same spatial weights are used. By default, stats::nlminb() is used for numerical optimization, using a heuristic to choose starting values. args(sacsarlm) #&gt; function (formula, data = list(), listw, listw2 = NULL, na.action, #&gt; Durbin, type, method = &quot;eigen&quot;, quiet = NULL, zero.policy = NULL, #&gt; tol.solve = .Machine$double.eps, llprof = NULL, interval1 = NULL, #&gt; interval2 = NULL, trs1 = NULL, trs2 = NULL, control = list()) #&gt; NULL Where larger data sets are used, a numerical Hessian approach is used to calculate the variance-covariance matrix of coefficients, rather than an analytical asymptotic approach. Apart from spautolm() which returns an &quot;spautolm&quot; object, the model fitting functions return &quot;sarlm&quot; objects. Standard methods for fitted models are provided, such as summary(): args(summary.sarlm) #&gt; function (object, correlation = FALSE, Nagelkerke = FALSE, Hausman = FALSE, #&gt; adj.se = FALSE, ...) #&gt; NULL The Nagelkerke= argument permits the return of a value approximately corresponding to a coefficient of determination, although the summary method anyway provides the value of stats::AIC() because a stats::logLik() method is provided for &quot;sarlm&quot; and &quot;spautolm&quot; objects. If the &quot;sarlm&quot; object is a SEM or SDEM, the Hausman test may be performed by setting Hausman=TRUE to see whether the regression coefficients are sufficiently like least squares coefficients, indicating absence of mis-specification from that source. As an example, we may fit SEM and SDEM to the 94 and 489 observation Boston data sets, and present the Hausman test results: eigs_489 &lt;- eigenw(lw_q_489) SDEM_489 &lt;- errorsarlm(form, data=boston_489, listw=lw_q_489, Durbin=TRUE, zero.policy=TRUE, control=list(pre_eig=eigs_489)) SEM_489 &lt;- errorsarlm(form, data=boston_489, listw=lw_q_489, zero.policy=TRUE, control=list(pre_eig=eigs_489)) cbind(data.frame(model=c(&quot;SEM&quot;, &quot;SDEM&quot;)), rbind(broom::tidy(Hausman.test(SEM_489)), broom::tidy(Hausman.test(SDEM_489))))[,1:4] #&gt; model statistic p.value parameter #&gt; 1 SEM 52.0 2.83e-06 14 #&gt; 2 SDEM 48.7 6.48e-03 27 Here we are using the control= list argument to pass through pre-computed eigenvalues for the default &quot;eigen&quot; method. Both test results for the 489 tract data set suggest that the regression coefficients do differ, perhaps that the footprints of the spatial processes do not match. Likelihood ratio tests of the spatial models against their least squares equivalents show that spatial process(es) are present, but we find that neither SEM not SDM are adequate representations. cbind(data.frame(model=c(&quot;SEM&quot;, &quot;SDEM&quot;)), rbind(broom::tidy(LR1.sarlm(SEM_489)), broom::tidy(LR1.sarlm(SDEM_489))))[,c(1, 4:6)] #&gt; model statistic p.value parameter #&gt; 1 SEM 198 0 1 #&gt; 2 SDEM 159 0 1 For the 94 air pollution model output zones, the Hausman tests find little difference between coefficients, but this is related to the fact that the SEM and SDEM models add little to least squares or SLX using likelihood ratio tests. eigs_94 &lt;- eigenw(lw_q_94) SDEM_94 &lt;- errorsarlm(form, data=boston_94, listw=lw_q_94, Durbin=TRUE, control=list(pre_eig=eigs_94)) SEM_94 &lt;- errorsarlm(form, data=boston_94, listw=lw_q_94, control=list(pre_eig=eigs_94)) cbind(data.frame(model=c(&quot;SEM&quot;, &quot;SDEM&quot;)), rbind(broom::tidy(Hausman.test(SEM_94)), broom::tidy(Hausman.test(SDEM_94))))[, 1:4] #&gt; model statistic p.value parameter #&gt; 1 SEM 15.66 0.335 14 #&gt; 2 SDEM 9.21 0.999 27 cbind(data.frame(model=c(&quot;SEM&quot;, &quot;SDEM&quot;)), rbind(broom::tidy(LR1.sarlm(SEM_94)), broom::tidy(LR1.sarlm(SDEM_94))))[,c(1, 4:6)] #&gt; model statistic p.value parameter #&gt; 1 SEM 2.593 0.107 1 #&gt; 2 SDEM 0.216 0.642 1 We can use spatialreg::LR.sarlm() to apply a likelihood ratio test between nested models, but here choose lmtest::lrtest(), which gives the same results, preferring models including spatially lagged covariates. broom::tidy(lmtest::lrtest(SEM_489, SDEM_489)) #&gt; Warning in tidy.anova(lmtest::lrtest(SEM_489, SDEM_489)): The following #&gt; column names in ANOVA output were not recognized or transformed: X.Df, #&gt; LogLik #&gt; Warning: Unknown or uninitialised column: &#39;term&#39;. #&gt; # A tibble: 2 x 5 #&gt; X.Df LogLik df statistic p.value #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 16 273. NA NA NA #&gt; 2 29 311. 13 74.4 1.23e-10 broom::tidy(lmtest::lrtest(SEM_94, SDEM_94)) #&gt; Warning in tidy.anova(lmtest::lrtest(SEM_94, SDEM_94)): The following #&gt; column names in ANOVA output were not recognized or transformed: X.Df, #&gt; LogLik #&gt; Warning: Unknown or uninitialised column: &#39;term&#39;. #&gt; # A tibble: 2 x 5 #&gt; X.Df LogLik df statistic p.value #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 16 59.7 NA NA NA #&gt; 2 29 81.3 13 43.2 0.0000421 The SLX model is fitted using least squares, and also returns a log likelihood value, letting us test whether we need a spatial process in the residuals. args(lmSLX) #&gt; function (formula, data = list(), listw, na.action, weights = NULL, #&gt; Durbin = TRUE, zero.policy = NULL) #&gt; NULL In the tract data set we obviously do: SLX_489 &lt;- lmSLX(form, data=boston_489, listw=lw_q_489, zero.policy=TRUE) broom::tidy(lmtest::lrtest(SLX_489, SDEM_489)) #&gt; Warning in modelUpdate(objects[[i - 1]], objects[[i]]): original model was #&gt; of class &quot;SLX&quot;, updated model is of class &quot;sarlm&quot; #&gt; Warning in tidy.anova(lmtest::lrtest(SLX_489, SDEM_489)): The following #&gt; column names in ANOVA output were not recognized or transformed: X.Df, #&gt; LogLik #&gt; Warning: Unknown or uninitialised column: &#39;term&#39;. #&gt; # A tibble: 2 x 5 #&gt; X.Df LogLik df statistic p.value #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 28 231. NA NA NA #&gt; 2 29 311. 1 159. 1.55e-36 but in the output zone case we do not. SLX_94 &lt;- lmSLX(form, data=boston_94, listw=lw_q_94) broom::tidy(lmtest::lrtest(SLX_94, SDEM_94)) #&gt; Warning in modelUpdate(objects[[i - 1]], objects[[i]]): original model was #&gt; of class &quot;SLX&quot;, updated model is of class &quot;sarlm&quot; #&gt; Warning in tidy.anova(lmtest::lrtest(SLX_94, SDEM_94)): The following #&gt; column names in ANOVA output were not recognized or transformed: X.Df, #&gt; LogLik #&gt; Warning: Unknown or uninitialised column: &#39;term&#39;. #&gt; # A tibble: 2 x 5 #&gt; X.Df LogLik df statistic p.value #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 28 81.2 NA NA NA #&gt; 2 29 81.3 1 0.216 0.642 This outcome is sustained also when we use the counts of house units by tract and output zones as weights: SLX_94w &lt;- lmSLX(form, data=boston_94, listw=lw_q_94, weights=units) SDEM_94w &lt;- errorsarlm(form, data=boston_94, listw=lw_q_94, Durbin=TRUE, weights=units, control=list(pre_eig=eigs_94)) broom::tidy(lmtest::lrtest(SLX_94w, SDEM_94w)) #&gt; Warning in modelUpdate(objects[[i - 1]], objects[[i]]): original model was #&gt; of class &quot;SLX&quot;, updated model is of class &quot;sarlm&quot; #&gt; Warning in tidy.anova(lmtest::lrtest(SLX_94w, SDEM_94w)): The following #&gt; column names in ANOVA output were not recognized or transformed: X.Df, #&gt; LogLik #&gt; Warning: Unknown or uninitialised column: &#39;term&#39;. #&gt; # A tibble: 2 x 5 #&gt; X.Df LogLik df statistic p.value #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 28 97.5 NA NA NA #&gt; 2 29 98.0 1 0.917 0.338 18.2.2 Generalized method of moments The estimation methods used for fitting SLM and the spatially lagged response part of SARAR models are based on instrumental variables, using the spatially lagged covariates (usually including lags of lags too) as instruments for the spatially lagged response (Piras 2010). This, and the use of spatially lagged covariates in the moment conditions for models including a spatial process in the residuals, means that including the spatially lagged covariates in the initial model is challenging, and functions in the sphet package do not provide a Durbin= argument. This makes it harder to accommodate data with multiple spatial process footprints. However, as Kelejian and Piras show in their recent review (2017), these approaches have other benefits, such as being able to instrument variables suspected of suffering from endogeneity or measurement error. Let us first compare the ML and GMM estimates of the SEM regression coefficients for rescaled squared NOX values. We use the sphet::spreg() wrapper function, and fit a SEM model. Extracting the matching rows from the summary objects for these models, we can see that the values are not dissimilar, despite the difference in estimation methods. library(sphet) SEM1_94 &lt;- spreg(form, data=boston_94, listw=lw_q_94, model=&quot;error&quot;) res &lt;- rbind(summary(SEM_94)$Coef[&quot;I((NOX * 10)^2)&quot;,], summary(SEM1_94)$CoefTable[&quot;I((NOX * 10)^2)&quot;,]) rownames(res) &lt;- c(&quot;ML&quot;, &quot;GMM&quot;) res #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; ML -0.00956 0.00261 -3.66 0.000247 #&gt; GMM -0.00885 0.00267 -3.31 0.000930 Using sphet::spreg(), we can instrument the rescaled squared NOX variable, dropping it first from the formula, next creating the rescaled squared NOX variable as a column in the &quot;sf&quot; object, extracting a matrix of coordinates from the centroids of the output zones, and creating a one-sided instrument formula from a second order polynomial in the coordinates (here improperly, as they are not projected) and mean distances to downtown and radial roads. The endog= argument takes a one-sided formula for the variables to be modelled in the first stage of the model. Had we re-run the original air pollution model many times under slightly varying scenarios, we could have used an ensemble of NOX loadings to yield its distribution by output zone. Because this is not possible, we assume that the measurement error can be captured by using selected instruments. Unfortunately, the NOX regression coefficient estimate from the second stage has fallen substantially in absolute size, although the sign is unchanged. formiv &lt;- update(form, . ~ . - I((NOX*10)^2)) boston_94$NOX2 &lt;- (boston_94$NOX*10)^2 suppressWarnings(ccoords &lt;- st_coordinates(st_centroid(st_geometry(boston_94)))) iform &lt;- formula(~poly(ccoords, degree=2) + DIS + RAD) SEM1_94iv &lt;- spreg(formiv, data=boston_94, listw=lw_q_94, endog = ~NOX2, instruments=iform, model=&quot;error&quot;) summary(SEM1_94iv)$CoefTable[&quot;NOX2&quot;,] #&gt; Estimate Std. Error t-value Pr(&gt;|t|) #&gt; -0.00195 0.00454 -0.43063 0.66674 Handling measurement error in this or similar ways is one of the benefits of GMM estimation methods, although here the choice of instruments was somewhat arbitrary. 18.2.3 Markov chain Monte Carlo (draft - a comparative piece is being written for submission in about two months) The Spatial Econometrics Library is part of the extensive Matlab code repository at https://www.spatial-econometrics.com/ and documented in LeSage and Pace (2009). The Google Summer of Coding project in 2011 by Abhirup Mallik mentored by Virgilio Gómez-Rubio yielded translations of some of the model fitting functions for SEM, SDEM, SLM, SDM, SAC and GNM from the Matlab code. These have now been added to spatialreg as spBreg_err(), spBreg_lag() and spBreg_sac() with Durbin= arguments to handle the inclusion of spatially lagged covariates. As yet, heteroskedastic disturbances are not accommodated. The functions return &quot;mcmc&quot; objects as specified in the coda package, permitting the use of tools from coda for handling model output. The default burnin is 500 draws, followed by default 2000 draws returned, but these values and many others may be set through the control= list argument. Fitting the SDEM model for the output zones takes about an order of magnitude longer than using ML, but there is more work to do subsequently, and this difference scales more in the number of samples than covariates or observations. system.time(SDEM_94B &lt;- spBreg_err(form, data=boston_94, listw=lw_q_94, Durbin=TRUE)) #&gt; user system elapsed #&gt; 1.92 0.00 1.92 system.time(SDEM_489B &lt;- spBreg_err(form, data=boston_489, listw=lw_q_489, Durbin=TRUE, zero.policy=TRUE)) #&gt; user system elapsed #&gt; 2.998 0.004 3.002 Most time in the ML case using eigenvalues is taken by log determinant setup and optimization, and by dense matrix asymptotic standard errors if chosen (always chosen with default &quot;eigen&quot; log determinant method): t(errorsarlm(form, data=boston_94, listw=lw_q_94, Durbin=TRUE)$timings[,2]) #&gt; set_up eigen_set_up eigen_opt coefs eigen_hcov eigen_se #&gt; [1,] 0.006 0.006 0.01 0.005 0.003 0.002 t(errorsarlm(form, data=boston_489, listw=lw_q_489, Durbin=TRUE, zero.policy=TRUE)$timings[,2]) #&gt; set_up eigen_set_up eigen_opt coefs eigen_hcov eigen_se #&gt; [1,] 0.01 0.06 0.024 0.006 0.082 0.176 while in the MCMC case, the default use of Spatial Econometric Toolbox gridded log determinants and obviously sampling takes most time: t(attr(SDEM_94B, &quot;timings&quot;)[ , 3]) #&gt; set_up SE_classic_set_up complete_setup sampling finalise #&gt; [1,] 0.004 0.26 0.034 1.56 0.058 t(attr(SDEM_489B, &quot;timings&quot;)[ , 3]) #&gt; set_up SE_classic_set_up complete_setup sampling finalise #&gt; [1,] 0.015 0.463 0.081 2.38 0.067 However, as we will see shortly, inference from model impacts may need sampling (where the spatially lagged response is included in the model), and in the case of MCMC models, the samples have already been drawn. 18.3 Implementation details We will briefly touch on selected implementation details that applied users of spatial regression models would be wise to review. The handling of the log determinant term applies to all such users, while impacts are restricted to those employing spatial econometrics style models either including the spatially lagged response or including spatially lagged covariates. 18.3.1 Handling the log determinant term It has been known for over twenty years that the sparse matrix representation of spatial weights overcomes the difficulties of fitting models with larger numbers of observations using maximum likelihood and MCMC where the log determinant term comes into play (R. K. Pace and Barry 1997a; R. K. Pace and Barry 1997b; R. K. Pace and Barry 1997c; R. K. Pace and Barry 1997d). During the development of these approaches in model fitting functions in spatialreg, use was first made of C code also used in the S-PLUS SpatialStats module (Kaluzny et al. 1998), then SparseM which used a compressed sparse row form very similar to &quot;nb&quot; and &quot;listw&quot; objects. This was followed by the use of spam and Matrix methods, both of which mainly use compressed sparse column representations. Details are provided in Bivand, Hauke and Kossowski (2013). The domain of the spatial coefficient(s) is given by the interval= argument to model fitting functions, and returned in the fitted object: SEM_94$interval #&gt; [1] -1.53 1.00 This case is trivial, because the upper bound is unity by definition, because of the use of row standardization. The interval is the inverse of the range of the eigenvalues of the weights matrix: 1/range(eigs_94) #&gt; [1] -1.53 1.00 Finding the interval within which to search for the spatial coefficient is trivial for smaller data sets, but more complex for larger ones. It is possible to use heuristics implemented in lextrW() (Griffith, Bivand, and Chun 2015): 1/c(lextrW(lw_q_94)) #&gt; lambda_n lambda_1 #&gt; -1.53 1.00 or RSpectra::eigs() after coercion to a Matrix package compressed sparse column representation: W &lt;- as(lw_q_94, &quot;CsparseMatrix&quot;) 1/Re(c(RSpectra::eigs(W, k=1, which=&quot;SR&quot;)$values, RSpectra::eigs(W, k=1, which=&quot;LR&quot;)$values)) #&gt; [1] -1.53 1.00 Why are we extracting the real part of the values returned by eigs()? Since nb_q_94 is symmetric, the row-standardized object lw_q_94 is symmetric by similarity, a result known since Ord (1975); consequently, we can take the real part without concern. Had the underlying neighbour relationships not been symmetric, we should be more careful. The baseline log determinant term as given by Ord (1975) for a coefficient value proposed in sampling or during numerical optimization; this extract matches the &quot;eigen&quot; method (with or without control=list(pre_eig=...)&quot;): coef &lt;- 0.5 sum(log(1 - coef * eigs_94)) #&gt; [1] -2.87 Using sparse matrix functions from Matrix, the LU decomposition can be used for asymmetric matrices; this extract matches the &quot;LU&quot; method: I &lt;- Diagonal(nrow(boston_94)) LU &lt;- lu(I - coef * W) dU &lt;- abs(diag(slot(LU, &quot;U&quot;))) sum(log(dU)) #&gt; [1] -2.87 and Cholesky decomposition for symmetric matrices, with similar.listw() used to handle asymmetric weights that are similar to symmetric. The default value od super allows the underlying Matrix code to choose between supernodal or simplicial decomposition; this extract matches the &quot;Matrix_J&quot; method: W &lt;- as(similar.listw(lw_q_94), &quot;CsparseMatrix&quot;) super &lt;- as.logical(NA) cch &lt;- Cholesky((I - coef * W), super=super) c(2 * determinant(cch, logarithm = TRUE)$modulus) #&gt; [1] -2.87 The &quot;Matrix&quot; and &quot;spam_update&quot; methods are to be preferred as they pre-compute the fill-reducing permutation of the decomposition since the weights do not change for different values of the coefficient. Maximum likelihood model fitting functions in spatialreg and splm use jacobianSetup() to populate env= environment with intermediate objects needed to find log determinants during optimization. Passing environments to objective functions is efficient because they are passed by reference rather than value. The con= argument passes through the populated control list, containing default values unless the key-value pairs were given in the function call (pre_eig= is extracted separately). The which= argument is 1 by default, but will also take 2 in SAC and GNM models. HSAR uses mcdet_setup() to set up Monte Carlo approximation terms. args(jacobianSetup) #&gt; function (method, env, con, pre_eig = NULL, trs = NULL, interval = NULL, #&gt; which = 1) #&gt; NULL For each value of coef, the do_ldet() function returns the log determinant, using the values stored in environment env=: args(do_ldet) #&gt; function (coef, env, which = 1) #&gt; NULL As yet the Bayesian models are limited to control argument ldet_method=&quot;SE_classic&quot; at present, using &quot;LU&quot; to generate a coarse grid of control argument nrho=200L log determinant values in the interval, spline interpolated to a finer grid of length control argument interpn=2000L, from which griddy Gibbs samples are drawn. It is hoped to add facilities to choose alternative methods in the future. This would offer possibilities to move beyond griddy Gibbs, but using gridded log determinant values seems reasonable at present. 18.3.2 Impacts Global impacts have been seen as crucial for reporting results from fitting models including the spatially lagged response (SLM, SDM, SAC. GNM) for over ten years (LeSage and Pace 2009). Extension to other models including spatially lagged covariates (SLX, SDEM) has followed (Elhorst 2010; Bivand 2012; Halleck Vega and Elhorst 2015). In order to infer from the impacts, linear combination may be used for SLX and SDEM models. For SLM, SDM, SAC and GNM models fitted with maximum likelihood or GMM, the variance-covariance matrix of the coefficients is available, and can be used to make random draws from a multivariate Normal distribution with mean set to coefficient values and variance to the estimated variance-covariance matrix. For these models fitted using Bayesian methods, draws are already available. In the SDEM case, the draws on the regression coefficients of the unlagged covariates represent direct impacts, and draws on the coefficients of the spatially lagged covariates represent indirect impacts, and their by-draw sums the total impacts. Impacts are calculated using model object class specific impacts() methods, here taking the method for &quot;sarlm&quot; objects as an example. In the sphet package, the impacts method for &quot;gstsls&quot; uses the spatialreg impacts() framework, as does the splm package for &quot;splm&quot; fitted model objects. impacts() methods require either a tr= - a vector of traces of the power series of the weights object typically computed with trW() or a listw= argument. If listw= is given, dense matrix methods are used. The evalues= argument is experimental, does not yet work for all model types, and takes the eigenvalues of the weights matrix. The R= argument gives the number of samples to be taken from the fitted model. The Q= permits the decomposition of impacts to components of the power series of the weights matrix (LeSage and Pace 2009). args(impacts.sarlm) #&gt; function (obj, ..., tr = NULL, R = NULL, listw = NULL, evalues = NULL, #&gt; useHESS = NULL, tol = 1e-06, empirical = FALSE, Q = NULL) #&gt; NULL The summary method for the output of impacts() methods where inference from samples was requested by default uses the summary() method for &quot;mcmc&quot; objects defined in the coda package. It can instead report just matrices of standard errors, z-values and p-values by setting zstats= and short= to TRUE. args(summary.lagImpact) #&gt; function (object, ..., zstats = FALSE, short = FALSE, reportQ = NULL) #&gt; NULL Since sampling is not required for inference for SLX and SDEM models, linear combination is used for models fitted using maximum likelihood; results are shown here for the air pollution variable only. The literature has not yet resolved the question of how to report model output, as each covariate is now represented by three impacts. Where spatially lagged covariates are included, two coefficients are replaced by three impacts. sum_imp_94_SDEM &lt;- summary(impacts(SDEM_94)) rbind(Impacts=sum_imp_94_SDEM$mat[5,], SE=sum_imp_94_SDEM$semat[5,]) #&gt; Direct Indirect Total #&gt; Impacts -0.01276 -0.01845 -0.0312 #&gt; SE 0.00235 0.00472 0.0053 The impacts from the same model fitted by MCMC are very similar: sum_imp_94_SDEM_B &lt;- summary(impacts(SDEM_94B)) rbind(Impacts=sum_imp_94_SDEM_B$mat[5,], SE=sum_imp_94_SDEM_B$semat[5,]) #&gt; Direct Indirect Total #&gt; Impacts -0.01275 -0.01765 -0.03041 #&gt; SE 0.00248 0.00552 0.00654 as also are those from the SLX model. In the SLX and SDEM models, the direct impacts are the consequences for the response of changes in air pollution in the same observational entity, and the indirect (local) impacts are the consequences for the response of changes in air pollution in neighbouring observational entities. sum_imp_94_SLX &lt;- summary(impacts(SLX_94)) rbind(Impacts=sum_imp_94_SLX$mat[5,], SE=sum_imp_94_SLX$semat[5,]) #&gt; Direct Indirect Total #&gt; Impacts -0.0128 -0.01874 -0.03151 #&gt; SE 0.0028 0.00556 0.00611 In contrast to local indirect impacts in SLX and SDEM models, global indirect impacts are found in models including the spatially lagged response. For purposes of exposition, let us fit an SLM: SLM_489 &lt;- lagsarlm(form, data=boston_489, listw=lw_q_489, zero.policy=TRUE) Traces of the first m= matrices of the power series in the spatial weights are pre-computed to speed up inference from samples from the fitted model, and from existing MCMC samples (LeSage and Pace 2009). The traces can also be used in other contexts too, so their pre-computation may be worthwhile anyway. The type= argument is &quot;mult&quot; by default, but may also be set to &quot;MC&quot; for Monte Carlo simulation or &quot;moments&quot; using a space-saving looping algorithm. args(trW) #&gt; function (W = NULL, m = 30, p = 16, type = &quot;mult&quot;, listw = NULL, #&gt; momentsSymmetry = TRUE) #&gt; NULL W &lt;- as(lw_q_489, &quot;CsparseMatrix&quot;) tr_489 &lt;- trW(W) str(tr_489) #&gt; num [1:30] 0 90.8 29.4 42.1 26.5 ... #&gt; - attr(*, &quot;timings&quot;)= Named num [1:2] 0.17 0.186 #&gt; ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;user.self&quot; &quot;elapsed&quot; #&gt; - attr(*, &quot;type&quot;)= chr &quot;mult&quot; #&gt; - attr(*, &quot;n&quot;)= int 489 In this case, the spatial process in the response is not strong, so the global indirect impacts (here for the air pollution variable) are weak. SLM_489_imp &lt;- impacts(SLM_489, tr=tr_489, R=2000) SLM_489_imp_sum &lt;- summary(SLM_489_imp, short=TRUE, zstats=TRUE) res &lt;- rbind(Impacts=sapply(SLM_489_imp$res, &quot;[&quot;, 5), SE=SLM_489_imp_sum$semat[5,]) colnames(res) &lt;- c(&quot;Direct&quot;, &quot;Indirect&quot;, &quot;Total&quot;) res #&gt; Direct Indirect Total #&gt; Impacts -0.00593 -1.01e-05 -0.00594 #&gt; SE 0.00105 1.01e-04 0.00105 Of more interest is trying to reconstruct the direct and total impacts using dense matrix methods; the direct global impacts are the mean of the diagonal of the dense impacts matrix, and the total global impacts are the sum of all matrix elements divided by the number of observations. The direct impacts agree, but the total impacts differ slightly. coef_SLM_489 &lt;- coef(SLM_489) IrW &lt;- Diagonal(489) - coef_SLM_489[1] * W S_W &lt;- solve(IrW) S_NOX_W &lt;- S_W %*% (diag(489) * coef_SLM_489[7]) c(Direct=mean(diag(S_NOX_W)), Total=sum(S_NOX_W)/489) #&gt; Direct Total #&gt; -0.00593 -0.00594 This bare-bones approach corresponds to using the listw= argument, and as expected gives the same output. sapply(impacts(SLM_489, listw=lw_q_489), &quot;[&quot;, 5) #&gt; direct indirect total #&gt; -5.93e-03 -1.01e-05 -5.94e-03 The experimental evalues= approach which is known to be numerically exact by definition gives the same results as the matrix power series trace approach, so the sight difference may be attributed to the consequences of inverting the spatial process matrix. sapply(impacts(SLM_489, evalues=eigs_489), &quot;[&quot;, 5) #&gt; direct indirect total #&gt; -5.93e-03 -1.01e-05 -5.94e-03 Impacts are crucial to the interpretation of Durbin models including spatially lagged covariates and models including the spatially lagged response. Tools to calculate impacts and their inferential bases are now available, and should be employed, but as yet some implementation details are under development and ways of presenting results in tabular form have not reached maturity. 18.3.3 Predictions We’ll use a predict() method for &quot;sarlm&quot; objects to double-check impacts, here for the pupil-teacher ratio (PTRATIO). The method was re-written by Martin Gubri based on Goulard, Laurent and Thomas-Agnan (2017). The pred.type= argument specifies the prediction strategy among those presented in the article. args(predict.sarlm) #&gt; function (object, newdata = NULL, listw = NULL, pred.type = &quot;TS&quot;, #&gt; all.data = FALSE, zero.policy = NULL, legacy = TRUE, legacy.mixed = FALSE, #&gt; power = NULL, order = 250, tol = .Machine$double.eps^(3/5), #&gt; spChk = NULL, ...) #&gt; NULL First we’ll increment PTRATIO by one to show that, using least squares, the mean difference between predictions from the incremented new data and fitted values is equal to the regression coefficient. nd_489 &lt;- boston_489 nd_489$PTRATIO &lt;- nd_489$PTRATIO + 1 OLS_489 &lt;- lm(form, data=boston_489) fitted &lt;- predict(OLS_489) nd_fitted &lt;- predict(OLS_489, newdata=nd_489) all.equal(unname(coef(OLS_489)[12]), mean(nd_fitted - fitted)) #&gt; [1] TRUE In models including the spatially lagged response, and when the spatial coefficient in different from zero, this is not the case in general, and is why we need impacts() methods. The difference here is not great, but neither is it zero, and needs to be handled. fitted &lt;- predict(SLM_489) nd_fitted &lt;- predict(SLM_489, newdata=nd_489, listw=lw_q_489, pred.type=&quot;TS&quot;, zero.policy=TRUE) all.equal(unname(coef_SLM_489[13]), mean(nd_fitted - fitted)) #&gt; [1] &quot;Mean relative difference: 0.00178&quot; In the Boston tracts data set, 17 observations of median house values, the response, are censored. Using these as an example and comparing some pred.type= variants for the SDEM model and predicting out-of-sample, we can see that there are differences, suggesting that this is a fruitful area for study. There have been a number of alternative proposals for handling missing variables (Gómez-Rubio, Bivand, and Rue 2015; Suesse 2018). Another reason for increasing attention on prediction is that it is fundamental for machine learning approaches, in which prediction for validation and test data sets drives model specification choice. The choice of training and other data sets with dependent spatial data remains an open question, and is certainly not as simple as with independent data. Here, we’ll list the predictions for the censored tract observations using three different prediction types, taking the exponent to get back to the USD median house values. nd &lt;- boston_506[is.na(boston_506$median),] t0 &lt;- exp(predict(SDEM_489, newdata=nd, listw=lw_q, pred.type=&quot;TS&quot;, zero.policy=TRUE)) suppressWarnings(t1 &lt;- exp(predict(SDEM_489, newdata=nd, listw=lw_q, pred.type=&quot;KP2&quot;, zero.policy=TRUE))) suppressWarnings(t2 &lt;- exp(predict(SDEM_489, newdata=nd, listw=lw_q, pred.type=&quot;KP5&quot;, zero.policy=TRUE))) data.frame(fit_TS=t0[,1], fit_KP2=c(t1), fit_KP5=c(t2), censored=boston_506$censored[as.integer(attr(t0, &quot;region.id&quot;))]) #&gt; fit_TS fit_KP2 fit_KP5 censored #&gt; 13 23912 29477 28147 right #&gt; 14 28126 27001 28516 right #&gt; 15 30553 36184 32476 right #&gt; 17 18518 19621 18878 right #&gt; 43 9564 6817 7561 left #&gt; 50 8371 7196 7383 left #&gt; 312 51477 53301 54173 right #&gt; 313 45921 45823 47095 right #&gt; 314 44196 44586 45361 right #&gt; 317 43427 45707 45442 right #&gt; 337 39879 42072 41127 right #&gt; 346 44708 46694 46108 right #&gt; 355 48188 49068 48911 right #&gt; 376 42881 45883 44966 right #&gt; 408 44294 44615 45670 right #&gt; 418 38211 43375 41914 right #&gt; 434 41647 41690 42398 right 18.4 Markov random field and multilevel models with spatial weights There is a large literature in spatial epidemiology using CAR and ICAR models in spatially structured random effects. These extend to multilevel models, in which the spatially structured random effects may apply at different levels of the model (Bivand et al. 2017). In order to try out some of the variants, we need to remove the no-neighbour observations from the tract level, and from the model output zone aggregated level, in two steps as reducing the tract level induces a no-neighbour outcome at the model output zone level. boston_94a &lt;- st_cast(aggregate(boston_489[,&quot;NOX_ID&quot;], list(boston_489$NOX_ID), unique), &quot;MULTIPOLYGON&quot;) nb_q_94a &lt;- spdep::poly2nb(boston_94a) NOX_ID_no_neighs &lt;- boston_94a$NOX_ID[which(spdep::card(nb_q_94a) == 0)] boston_487 &lt;- boston_489[is.na(match(boston_489$NOX_ID, NOX_ID_no_neighs)),] boston_93 &lt;- st_cast(aggregate(boston_487[, &quot;NOX_ID&quot;], list(ids = boston_487$NOX_ID), unique), &quot;MULTIPOLYGON&quot;) row.names(boston_93) &lt;- as.character(boston_93$NOX_ID) nb_q_93 &lt;- spdep::poly2nb(boston_93, row.names=unique(as.character(boston_93$NOX_ID))) The lme4 package lets us add an IID unstructured random effect at the model output zone level: library(lme4) MLM &lt;- lmer(update(form, . ~ . + (1 | NOX_ID)), data=boston_487, REML=FALSE) copying the random effect into the &quot;sf&quot; object for mapping below. boston_93$MLM_re &lt;- ranef(MLM)[[1]][,1] Two packages, hglm and HSAR, offer SAR upper level spatially structured random effects, and require the specification of a sparse matrix mapping the upper level enities onto lower level entities, and sparse binary weights matrices: library(Matrix) suppressMessages(library(MatrixModels)) Delta &lt;- as(model.Matrix(~ -1 + as.factor(NOX_ID), data=boston_487, sparse=TRUE), &quot;dgCMatrix&quot;) M &lt;- as(spdep::nb2listw(nb_q_93, style=&quot;B&quot;), &quot;CsparseMatrix&quot;) The extension of hglm to sparse spatial setting extended its facilities (Alam, Rönnegård, and Shen 2015), and also permits the modelling of discrete responses. First we fit an IID random effect: suppressPackageStartupMessages(library(hglm)) y_hglm &lt;- log(boston_487$median) X_hglm &lt;- model.matrix(lm(form, data=boston_487)) suppressWarnings(HGLM_iid &lt;- hglm(y=y_hglm, X=X_hglm, Z=Delta)) followed by a SAR model at the upper level (corresponding to a spatial error (SEM) model), which reports the spatially structured random effect without fully converging, so coefficient standard errors are not available: suppressWarnings(HGLM_sar &lt;- hglm(y=y_hglm, X=X_hglm, Z=Delta, rand.family=SAR(D=M))) boston_93$HGLM_re &lt;- unname(HGLM_iid$ranef) boston_93$HGLM_ss &lt;- HGLM_sar$ranef[,1] The HSAR package is restricted to the Gaussian response case, and fits an upper level SEM using MCMC; if W= is a lower level weights matrix, it will also fit a lower level SLM (Dong and Harris 2015; Dong et al. 2015): library(HSAR) suppressWarnings(HSAR &lt;- hsar(form, data=boston_487, W=NULL, M=M, Delta=Delta, burnin=500, Nsim=2500, thinning=1)) boston_93$HSAR_ss &lt;- HSAR$Mus[1,] The R2BayesX package provides flexible support for structured additive regression models, including spatial multilevel models. The models include an IID unstructured random effect at the upper level using the &quot;re&quot; specification (Umlauf et al. 2015); we choose the &quot;MCMC&quot;method: suppressPackageStartupMessages(library(R2BayesX)) BX_iid &lt;- bayesx(update(form, . ~ . + sx(NOX_ID, bs=&quot;re&quot;)), family=&quot;gaussian&quot;, data=boston_487, method=&quot;MCMC&quot;, iterations=12000, burnin=2000, step=2, seed=123) boston_93$BX_re &lt;- BX_iid$effects[&quot;sx(NOX_ID):re&quot;][[1]]$Mean and the &quot;mrf&quot; (Markov random field) spatially structured random effect specification based on a graph derived from converting a suitable &quot;nb&quot; object for the upper level. The &quot;region.id&quot; attribute of the &quot;nb&quot; object needs to contain values corresponding the the indexing variable. RBX_gra &lt;- nb2gra(nb_q_93) BX_mrf &lt;- bayesx(update(form, . ~ . + sx(NOX_ID, bs=&quot;mrf&quot;, map=RBX_gra)), family=&quot;gaussian&quot;, data=boston_487, method=&quot;MCMC&quot;, iterations=12000, burnin=2000,step=2, seed=123) boston_93$BX_ss &lt;- BX_mrf$effects[&quot;sx(NOX_ID):mrf&quot;][[1]]$Mean In a very similar way, mgcv::gam() can take an &quot;mrf&quot; term using a suitable &quot;nb&quot; object for the upper level. In this case the &quot;nb&quot; object needs to have the contents of the &quot;region.id&quot; attribute copied as the names of the neighbour list components, and the indexing variable needs to be a factor (Wood 2017) (the &quot;REML&quot; method of bayesx() gives the same result here): library(mgcv) names(nb_q_93) &lt;- attr(nb_q_93, &quot;region.id&quot;) boston_487$NOX_ID &lt;- as.factor(boston_487$NOX_ID) GAM_MRF &lt;- gam(update(form, . ~ . + s(NOX_ID, bs=&quot;mrf&quot;, xt=list(nb=nb_q_93))), data=boston_487, method=&quot;REML&quot;) boston_93$GAM_ss &lt;- aggregate(predict(GAM_MRF, type=&quot;terms&quot;, se=FALSE)[,14], list(boston_487$NOX_ID), mean)$x In the cases of hglm(), bayesx() and gam(), we could also model discrete responses without further major difficulty, and bayesx() and gam() also facilitate the generalization of functional form fitting for included covariates. res &lt;- rbind(iid_lmer=summary(MLM)$coefficients[6, 1:2], iid_hglm=summary(HGLM_iid)$FixCoefMat[6, 1:2], iid_BX=BX_iid$fixed.effects[6, 1:2], sar_hsar=c(HSAR$Mbetas[1, 6], HSAR$SDbetas[1, 6]), mrf_BX=BX_mrf$fixed.effects[6, 1:2], mrf_GAM=c(summary(GAM_MRF)$p.coeff[6], summary(GAM_MRF)$se[6])) Unfortunately, the coefficient estimates for the air pollution variable for these multilevel models are not helpful. All remain negative, but the inclusion of the model output zone level effects, be they IID or spatially structured, suggest that it hard to disentangle the influence of the scale of observation from that of covariates observed at that scale. suppressPackageStartupMessages(library(ggplot2)) df_res &lt;- as.data.frame(res) names(df_res) &lt;- c(&quot;mean&quot;, &quot;sd&quot;) limits &lt;- aes(ymax = mean + qnorm(0.975)*sd, ymin=mean + qnorm(0.025)*sd) df_res$model &lt;- row.names(df_res) p &lt;- ggplot(df_res, aes(y=mean, x=model)) + geom_point() + geom_errorbar(limits) + geom_hline(yintercept = 0, col=&quot;#EB811B&quot;) + coord_flip() p + ggtitle(&quot;NOX coefficients and error bars&quot;) + theme(plot.background = element_rect(fill = &quot;transparent&quot;,colour = NA), legend.background = element_rect(colour = NA, fill = &quot;transparent&quot;)) (#fig:multi-level_coefs)Polish municipality types 2015 This map shows that the model output zone level IID random effects are very similar across the three model fitting functions reported. library(tmap) tm_shape(boston_93) + tm_fill(c(&quot;MLM_re&quot;, &quot;HGLM_re&quot;, &quot;BX_re&quot;), midpoint=0, title=&quot;IID&quot;) + tm_facets(free.scales=FALSE) + tm_borders(lwd=0.3, alpha=0.4) + tm_layout(panel.labels=c(&quot;MLM&quot;, &quot;HGLM&quot;, &quot;BX&quot;)) (#fig:multi-level_maps_1)IID random effects The spatially structured SAR and MRF random effects (MRF term in the gam() case) are also very similar, with the MRF somewhat less smoothed than the SAR values. tm_shape(boston_93) + tm_fill(c(&quot;HGLM_ss&quot;, &quot;HSAR_ss&quot;, &quot;BX_ss&quot;, &quot;GAM_ss&quot;), midpoint=0, title=&quot;SS&quot;) + tm_facets(free.scales=FALSE) + tm_borders(lwd=0.3, alpha=0.4) + tm_layout(panel.labels=c(&quot;HGLM SAR&quot;, &quot;HSAR SAR&quot;, &quot;BX MRF&quot;, &quot;GAM MRF&quot;)) (#fig:multi-level_maps_2)Spatially structured random effects Although there is still a great need for more thorough comparative studies of model fitting functions for spatial regression, there has been much progress over recent years. References "],
["movement-data.html", "Chapter 19 Movement data", " Chapter 19 Movement data "],
["statistical-modelling-of-spatiotemporal-data.html", "Chapter 20 Statistical modelling of spatiotemporal data", " Chapter 20 Statistical modelling of spatiotemporal data Wikle/Zambione/Cressie: (Wikle, Zammit-Mangion, and Cressie 2019) Geostatistics: (Gräler, Pebesma, and Heuvelink 2016) scalable method comparison: (Heaton et al. 2018) Point patterns: (Stoyan et al. 2017) (Gabriel, Rowlingson, and Diggle 2013) Spatstat book: (Baddeley, Rubak, and Turner 2015) R-INLA: (Blangiardo et al. 2013), (Blangiardo and Cameletti 2015) Possibly: M. Cameletti: Stem: Spatio-temporal models in R Estimation of the parameters of a spatio-temporal model using the EM algorithm, estimation of the parameter standard errors using a spatio-temporal parametric bootstrap, spatial mapping. References "],
["sp-and-raster.html", "Chapter 21 sp and raster 21.1 links and differences between sf and sp 21.2 migration packages 21.3 raster, stars and sf", " Chapter 21 sp and raster 21.1 links and differences between sf and sp difference in data structures; how to convert; limitations of sf to sp conversion 21.2 migration packages link to sf wiki: https://github.com/r-spatial/sf/wiki/Migrating 21.3 raster, stars and sf map algebra; ABM; SDM; refer to Robert’s book on http://rspatial.org/ "],
["r-data-structures.html", "R data structures 21.4 Homogeneous vectors 21.5 Heterogeneous vectors: list 21.6 Attributes 21.7 various names attributes 21.8 using structure", " R data structures This chapter provides some minimal set of R basics that may make it easier to read this book. A more comprehensive book on R basics is given in (Wickham 2014a), chapter 2. As pointed out by (Chambers 2016), everything that exists in R is an object. This includes objects that make things happen, such as language objects or functions, but also the more basic “things”, such as data objects. 21.4 Homogeneous vectors Data objects contain data, and possibly metadata. Data is always in the form of a vector, which can have different type. We can find the type by typeof, and vector length by length. Vectors are created by c, which combines individual elements: typeof(1:10) #&gt; [1] &quot;integer&quot; length(1:10) #&gt; [1] 10 typeof(1.0) #&gt; [1] &quot;double&quot; length(1.0) #&gt; [1] 1 typeof(c(&quot;foo&quot;, &quot;bar&quot;)) #&gt; [1] &quot;character&quot; length(c(&quot;foo&quot;, &quot;bar&quot;)) #&gt; [1] 2 typeof(c(TRUE, FALSE)) #&gt; [1] &quot;logical&quot; Vectors of this kind can only have a single type. Note that vectors can have length zero, e.g. in, i = integer(0) typeof(i) #&gt; [1] &quot;integer&quot; i #&gt; integer(0) length(i) #&gt; [1] 0 We can retrieve (or in assignments: replace) elements in a vector using [ or [[: a = c(1,2,3) a[2] #&gt; [1] 2 a[[2]] #&gt; [1] 2 a[2:3] #&gt; [1] 2 3 a[2:3] = c(5,6) a #&gt; [1] 1 5 6 a[[3]] = 10 a #&gt; [1] 1 5 10 where the difference is that [ can operate on an index range (or multiple indexes), and [[ operates on a single vector value. 21.5 Heterogeneous vectors: list An additional vector type is the list, which can combine any types in its elements: l &lt;- list(3, TRUE, &quot;foo&quot;) typeof(l) #&gt; [1] &quot;list&quot; length(l) #&gt; [1] 3 For lists, there is a further distinction between [ and [[: the single [ returns always a list, and [[ returns the contents of a list element: l[1] #&gt; [[1]] #&gt; [1] 3 l[[1]] #&gt; [1] 3 For replacement, one case use [ when providing a list, and [[ when providing a new value: l[1:2] = list(4, FALSE) l #&gt; [[1]] #&gt; [1] 4 #&gt; #&gt; [[2]] #&gt; [1] FALSE #&gt; #&gt; [[3]] #&gt; [1] &quot;foo&quot; l[[3]] = &quot;bar&quot; l #&gt; [[1]] #&gt; [1] 4 #&gt; #&gt; [[2]] #&gt; [1] FALSE #&gt; #&gt; [[3]] #&gt; [1] &quot;bar&quot; In case list elements are named, as in l = list(first = 3, second = TRUE, third = &quot;foo&quot;) l #&gt; $first #&gt; [1] 3 #&gt; #&gt; $second #&gt; [1] TRUE #&gt; #&gt; $third #&gt; [1] &quot;foo&quot; we can use names as in l[[&quot;second&quot;]] and this can be abbreviated to l$second #&gt; [1] TRUE l$second = FALSE l #&gt; $first #&gt; [1] 3 #&gt; #&gt; $second #&gt; [1] FALSE #&gt; #&gt; $third #&gt; [1] &quot;foo&quot; This is convenient, but also requires name look-up in the names attribute (see below). 21.5.1 NULL and removing list elements NULL is the null value in R; it is special in the sense that it doesn’t work in simple comparisons: 3 == NULL # not FALSE! #&gt; logical(0) NULL == NULL # not even TRUE! #&gt; logical(0) but has to be treated specially, using is.null: is.null(NULL) #&gt; [1] TRUE When we want to remove one or more list elements, we can do so by creating a new list that does not contain the elements that needed removal, as in l = l[c(1,3)] # remove second, implicitly l #&gt; $first #&gt; [1] 3 #&gt; #&gt; $third #&gt; [1] &quot;foo&quot; but we can also assign NULL to the element we want to eliminate: l$second = NULL l #&gt; $first #&gt; [1] 3 #&gt; #&gt; $third #&gt; [1] &quot;foo&quot; 21.6 Attributes We can glue arbitrary metadata objects to data objects, as in a = 1:3 attr(a, &quot;some_meta_data&quot;) = &quot;foo&quot; a #&gt; [1] 1 2 3 #&gt; attr(,&quot;some_meta_data&quot;) #&gt; [1] &quot;foo&quot; and this can be retrieved, or replaced by attr(a, &quot;some_meta_data&quot;) #&gt; [1] &quot;foo&quot; attr(a, &quot;some_meta_data&quot;) = &quot;bar&quot; attr(a, &quot;some_meta_data&quot;) #&gt; [1] &quot;bar&quot; In essence, the attribute of an object is a named list, and we can get or set the complete list by attributes(a) #&gt; $some_meta_data #&gt; [1] &quot;bar&quot; attributes(a) = list(some_meta_data = &quot;foo&quot;) attributes(a) #&gt; $some_meta_data #&gt; [1] &quot;foo&quot; A number of attributes are treated specially by R, see e.g. ?attributes. 21.6.1 object class and class attribute Every object in R “has a class”, meaning that class(obj) returns a character vector with the class of obj. Some objects have an implicit class, e.g. vectors class(1:3) #&gt; [1] &quot;integer&quot; class(c(TRUE, FALSE)) #&gt; [1] &quot;logical&quot; class(c(&quot;TRUE&quot;, &quot;FALSE&quot;)) #&gt; [1] &quot;character&quot; but we can also set the class explicit, either by using attr or by using class in the left-hand side of an expression: a = 1:3 class(a) = &quot;foo&quot; a #&gt; [1] 1 2 3 #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;foo&quot; class(a) #&gt; [1] &quot;foo&quot; attributes(a) #&gt; $class #&gt; [1] &quot;foo&quot; in which case the newly set class overrides the earlier implicit class. This way, we can add methods for class foo, e.g. by print.foo = function(x, ...) print(paste(&quot;an object of class foo with length&quot;, length(x))) print(a) #&gt; [1] &quot;an object of class foo with length 3&quot; Providing such methods are generally intended to create more usable software, but at the same time they may make the objects more opaque. It is sometimes useful to see what an object “is made of” by printing it after the class attribute is removed, as in unclass(a) #&gt; [1] 1 2 3 As a more elaborate example, consider the case where a polygon is made using package sf: library(sf) p = st_polygon(list(rbind(c(0,0), c(1,0), c(1,1), c(0,0)))) p #&gt; POLYGON ((0 0, 1 0, 1 1, 0 0)) which prints the well-known-text form; to understand what the data structure is like, we can use unclass(p) #&gt; [[1]] #&gt; [,1] [,2] #&gt; [1,] 0 0 #&gt; [2,] 1 0 #&gt; [3,] 1 1 #&gt; [4,] 0 0 21.6.2 the dim attribute The dim attribute sets the matrix or array dimensions: a = 1:8 class(a) #&gt; [1] &quot;integer&quot; attr(a, &quot;dim&quot;) = c(2,4) # or: dim(a) = c(2,4) class(a) #&gt; [1] &quot;matrix&quot; a #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 1 3 5 7 #&gt; [2,] 2 4 6 8 attr(a, &quot;dim&quot;) = c(2,2,2) # or: dim(a) = c(2,2,2) class(a) #&gt; [1] &quot;array&quot; a #&gt; , , 1 #&gt; #&gt; [,1] [,2] #&gt; [1,] 1 3 #&gt; [2,] 2 4 #&gt; #&gt; , , 2 #&gt; #&gt; [,1] [,2] #&gt; [1,] 5 7 #&gt; [2,] 6 8 21.7 various names attributes Named vectors carry their names in a names attribute. We saw examples for lists above, an example for a numeric vector is: a = c(first = 3, second = 4, last = 5) a[&quot;second&quot;] #&gt; second #&gt; 4 attributes(a) #&gt; $names #&gt; [1] &quot;first&quot; &quot;second&quot; &quot;last&quot; More name attributes are e.g. dimnames of matrices or arrays: a = matrix(1:4, 2, 2) dimnames(a) = list(rows = c(&quot;row1&quot;, &quot;row2&quot;), cols = c(&quot;col1&quot;, &quot;col2&quot;)) a #&gt; cols #&gt; rows col1 col2 #&gt; row1 1 3 #&gt; row2 2 4 attributes(a) #&gt; $dim #&gt; [1] 2 2 #&gt; #&gt; $dimnames #&gt; $dimnames$rows #&gt; [1] &quot;row1&quot; &quot;row2&quot; #&gt; #&gt; $dimnames$cols #&gt; [1] &quot;col1&quot; &quot;col2&quot; Data.frame objects have rows and columns, and each have names: df = data.frame(a = 1:3, b = c(TRUE, FALSE, TRUE)) attributes(df) #&gt; $names #&gt; [1] &quot;a&quot; &quot;b&quot; #&gt; #&gt; $class #&gt; [1] &quot;data.frame&quot; #&gt; #&gt; $row.names #&gt; [1] 1 2 3 21.8 using structure When programming, the pattern of adding or modifying attributes before returning an object is extremely common, an example being: f = function(x) { a = create_obj(x) # call some other function attributes(a) = list(class = &quot;foo&quot;, meta = 33) a } The last two statements can be contracted in f = function(x) { a = create_obj(x) # call some other function structure(a, class = &quot;foo&quot;, meta = 33) } where function structure adds, replaces, or (in case of value NULL) removes attributes from the object in its first argument. References "],
["references.html", "References", " References "]
]
