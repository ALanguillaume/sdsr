[
["index.html", "Spatial Data Science with R Preface", " Spatial Data Science with R Edzer Pebesma, Roger Bivand 2018-03-16 Preface "],
["intro.html", "Chapter 1 Getting Started 1.1 A first map 1.2 Reading and writing 1.3 Exercises", " Chapter 1 Getting Started This chapter gives a quick start to get you going with spatial data science with R. It is easier to read when understanding R at the level of, say, R for Data Science (Wickham and Grolemund 2017). 1.1 A first map There is a lot to say about spatial data, but let us first create a map. We can create a simple map by: library(tidyverse) library(sf) system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;) %&gt;% read_sf() %&gt;% st_transform(32119) %&gt;% select(BIR74) %&gt;% plot(graticule = TRUE, axes = TRUE) Figure 1.1: a first map A lot went on, here. We will describe the steps in detail. First, we loaded two R packages: library(tidyverse) library(sf) where tidyverse is needed for the tidyverse functions and methods, and sf is needed for the spatial commands and spatial tidyverse methods. Package sf implements simple features, a standardised way to encode polygon data (points, lines, polygons). We will say more about simple features in chapter 3. Most commands in package sf start with st_, short for spatiotemporal, a convention it shares with e.g. PostGIS. The %&gt;% (pipe) symbols should be read as then: we read a %&gt;% b() %&gt;% c() %&gt;% d(n = 10) as a then b then c then d, and that is just alternative syntax for d(c(b(a)), n = 10) or tmp1 &lt;- b(a) tmp2 &lt;- c(tmp1) tmp3 &lt;- d(tmp2, n = 10) The pipe form is easier to read because we don’t have to go from right to left, and avoids the need to choose names for intermediate results. For the illustration we picked a data file that comes with sf, the location of which depends on the operating system used: (file &lt;- system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;)) #&gt; [1] &quot;/home/edzer/R/x86_64-pc-linux-gnu-library/3.4/sf/gpkg/nc.gpkg&quot; Never use system.file if you want to read your own data; in that case, fname should be the data source (typically file) name (section 1.2). (Parens around this expression are used to have the result not only stored, but also printed.) Then, we read this file into R using read_sf: (nc &lt;- read_sf(file)) #&gt; Simple feature collection with 100 features and 14 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -84.3 ymin: 33.9 xmax: -75.5 ymax: 36.6 #&gt; epsg (SRID): 4267 #&gt; proj4string: +proj=longlat +datum=NAD27 +no_defs #&gt; # A tibble: 100 x 15 #&gt; AREA PERIMETER CNTY_ CNTY_ID NAME FIPS FIPSNO CRESS_ID BIR74 SID74 #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.114 1.44 1825 1825 Ashe 37009 37009 5 1091 1.00 #&gt; 2 0.0610 1.23 1827 1827 Allegh… 37005 37005 3 487 0 #&gt; 3 0.143 1.63 1828 1828 Surry 37171 37171 86 3188 5.00 #&gt; 4 0.0700 2.97 1831 1831 Currit… 37053 37053 27 508 1.00 #&gt; 5 0.153 2.21 1832 1832 Northa… 37131 37131 66 1421 9.00 #&gt; 6 0.0970 1.67 1833 1833 Hertfo… 37091 37091 46 1452 7.00 #&gt; # ... with 94 more rows, and 5 more variables: NWBIR74 &lt;dbl&gt;, BIR79 &lt;dbl&gt;, #&gt; # SID79 &lt;dbl&gt;, NWBIR79 &lt;dbl&gt;, geom &lt;MULTIPOLYGON [°]&gt; which creates a “spatial tibble”: class(nc) #&gt; [1] &quot;sf&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; This object is transformed into a new coordinate reference system (North Carolina State Plane, with EPSG code 32119): (nc.32119 &lt;- st_transform(nc, 32119)) #&gt; Simple feature collection with 100 features and 14 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: 124000 ymin: 14700 xmax: 931000 ymax: 318000 #&gt; epsg (SRID): 32119 #&gt; proj4string: +proj=lcc +lat_1=36.16666666666666 +lat_2=34.33333333333334 +lat_0=33.75 +lon_0=-79 +x_0=609601.22 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs #&gt; # A tibble: 100 x 15 #&gt; AREA PERIMETER CNTY_ CNTY_ID NAME FIPS FIPSNO CRESS_ID BIR74 SID74 #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.114 1.44 1825 1825 Ashe 37009 37009 5 1091 1.00 #&gt; 2 0.0610 1.23 1827 1827 Allegh… 37005 37005 3 487 0 #&gt; 3 0.143 1.63 1828 1828 Surry 37171 37171 86 3188 5.00 #&gt; 4 0.0700 2.97 1831 1831 Currit… 37053 37053 27 508 1.00 #&gt; 5 0.153 2.21 1832 1832 Northa… 37131 37131 66 1421 9.00 #&gt; 6 0.0970 1.67 1833 1833 Hertfo… 37091 37091 46 1452 7.00 #&gt; # ... with 94 more rows, and 5 more variables: NWBIR74 &lt;dbl&gt;, BIR79 &lt;dbl&gt;, #&gt; # SID79 &lt;dbl&gt;, NWBIR79 &lt;dbl&gt;, geom &lt;MULTIPOLYGON [m]&gt; and a single attribute column is selected (nc.32119.bir74 &lt;- select(nc.32119, BIR74)) #&gt; Simple feature collection with 100 features and 1 field #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: 124000 ymin: 14700 xmax: 931000 ymax: 318000 #&gt; epsg (SRID): 32119 #&gt; proj4string: +proj=lcc +lat_1=36.16666666666666 +lat_2=34.33333333333334 +lat_0=33.75 +lon_0=-79 +x_0=609601.22 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs #&gt; # A tibble: 100 x 2 #&gt; BIR74 geom #&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [m]&gt; #&gt; 1 1091 (((387345 278387, 381334 282774, 379438 282943, 373250 290553, 36… #&gt; 2 487 (((408602 292425, 408565 293985, 406643 296873, 406420 3e+05, 402… #&gt; 3 3188 (((478717 277490, 476936 278867, 471503 279173, 470806 281394, 46… #&gt; 4 508 (((878194 289128, 877381 291117, 875994 290881, 874941 292805, 87… #&gt; 5 1421 (((769835 277796, 768364 274842, 762616 274401, 763168 269009, 76… #&gt; 6 1452 (((812328 277876, 791158 277012, 789882 277579, 777724 277107, 76… #&gt; # ... with 94 more rows Finally, the result is plotted, with the command: plot(nc.32119.bir74, graticule = TRUE, axes = TRUE) as shown in figure 1.1. Where do these commands come from? library and system.file are base R. We can ask for help about a particular command by entering e.g. ?library The command read_sf is an alternative to the st_read, which returns a spatial tibble instead of a spatial data frame, and will be discussed in section 1.2. The st_transform method is used here to convert from the geographic coordinates (degrees longitude and latitude) into “flat” coordinates, meaning \\(x\\) and \\(y\\) coordinates in a planar system. It will be discussed in section 7.2. The plot method for sf objects chooses default colors and legends settings; we instructed it to add a graticule (the grey lines of equal longitude and latitude) and degree labels along the axes. It is described in chapter 9. As witnessed by the plot, the plot command receives county polygons as well as BIR74 values for each polygon. How is it possible that we select only the BIR74 variable, but still can plot the polygons? This is because package sf provides a select method: methods(select) #&gt; [1] select.data.frame* select.default* select.grouped_df* #&gt; [4] select.list select.sf select.tbl_cube* #&gt; see &#39;?methods&#39; for accessing help and source code and this method (select.sf) makes the geometry (geom) sticky: nc %&gt;% select(BIR74) %&gt;% names() #&gt; [1] &quot;BIR74&quot; &quot;geom&quot; We get the “normal” select behaviour if we first coerce to a normal tibble: nc %&gt;% as_tibble(validate = TRUE) %&gt;% select(BIR74) %&gt;% names() #&gt; [1] &quot;BIR74&quot; A ggplot is created when we use geom_sf: ggplot() + geom_sf(data = nc.32119) + aes(fill = BIR74) + theme(panel.grid.major = element_line(color = &quot;white&quot;)) + scale_fill_gradientn(colors = sf.colors(20)) and a facet plot for a pair of columns in nc.32119 is obtained by gathering the columns: nc2 &lt;- nc.32119 %&gt;% select(SID74, SID79) %&gt;% gather(VAR, SID, -geom) ggplot() + geom_sf(data = nc2, aes(fill = SID)) + facet_wrap(~VAR, ncol = 1) + scale_y_continuous(breaks = 34:36) + scale_fill_gradientn(colors = sf.colors(20)) + theme(panel.grid.major = element_line(color = &quot;white&quot;)) An interactive, leaflet-type map is obtained by suppressPackageStartupMessages(library(mapview)) nc.32119 %&gt;% mapview(zcol = &quot;BIR74&quot;, legend = TRUE, col.regions = sf.colors) 1.2 Reading and writing Typical R data science tasks start with reading data from an external source; this may be a file, or a set of files like a “shapefile”, or a database, or a web service. Package sf can read from a large number of different data sources; the following command shows how many nrow(st_drivers(&quot;vector&quot;)) # vector drivers #&gt; [1] 80 nrow(st_drivers(&quot;raster&quot;)) # raster drivers #&gt; [1] 136 (the output you see may differ because of different operating system and configuration.) 1.2.1 GDAL st_drivers lists the drivers available to GDAL, the geospatial data abstraction library. This library can be seen as the swiss army knive of spatial data; besides for R it is used in Python, QGIS, PostGIS, and more than 100 other software projects. The dependency of sf on other R libraries and system libraries is shown in figure 1.2. Figure 1.2: sf and its dependencies Note that the C/C++ libraries used (GDAL, GEOS, Proj, liblwgeom, udunits2) are all developed, maintained and used by (data) science communities that are much larger than the R community. By using these libraries, we share how we understand what we are doing with all the other communities that use these libraries. This is not only important for resolving problems, but also for establishing which findings are facts. GDAL is a “library of libraries” – in order to read all these data sources it needs a large number of other libraries. It typically links to over 100 other libraries. Binary packages distributed by CRAN contain only statically linked code: CRAN does not want to make any assumptions about presence of third-party libraries on the host system. As a consequence, when the binary sf package is installed from CRAN, it includes a copy of all the required external libraries as well as their dependencies, which may amount to 50 or 100 Mb. 1.2.2 st_read or read_sf? The function to read vector data is st_read. Function read_sf is largely the same as `st_read, but chooses a few tidyverse-style defaults: it is silent by default, where st_read gives a short report it returns a spatial tibble instead of a spatial data frame it sets as default stringsAsFactors = FALSE, where st_read listens to the global option default.stringsAsFactors() (which is TRUE by default) In the same fashion, compared to st_write write_sf, is also silent overwrites layers (i.e., sets delete_layer = TRUE) by default, which st_write does not do. (TBD: reading and writing raster data) 1.2.3 Getting help 1.2.4 Reading from files, and shapefiles We saw above that a spatial dataset can be read from a single file by nc &lt;- system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;) %&gt;% read_sf() In some cases, spatial datasets are contained in multiple files, e.g. in the case of shapefiles. A “shapefile” should be really understood as a set of files with a common prefix, or even a directory with several of such sets. Package sf comes with a couple of shapefiles packaged, a directory listing of the shape directory in the packge is obtained by list.files(system.file(&quot;shape/&quot;, package = &quot;sf&quot;)) #&gt; [1] &quot;nc.dbf&quot; &quot;nc.prj&quot; #&gt; [3] &quot;nc.shp&quot; &quot;nc.shx&quot; #&gt; [5] &quot;storms_xyz_feature.dbf&quot; &quot;storms_xyz_feature.shp&quot; #&gt; [7] &quot;storms_xyz_feature.shx&quot; &quot;storms_xyz.dbf&quot; #&gt; [9] &quot;storms_xyz.shp&quot; &quot;storms_xyz.shx&quot; #&gt; [11] &quot;storms_xyzm_feature.dbf&quot; &quot;storms_xyzm_feature.shp&quot; #&gt; [13] &quot;storms_xyzm_feature.shx&quot; &quot;storms_xyzm.dbf&quot; #&gt; [15] &quot;storms_xyzm.shp&quot; &quot;storms_xyzm.shx&quot; We can read a single shapefile by nc &lt;- system.file(&quot;shape/nc.shp&quot;, package=&quot;sf&quot;) %&gt;% read_sf() and it is important to know that in that case all four files starting with nc are read from this directory. We can also read the directory with shapfiles by nc &lt;- system.file(&quot;shape&quot;, package=&quot;sf&quot;) %&gt;% read_sf() #&gt; Warning in evalq((function (..., call. = TRUE, immediate. = FALSE, #&gt; noBreaks. = FALSE, : automatically selected the first layer in a data #&gt; source containing more than one. but we see some warnings now, indicating that we are reading a single layer from a multi-layer dataset. Indeed, this directory contains multiple layers, which can be queried by system.file(&quot;shape&quot;, package=&quot;sf&quot;) %&gt;% st_layers() #&gt; Driver: ESRI Shapefile #&gt; Available layers: #&gt; layer_name geometry_type features fields #&gt; 1 storms_xyzm_feature Measured Line String 71 1 #&gt; 2 storms_xyz_feature 3D Line String 71 1 #&gt; 3 storms_xyzm Measured Line String 71 0 #&gt; 4 storms_xyz 3D Line String 71 0 #&gt; 5 nc Polygon 100 14 From this list, we could pick one, and use it as the layer argument, as in dataset &lt;- system.file(&quot;shape&quot;, package=&quot;sf&quot;) layer &lt;- &quot;nc&quot; nc &lt;- read_sf(dataset, layer) which is essentially a convoluted way of what we did before to read nc.shp. Considering shapefiles in directories as layers in a dataset is not something that sf came up with, but is the way GDAL handles this. Although it is a good idea in general to give up using shapefiles, we cannot always control the format of the spatial data we get to start with. nc &lt;- system.file(&quot;shape&quot;, package=&quot;sf&quot;) %&gt;% read_sf() #&gt; Warning in evalq((function (..., call. = TRUE, immediate. = FALSE, #&gt; noBreaks. = FALSE, : automatically selected the first layer in a data #&gt; source containing more than one. 1.2.5 Reading from a text string In the special case of a GeoJSON (Butler et al. 2016) dataset, when the dataset is contained in a length-one character vector, it can be directly passed to read_sf and read from memory: str &lt;- &#39;{ &quot;type&quot;: &quot;FeatureCollection&quot;, &quot;features&quot;: [ { &quot;type&quot;: &quot;Feature&quot;, &quot;geometry&quot;: { &quot;type&quot;: &quot;Point&quot;, &quot;coordinates&quot;: [102.0, 0.5] }, &quot;properties&quot;: { &quot;prop0&quot;: &quot;value0&quot; } }, { &quot;type&quot;: &quot;Feature&quot;, &quot;geometry&quot;: { &quot;type&quot;: &quot;LineString&quot;, &quot;coordinates&quot;: [ [102.0, 0.0], [103.0, 1.0], [104.0, 0.0], [105.0, 1.0] ] }, &quot;properties&quot;: { &quot;prop0&quot;: &quot;value0&quot;, &quot;prop1&quot;: 0.0 } }, { &quot;type&quot;: &quot;Feature&quot;, &quot;geometry&quot;: { &quot;type&quot;: &quot;Polygon&quot;, &quot;coordinates&quot;: [ [ [100.0, 0.0], [101.0, 0.0], [101.0, 1.0], [100.0, 1.0], [100.0, 0.0] ] ] }, &quot;properties&quot;: { &quot;prop0&quot;: &quot;value0&quot;, &quot;prop1&quot;: { &quot;this&quot;: &quot;that&quot; } } } ] }&#39; (sf_obj &lt;- read_sf(str)) #&gt; Simple feature collection with 3 features and 2 fields #&gt; geometry type: GEOMETRY #&gt; dimension: XY #&gt; bbox: xmin: 100 ymin: 0 xmax: 105 ymax: 1 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; # A tibble: 3 x 3 #&gt; prop0 prop1 geometry #&gt; &lt;chr&gt; &lt;chr&gt; &lt;GEOMETRY [°]&gt; #&gt; 1 value0 &lt;NA&gt; POINT (102 0.5) #&gt; 2 value0 0.0 LINESTRING (102 0, 103 1, 104 0, 105 1) #&gt; 3 value0 &quot;{ \\&quot;this\\&quot;: \\&quot;that\\&quot; }&quot; POLYGON ((100 0, 101 0, 101 1, 100 1, 1… 1.2.6 Database reading and writing databases 1.3 Exercises Read the shapefile storms_xyz_feature from the shape directory in the sf package Copy this file to another directory on your computer, and read it from there (note: a shapefile consists of more than one file!) How many features does this dataset contain? Plot the dataset, with axes = TRUE (hint: before plotting, pipe through st_zm to drop Z and M coordinates; more about this in chapter 3). Before plotting, pipe the dataset through st_set_crs(4326). What is different in the plot obtained? References "],
["spaces-1-2-and-3-dimensional-spherical-time-bounded-spaces.html", "Chapter 2 Spaces: 1, 2 and 3-dimensional, spherical, time, bounded spaces", " Chapter 2 Spaces: 1, 2 and 3-dimensional, spherical, time, bounded spaces discusses spaces bounded: water, rivers, road networks "],
["geometries.html", "Chapter 3 Geometries", " Chapter 3 Geometries vertices, edges, lines, polygons, simple features; raster cells "],
["vectorraster.html", "Chapter 4 Vector/Raster", " Chapter 4 Vector/Raster differences, correspondence; properties of rasters; arrays "],
["geometric-manipulations.html", "Chapter 5 Geometric Manipulations", " Chapter 5 Geometric Manipulations discuss "],
["attributes.html", "Chapter 6 Attributes", " Chapter 6 Attributes discuss "],
["reference-systems.html", "Chapter 7 Reference Systems 7.1 Units of measurement 7.2 Coordinate transformation and conversion", " Chapter 7 Reference Systems Units of measure, reference systems, coordinate transformation and conversion 7.1 Units of measurement 7.2 Coordinate transformation and conversion "],
["plotting-spatial-data.html", "Chapter 8 Plotting spatial data", " Chapter 8 Plotting spatial data Plotting of lines, symbols, polygons (choroplets; overlapping polygons), rasters using color "],
["plot.html", "Chapter 9 Base Plot", " Chapter 9 Base Plot "],
["ggplot2.html", "Chapter 10 ggplot2", " Chapter 10 ggplot2 geom_sf examples; useful annotations and manipulations "],
["interactive-maps.html", "Chapter 11 Interactive Maps", " Chapter 11 Interactive Maps base plot: identify, locator leaflet, tmap, mapview mapedit? "],
["summarizing-geometries.html", "Chapter 12 Summarizing Geometries", " Chapter 12 Summarizing Geometries Properties: dimension, length, area, etc, if not earlier in Ch 3? counts, density, intensity (units; meaningful) "],
["point-pattern-analysis.html", "Chapter 13 Point Pattern Analysis", " Chapter 13 Point Pattern Analysis Basics PP, beyond counting; basic steps in PPA sf - spatstat interface; rasters; "],
["manipulating-attributes-summarise-aggregate-union-sample.html", "Chapter 14 Manipulating attributes: summarise, aggregate, union, sample", " Chapter 14 Manipulating attributes: summarise, aggregate, union, sample "],
["units-of-measure-revisited-attribute-units-intensive-and-extensive-variables.html", "Chapter 15 Units of measure revisited: attribute units, intensive and extensive variables", " Chapter 15 Units of measure revisited: attribute units, intensive and extensive variables "],
["up-and-downscaling.html", "Chapter 16 Up- and Downscaling", " Chapter 16 Up- and Downscaling sampling largest sub-geometry area-weighted interpolation "],
["spatial-interpolation-and-geostatistics.html", "Chapter 17 Spatial Interpolation and geostatistics", " Chapter 17 Spatial Interpolation and geostatistics intro ; variograms; gstat (needs to be sf-ed) "],
["area-data-and-spatial-correlation.html", "Chapter 18 Area Data and Spatial Correlation", " Chapter 18 Area Data and Spatial Correlation spdep stuff "],
["spatial-regression-and-autocorrelation.html", "Chapter 19 Spatial Regression and Autocorrelation", " Chapter 19 Spatial Regression and Autocorrelation intro; "],
["raster-modelling.html", "Chapter 20 Raster Modelling", " Chapter 20 Raster Modelling map algebra; ABM; SDM; Robert’s book. "],
["array-data-raster-and-vector-data-cubes.html", "Chapter 21 Array data: raster and vector data cubes", " Chapter 21 Array data: raster and vector data cubes "],
["movement-data.html", "Chapter 22 Movement data", " Chapter 22 Movement data "],
["statistical-modelling-of-spatiotemporal-data.html", "Chapter 23 Statistical modelling of spatiotemporal data", " Chapter 23 Statistical modelling of spatiotemporal data "],
["scalability.html", "Chapter 24 Scalability", " Chapter 24 Scalability "],
["references.html", "References", " References "]
]
