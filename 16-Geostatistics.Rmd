# Multivariate and Spatiotemporal Geostatistics {#stgeostatistics}

```{r load16, eval = Sys.getenv("USER") == "edzer", echo=FALSE}
# try(load("data/ch16.rda"))
```

Building on the simple interpolation methods presented in chapter
\@ref(interpolation), this chapter works out a case study for
spatiotemporal interpolation, using NO$_2$ air quality data, and
populations density as covariate.

## Preparing the air quality dataset

The dataset we work with is an air quality dataset obtained from
the European Environmental Agency (EEA). European member states report
air quality measurements to this Agency. So-called _validated_
data are quality controlled by member states, and are reported on a
yearly basis. They form the basis for policy compliancy evaluations.

The EEA's [air quality
e-reporting](https://www.eea.europa.eu/data-and-maps/data/aqereporting-8)
website gives access to the data reported by European member states.
We decided to download hourly (time series) data, which is the data primarily measured.
A web form helps convert simple selection criteria into an http GET request. The following URL
```
https://fme.discomap.eea.europa.eu/fmedatastreaming/AirQualityDownload/AQData_Extract.fmw?CountryCode=DE&CityName=&Pollutant=8&Year_from=2017&Year_to=2017&Station=&Samplingpoint=&Source=E1a&Output=TEXT&UpdateDate=
```
was created to select all validated (`Source=E1a`) $NO_2$
(`Pollutant=8`) data for 2017 (`Year_from`, `Year_to`) from Germany
(`CountryCode=DE`).  It returns a text file with a set of URLs to CSV
files, each containing the hourly values for the whole period for a
single measurement station. These files were downloaded and converted
to the right encoding using the `dos2unix` command line utility.

In the following, we will read all the files into a list,
```{r listfiles}
files = list.files("aq", pattern = "*.csv", full.names = TRUE)
r = lapply(files[-1], function(f) read.csv(f))
```
then convert the time variable into a `POSIXct` variable, and time order them
```{r fixtimes}
Sys.setenv(TZ = "UTC") # make sure times are not interpreted as DST
r = lapply(r, function(f) {
		f$t = as.POSIXct(f$DatetimeBegin) 
		f[order(f$t), ] 
	}
) 
```
and we deselect smaller datasets that do not contain hourly data:
```{r deselect_smaller_datasets}
r = r[sapply(r, nrow) > 1000]
names(r) =  sapply(r, function(f) unique(f$AirQualityStationEoICode))
length(r) == length(unique(names(r)))
```
and then combine all files using `xts::cbind`, so that they are matched based on time:
```{r converttoxts}
library(xts)
r = lapply(r, function(f) xts(f$Concentration, f$t))
aq = do.call(cbind, r)
```
We can now select those stations for which we have 75\% of the hourly values measured, i.e. drop those with more than 25\% hourly values missing:
```{r removestationswithmissingvalues}
# remove stations with more than 75% missing values:
sel = apply(aq, 2, function(x) sum(is.na(x)) < 0.75 * 365 * 24)
aqsel = aq[, sel] # stations are in columns
```

Next, the [station metadata](http://ftp.eea.europa.eu/www/AirBase_v8/AirBase_v8_stations.zip) was read 
and filtered for rural background stations in Germany by
```{r filterruralbackground}
library(tidyverse)
read.csv("aq/AirBase_v8_stations.csv", sep = "\t", stringsAsFactors = FALSE) %>% 
	as_tibble  %>% 
	filter(country_iso_code == "DE", station_type_of_area == "rural", 
				 type_of_station == "Background") -> a2
```
These stations contain coordinates, and an `sf` object with (static) station metadata is created by
```{r importstationcoords}
library(sf)
library(stars)
a2.sf = st_as_sf(a2, coords = c("station_longitude_deg", "station_latitude_deg"), crs = 4326)
```

We now subset the air quality data to stations that are of type rural background:
```{r subseteuropean}
sel =  colnames(aqsel) %in% a2$station_european_code
aqsel = aqsel[, sel]
```

We can compute station means, and join these to stations locations by
```{r computestationmeans}
tb = tibble(NO2 = apply(aqsel, 2, mean, na.rm = TRUE), station_european_code = colnames(aqsel))
crs = 32632
right_join(a2.sf, tb) %>% st_transform(crs) -> no2.sf 
# load German boundaries
data(air, package = "spacetime")
de <- st_transform(st_as_sf(DE_NUTS1), crs)
ggplot() + geom_sf(data = de) +  geom_sf(data = no2.sf, mapping = aes(col = NO2))
```

## Multivariable geostatistics

Multivariable geostatics involves the _joint_ modelling, prediction
and simulation of multiple variables, 
$$Z_1(s) = X_1 \beta_1 + e_1(s)$$
$$...$$
$$Z_n(s) = X_n \beta_n + e_n(s).$$
In addition to having observations, trend models, and variograms
for each variable, the _cross_ variogram for each pair of residual
variables, describing the covariance of $e_i(s), e_j(s+h)$,
is required. If this cross covariance is non-zero, knowledge of
$e_j(s+h)$ may help predict (or simulate) $e_i(s)$.  This is
especially true if $Z_j(s)$ is more densely sample than $Z_i(s)$.
Prediction and simulation under this model are called cokriging
and cosimulation. Examples using gstat are found when running the
demo scripts
```{r eval=FALSE}
library(gstat)
demo(cokriging)
demo(cosimulation)
```
and are illustrated and discussed in [@asdar].

In case the different variables considered are observed at
the same set of locations, for instance different air quality
parameters, then the statistical _gain_ of using cokriging
as opposed to direct (univariable) kriging is often modest,
when not negligible. A gain may however be that the prediction
is truly multivariable: in addition to the prediction vector
$\hat{Z(s_0)}=(\hat{Z}_1(s_0),...,\hat{Z}_n(s_0))$ we get the full
covariance matrix of the prediction error [@ver1993multivariable].
This means for instance that if we are interested in some
linear combination of $\hat{Z}(s_0)$, such as $\hat{Z}_2(s_0)
- \hat{Z}_1(s_0)$, that we can get the standard error of that
combination because we have the correlations between the prediction
errors.

Although sets of direct and cross variograms can be computed
and fitted automatically, multivariable geostatistical modelling
becomes quickly hard to manage when the number of variables gets
large, because the number of direct and cross variograms required
is $n(n+1)/2$.

In case different variables refer to the same variable take at
different time steps, one could use a multivariable (cokriging)
prediction approach, but this would not allow for e.g. interpolation
between two time steps. For this, and for handling the case of
having data observed at many time instances, one can also model
its variation as a function of continuous space _and_ time, as of
$Z(s,t)$, which we will do in the next section.

## Spatiotemporal geostatistics

Spatiotemporal geostatistical processes are modelled as variables
having a value everywhere in space and time, $Z(s,t)$, with $s$ and
$t$ the continuously indext space and time index. Given observations
$Z(s_i,t_j)$ and a variogram (covariance) model $\gamma(s,t)$ we can
predict $Z(s_0,t_0)$ at arbitrary space/time locations $(s_0,t_0)$
using standard Gaussian process theory.

Several books have been written recently about modern approaches
to handling and modelling spatiotemporal geostatistical data,
including [@wikle2019spatio] and [@blangiardo2015spatial]. Here,
we will use [@RJ-2016-014] and give some simple examples building
upon the dataset used throughout this chapter.

### A spatiotemporal variogram model
Starting with the spatiotemporal matrix of NO$_2$ data in `aq`
constructed at the beginning of this chapter, we will first select
the rural background stations:

```{r computespacetimesamplevariogram, eval = !exists("v.st")}
aqx = aq[ , colnames(aq) %in% a2$station_european_code]
sfc = st_geometry(a2.sf)[match(colnames(aqx), a2.sf$station_european_code)]
st_as_stars(NO2 = as.matrix(aqx)) %>%
	st_set_dimensions(names = c("time", "station")) %>%
	st_set_dimensions("time", index(aqx)) %>%
	st_set_dimensions("station", sfc) -> no2.st
v.st = variogramST(NO2~1, no2.st[,1:(24*31)], tlags = 0:48, 
	cores = getOption("mc.cores", 2))
```

```{r plotvariograms}
v1 = plot(v.st)
v2 = plot(v.st, map = FALSE)
print(v1, split = c(1,1,2,1), more = TRUE)
print(v2, split = c(2,1,2,1), more = FALSE)
```

```{r fitprodsummodel}
# product-sum
prodSumModel <- vgmST("productSum",
	space=vgm(150, "Exp", 200, 0),
	time= vgm(20, "Sph",   40, 0),
	k=2)
StAni = estiStAni(v.st, c(0,200000))
(fitProdSumModel <- fit.StVariogram(v.st, prodSumModel, fit.method = 7,
	stAni = StAni, method = "L-BFGS-B",
	control = list(parscale = c(1,10,1,1,0.1,1,10)),
	lower = rep(0.0001, 7)))
plot(v.st, fitProdSumModel, wireframe=FALSE, all=TRUE, scales=list(arrows=FALSE), zlim=c(0,150))
```
which can also be plotted as wireframes, by
```{r plotprodsummodel}
plot(v.st, model=fitProdSumModel, wireframe=TRUE, all=TRUE, scales=list(arrows=FALSE), zlim=c(0,185))
```

Hints about the fitting strategy and alternative models for
spatiotemporal variograms are given in [@RJ-2016-014].  

With this fitted model, and given the observations, we can
carry out kriging or simulation at arbitrary points in space and
time. For instance, we could estimate (or simulate) values in the
time series that are now missing: this occurs regularly, and in
section \@ref(kriging) we used means over time series based on
simply ignoring up to 25% of the observations: substituting these
with estimated or simulated values based on neigbouring (in space
and time) observations before computing yearly mean values seems
a more reasonable approach.

More in general, we can estimate at arbitrary locations and time
points, and we will illustrate this with predicting time series at
particular locations, and and predicting spatial slices [@RJ-2016-014]. 
We can create
a `stars` object to denote two points and all time instances with
```{r createstarsobject}
set.seed(123)
pt = st_sample(de, 2)
t = st_get_dimension_values(no2.st, 1)
st_as_stars(list(pts = matrix(1, length(t), length(pt)))) %>%
	st_set_dimensions(names = c("time", "station")) %>%
	st_set_dimensions("time", t) %>%
	st_set_dimensions("station", pt) -> new_pt
```

and obtain the corresponding spatiotemporal predictions using `krigeST` with
```{r spacetimekriging, eval = !exists("new_ts")}
no2.st <- st_transform(no2.st, crs)
new_ts <- krigeST(NO2~1, data = no2.st["NO2"], newdata = new_pt,
	     nmax = 50, stAni = StAni, modelList = fitProdSumModel,
		 progress = FALSE)
plot(as.xts(new_ts[2]))
```

Alternatively, we can create spatiotemporal predictions for a set of time-stamped
raster maps, created by
```{r spatiotemporalpredictionsgrid}
t4 = t[(1:4 - 0.5) * (3*24*30)]

d = dim(grd)
st_as_stars(pts = array(1, c(d[1], d[2], time=length(t4)))) %>%
	st_set_dimensions("time", t4) %>%
	st_set_dimensions("x", st_get_dimension_values(grd, "x")) %>%
	st_set_dimensions("y", st_get_dimension_values(grd, "y")) %>%
	st_set_crs(crs) -> grd.st
```
and the subsequent predictions are obtained by

```{r spatiotemporalpredictions, eval = !exists("new_int") }
new_int <- krigeST(NO2~1, data = no2.st["NO2"], newdata = grd.st,
         nmax = 200, stAni = StAni, modelList = fitProdSumModel,
         progress = FALSE)
names(new_int)[2] = "NO2"
```
```{r plotspatiotemporalpredictions}
g + geom_stars(data = new_int, aes(fill = NO2, x = x, y = y)) + 
    facet_wrap(~as.Date(time)) +
    geom_sf(data = st_cast(de, "MULTILINESTRING")) + 
    geom_sf(data = no2.sf, col = 'grey', cex = .5) + 
	coord_sf(lims_method = "geometry_bbox")
```
and shown in figure \@ref(fig:plotspatiotemporalpredictions).

A larger value for `nmax` was needed here to decrease the visible
disturbance (sharp edges) caused by discrete neighbourhood
selections, which are now done in space _and_ time.
