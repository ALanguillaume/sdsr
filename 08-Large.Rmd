# Large data sets {#large}

This chapter describes how large spatial and spatiotemporal datasets
can be handled with R, with a focus on packages `sf` and `stars`.
For practical use, we classify large data sets as either

* too large to fit in working memory, or
* also too large to fit on the local hard drive, or
* also too large to download it to own compute infrastructure

which very roughly corresponds to Gigabyte-, Terabyte- and
Petabyte-sized data sets.

## Vector data: `sf` {#largesf}

### Reading from disk

`st_read` arguments: query, spatial query, `wkt_filter`

Function `st_read` reads vector data from disk, using GDAL, and
then keeps the data read in working memory. In case the file is
too large to be read in working memory, several options exist to
read parts of the file. The first is to set argument `wkt_filter`
with a WKT text string containing a geometry; only geometries from
the target file that intersect with this geometry will be returned.
An example is

```{r}
library(sf)
(file = system.file("gpkg/nc.gpkg", package="sf"))
bb = "POLYGON ((-81.7 36.2, -80.4 36.2, -80.4 36.5, -81.7 36.5, -81.7 36.2))"
nc.1 = st_read(file, wkt_filter = bb)
```

The second option is to use the `query` argument to `st_read`,
which can be any query in "OGR SQL" dialect, which can be used to
select features from a layer, and limit fields. An example is:

```{r}
q = paste("select BIR74,SID74,geom from 'nc.gpkg' where BIR74 > 1500")
nc.2 = st_read(file, query = q)
```
note that `nc.pgkg` is the _layer name_, which can be obtained from `file` using `st_layers`.
Sequences of records can be read using `LIMIT` and `OFFSET`, to read records 51-60 use
```{r}
q = paste("select BIR74,SID74,geom from 'nc.gpkg' LIMIT 10 OFFSET 50")
nc.2 = st_read(file, query = q)
```

Further query options include selection on geometry type, polygon
area. When the dataset queried is a spatial database, then the query
is passed on to the database and not interpreted by GDAL; this means
that more powerful features will be available.  Further information
is found in the GDAL documentation under "OGR SQL dialect".

Very large file files or directories that are zipped can be read
without the need to unzip them, using the `/vsizip` (for zip),
`/vsigzip` (for gzip) or `/vsitar` (for tar files) prefix to files;
this is followed by the path to the zip file, and then followed by
the file inside this zip file. Reading files this way may come at
some computatational cost.

### Reading from databases

`st_read` methods for DBI objects; example PostGIS; persistent spatial index; 

dbplyr: passes on spatial predicates?

### Reading from online resources or web services

direct: /vsis3 /vsicurl /vsixxx

/vsicurl generic online resource; specialized interfaces are
available for cloud platforms or software: `/vsis3` (Amazon S3),
`/vsigs` (Google Cloud Storage), `/vsiaz` (Azure), `/vsioss`
(Alibaba Cloud), or `/vsiswift` (OpenStack Swift Object Storage).

API's: OpenStreetMap, osmdata

## Raster data: `stars`

A common challenge with raster datasets is not only that they come
in large files (single Sentinel-2 tiles are around 1 Gb), but that
many of these files, potentially thousands, are needed to address
the area and time period of interest. At time of
writing this, the Copernicus program which runs all Sentinel
satellites publishes 160 Tb of images per day.  This means that a
classic pattern in using R, consisting of

* downloading data to local disc, 
* loading the data in memory, 
* analysing it

is not going to work.

Cloud-based Earth Observation processing platforms like Google Earth
Engine [@gorelick] or [Sentinel Hub](https://www.sentinel-hub.com/)
recognize this and let users work with datasets up to the petabyte
range rather easily and with a great deal of interactivity. They
share the following properties:

* computations are posponed as long as possible (lazy evaluation),
* only the data you ask for are being computed and returned, and nothing more,
* storing intermediate results is avoided in favour of on-the-fly computations,
* maps with useful results are generated and shown quickly to allow for interactive model development.

This is similar to the `dbplyr` interface to databases
and cloud-based analytics environments, but differs in the aspect of
_what_ we want to see quickly: rather than the first $n$ records,
we want a quick _overview_ of the results, in the form of a map
covering the whole area, or part of it, but at screen resolution
rather than native (observation) resolution.

If for instance we want to "see" results for the United States on
screen with 1000 x 1000 pixels, we only need to compute results
for this many pixels, which corresponds roughly to data
on a grid with 3000 m x 3000 m grid cells.  For Sentinel-2
data with 10 m resolution, this means we can subsample with
a factor 300, giving 3 km x 3 km resolution.  Processing,
storage and network requirements then drop a factor $300^2 \approx 10^5$, compared
to working on the native 10 m x 10 m resolution. On the platforms
mentioned, zooming in the map triggers further computations on a
finer resolution and smaller extent.

A simple optimisation that follows these lines is how stars' plot
method works:  in case of plotting large rasters, it subsamples
the array before it plots, drastically saving time.  The degree
of subsampling is derived from the plotting region size and the
plotting resolution (pixel density). For vector devices, such as pdf,
R sets plot resolution to 75 dpi, corresponding to 0.3 mm per pixel.
Enlarging plots may reveal this, but replotting to an enlarged
devices will create a plot at target density.

### `stars` proxy objects

To handle datasets that are too large to fit in memory, `stars`
provides `stars_proxy` objects.  To demonstrate its use, we will
use the `starsdata` package, an R data package with larger datasets
(around 1 Gb total). It can be installed by
```{r eval=FALSE}
install.packages("starsdata", repos = "http://pebesma.staff.ifgi.de", type = "source")
```
We can "load" a Sentinel-2 image from it by
```{r}
granule = system.file("sentinel/S2A_MSIL1C_20180220T105051_N0206_R051_T32ULE_20180221T134037.zip", package = "starsdata")
file.size(granule)
base_name = strsplit(basename(granule), ".zip")[[1]]
s2 = paste0("SENTINEL2_L1C:/vsizip/", granule, "/", base_name, ".SAFE/MTD_MSIL1C.xml:10m:EPSG_32632")
(p = read_stars(s2, proxy = TRUE))
object.size(p)
```
and we see that this does not actually load _any_ of the pixel
values, but keeps the reference to the dataset and fills the
dimensions table. (The convoluted `s2` name is needed to point
GDAL to the right file inside the `.zip` file containing 115 files
in total).

The idea of a proxy object is that we can build expressions like
```{r}
p2 = p * 2
```
but that the computations for this are postponed. Only when we
really need the data, e.g. because we want to plot it, is `p *
2` evaluated.  We need data when either

* we want to `plot` data, or
* we want to write an object to disk, with `write_stars`, or
* we want to explicitly load an object in memory, with `st_as_stars`

In case the entire object does not fit in memory, `plot` and
`write_stars` choose different strategies to deal with this:

* `plot` fetches only the pixels that can be seen, rather than all
pixels available, and
* `write_stars` reads, processes, and writes data chunk by chunk.

Downsampling and chunking is implemented for spatially dense images,
not e.g. for dense time series, or other dense dimensions.

As an example,
```{r}
plot(p)
```
only fetches the pixels that can be seen on the plot device, rather
than the 10980 x 10980 pixels available in each band. The downsampling
ratio taken is
```{r}
floor(sqrt(prod(dim(p)) / prod(dev.size("px"))))
```
meaning that for every 19 x 19 sub-image in the original image,
only one pixel is read, and plotted. This value is still a bit too
high as it ignores the white space and space for the key on the
plotting device.

### Operations on proxy objects

A few dedicated methods are available for `stars_proxy` objects:
```{r}
methods(class = "stars_proxy")
```
We have seen `plot` and `print` in action; `dim` reads out
the dimension from the dimensions metadata table. 

The three methods that actually fetch data are `st_as_stars`,
`plot` and `write_stars`.  `st_as_stars` reads the actual data into a
`stars` object, its argument `downsample` controls the downsampling
rate. `plot` does this too, choosing an appropriate `downsample`
value from the device resolution, and plots the object. `write_stars`
writes a `star_proxy` object to disc.

All other methods for `stars_proxy` objects do not actually operate
on the raster data but add the operations to a _to do_ list,
attached to the object. Only when actual raster data are fetched,
e.g. by calling `plot` or `st_as_stars`, the commands in this list
are executed.

`st_crop` limits the extent (area) of the raster that will be
read. `c` combines `stars_proxy` objects, but still doesn't read
any data. `adrop` drops empty dimensions, `aperm` changes dimension
order.

`write_stars` reads and processes its input chunk-wise; it has an
argument `chunk_size` that lets users control the size of spatial
chunks.

## Very large datasets

finding them: STAC, pkg rstac

openEO; UDFs offering chunks as stars objects

GEE, rgee;

CDS, ECMWF, ERA5;


